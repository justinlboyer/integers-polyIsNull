%% LyX 2.2.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt]{article}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose}
\setcounter{secnumdepth}{0}
\usepackage{wrapfig}
\usepackage{calc}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{commath}
\usepackage[unicode=true,
 bookmarks=false,
 breaklinks=false,pdfborder={0 0 1},backref=section,colorlinks=false]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.


%\usepackage{exsheets}
\usepackage[toc,page]{appendix}
%\usepackage{amsthm} 
\usepackage{array}
\mathtoolsset{showonlyrefs} 
\usepackage{graphicx}
\usepackage{listings}
%\usepackage{indentfirst}
%\usepackage{setspace}
%\doublespacing
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{dcolumn}
\newcolumntype{2}{D{.}{}{2.0}}


\usepackage{pgf}
\usepackage{tikz}\usepackage{mathrsfs}
\usetikzlibrary{arrows}


\newcommand{\ssol}{\vspace{3em}}
\newcommand{\lsol}{\vspace{10em}}
\newcommand{\blnk}{{\underline {\hspace{1.5in}}}}
\newcommand{\Lim}[1]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle \lim_{#1}\;$}}}


%\usepackage{mcode}

 % Turns off automatic numbering of sections.

%\SetupExSheets{headings=block}












\makeatother

\begin{document}

\section{Comparing the integers and the polynomials with coefficients in the
rational, real or complex number fields}

Suppose I want to solve a polynomial equation 
\[
\frac{a_{d}}{b_{d}}x^{d}+\frac{a_{d-1}}{b_{d-1}}x^{d-1}+\ldots+\frac{a_{0}}{b_{0}}=0
\]
with rational numbers $a_{1}$ as coefficients. I could turn it into
a polynomial equation with integer coefficients just by multiplying
both sides of the above equation by a common multiple of $\left\{ b_{d},b_{d-1},\ldots,b_{0}\right\} $.
So every root of a polynomial with rational coefficients is a root
of a polynomial with integer coefficients. A deeper fact, called Gauss's
Lemma, says that any factorization of a polynomial 
\[
a_{d}x^{d}+a_{d-1}x^{d-1}+\ldots+a_{0}
\]
with integer coefficients $a_{i}$ that can't be factored into two
polynomial factors with integer coefficients cannot be factored into
two polynomial factors with rational coefficients. These two facts
led mathematicians to study systems of polynomials with coefficients
in a number system in which you could add, subtract, multiply $and$
divide. They called such a system a $field$. Besides the field of
rational numbers, denoted $\mathbb{Q}$, other examples of fields
are the field $\mathbb{R}$ of real numbers and the field $\mathbb{C}$
of complex numbers. They also noticed that the system $\mathbb{F}\left[x\right]$
of polynomials with coefficients in a field $\mathbb{F}$ behaves
a lot like the most familiar number system, the integers. Both have
unique factorization into prime factors, greatest common divisors,
least common multiples, etc.

\section{The Mapping $\Phi_{b}$}

What I want to do is create a function (also called a mapping) that
takes a polynomial with integer coefficients to the (corresponding)
number system formed by its coefficients. For example I could ask
that the mapping take any polynomial with integer coefficients to
the integer I get by substituting an integer $b$ for every \char`\"{}$x$''.
As an example of this, if $b=10$, the mapping would send $2x^{2}+3x+4\xrightarrow[\Phi_{10}]{}234$.
It is common to use the letter $\mathbb{Z}$ to denote the integer
number system and $\mathbb{Z}\left[x\right]$ to denote the ring of
polynomials with integer coefficients.

\subparagraph{Example}

We would then denote the function described just above as 
\[
\begin{array}{c}
\Phi_{10}:\mathbb{Z}[x]\rightarrow\mathbb{Z}\\
f(x)\mapsto f(10).
\end{array}
\]
In other words, if $f(x)=5x^{4}+4x^{3}+2x+1$,

\[
\Phi_{10}(f(x))=f(10)=5(10)^{4}+4(10)^{3}+2(10)+1=54021.
\]

More generally 
\[
\Phi_{b}(f(x))=f(b)
\]


\subsection*{Properties of $\Phi_{b}$ }

The following proofs use two polynomials $f_{1},f_{2}$. with coefficients
in any of the number systems $\mathbb{Z}$, $\mathbb{Q}$, $\mathbb{R}$
or $\mathbb{C}$. 
\begin{itemize}
\item $\Phi_{b}$ preserves addition 
\end{itemize}
\begin{align*}
\Phi_{b}(f_{1}(x)+f_{2}(x)) & =f_{1}(b)+f_{2}(b)\\
 & =\Phi_{b}(f_{1}(x))+\Phi_{b}(f_{2}(x))
\end{align*}

\begin{itemize}
\item $\Phi_{b}$ preserves multiplication 
\end{itemize}
\begin{align*}
\Phi_{b}(f_{1}(x)f_{2}(x)) & =f_{1}(b)f_{2}(b)\\
 & =\Phi_{b}(f_{1}(x))\Phi_{b}(f_{2}(x))
\end{align*}

\begin{itemize}
\item A polynomial maps to zero if and only if the polynomial is divisible
by $x-b$ 
\end{itemize}
To see why this last statement is true, suppose we are given $f(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{0}$,
and a number $r$ such that 
\[
f(r)=0.
\]

By long division of polynomials 
\[
a_{n}x^{n}+\cdots+a_{0}=(x-r)\text{·}(c_{n-1}x^{n-1}+\cdots c_{0})+\text{constant}
\]
Substituting $x=r$ into the both sides of the equation above we get
\[
0=0+\text{constant}.
\]
So by necessity $\text{constant}=0$, therefore 
\[
f(x)=(x-r)(c_{n-1}x^{n-1}+\cdots c_{0}).
\]


\subsection*{A rough comparison of prime factorization}

This section will compare prime factorization in the ring of integers
and the ring of polynomials with coefficients in a field. Prime factorization
is decomposing something into its constituent primes. Among the integers
we have the \emph{the fundamental theorem of arithmetic}, which says
that every positive integer has a unique prime factorization. In math
speak this states that any positive integer $k\geq2$, $k$ may be
rewritten as 
\[
k=p_{1}^{l_{1}}p_{2}^{l_{2}}\cdots p_{n}^{l_{n}}
\]
where the $p_{i}$'s are the $n$ distinct prime factors, each of
order $l_{i}$. One way to understand this is to think of it as taking
a positive integer, breaking it up into several unique parts, multiplying
those parts together and getting the same positive integer back. \\

Do the polynomials with coefficients in a field $\mathbb{F}$ have
a similar analogue? In this set-up, polynomials of degree zero, that
is, polynomials with only constant terms $a_{0}$, play the role of
$\text{\textpm1}$ in the integers, so that prime factors are always
polynomials of degree $d\geq1$. Well, we know we are looking to break
up the polynomial into a bunch of parts, each of which can't be broken
up further\textendash multiply those parts together and get our original
polynomial.

For example in $\mathbb{Q}\left[x\right]$ 
\[
x^{2}-1=\left(x+1\right)\left(x-1\right)
\]
but 
\[
x^{2}-2
\]
is prime in the polynomial system in $\mathbb{Q}\left[x\right]$ since
$\sqrt{2}$ is not in the number system $\mathbb{Q}$. However in
$\mathbb{R}\left[x\right]$ we have the prime factorization 
\[
x^{2}-2=\left(x+\sqrt{2}\right)\left(x-\sqrt{2}\right).
\]
Similarly the prime factorization of $f(x)=x^{2}+1$ in $\mathbb{R}\left[x\right]$
is $x^{2}+1$. Since the square of any real number is $\geq0$ 
\[
x^{2}+1
\]
is prime in $\mathbb{R}\left[x\right]$. However in the system of
polynomials $\mathbb{C}\left[x\right]$ we have the factorization
\[
x^{2}+1=\left(x+i\right)\text{·}\left(x-i\right).
\]

So what mathematicians have decided is that a polynomial $f\left(x\right)$
in $\mathbb{F}\left[x\right]$ is called irreducible (irreducible
is the word mathematicians use for when a polynomial is in this example
\char`\"{}prime'') if the polynomial cannot be written as a product
\[
f\left(x\right)=g\left(x\right)\text{·}h\left(x\right)
\]
where $g\left(x\right)$ and $h\left(x\right)$ are polynomials in
the same system $\mathbb{F}\left[x\right]$ and the degrees of $g\left(x\right)$
and $h\left(x\right)$ are both less than the degree of $f\left(x\right)$.
In other words, $f\left(x\right)$ is prime or irreducible any polynomial
$g\left(x\right)$ in the same system $\mathbb{F}\left[x\right]$
that you try to divide into $f\left(x\right)$ always leaves a remainder.
So the only polynomials that divide an irreducible polynomial are
the irreducible polynomial itself and the constant polynomials $a_{0}$.
Remind you of anything?

\subparagraph{Exercise:}

If you don't know already, try to figure out how to do prime factorization
in any system $\mathbb{F}\left[x\right]$ of polynomials where $\mathbb{F}$
is one of our fields. \\

The 'miracle' is that every polynomial $f(x)$ in $\mathbb{R}\left[x\right]$
can be factored into irreducible factors of degrees $1$ and $2$.
But then we can consider each irreducible factor 
\[
ax^{2}+bx+c
\]
of $f(x)$ with $a$, $b$ and $c$ real as a polynomial in $\mathbb{C}\left[x\right]$.
(Remember that real numbers are also complex numbers, it's just that
their imaginary part is zero). We can factor $ax^{2}+bx+c$ in $\mathbb{C}\left[x\right]$
by the quadratic formula into
\[
a\left(x-\frac{-b+\sqrt{b^{2}-4ac}}{2a}\right)\left(x-\frac{-b-\sqrt{b^{2}-4ac}}{2a}\right).
\]
 In fact, not only can any polynomial $f(x)$ in $\mathbb{R}\left[x\right]$
be factored completely in $\mathbb{C}\left[x\right]$ as 
\[
f(x)=a_{d}\text{·}(x-r_{1})^{l_{1}}(x-r_{2})^{l_{2}}\cdots(x-r_{i})^{l_{i}},
\]
the same is true for any $f(x)$ in $\mathbb{C}\left[x\right]$. We
know this fact as the \emph{Fundamental Theorem of Algebra}. The Fundamental
Theorem of Algebra states that the only irreducible polynomials in
$\mathbb{C}\left[x\right]$ are the polynomials of degree one! So,
roughly speaking, every $d$-th degree polynomial in $\mathbb{C}\left[x\right]$
has $d$ complex roots. Tie this together with long division of polynomials
and we find that we could rewrite the polynomial 
\[
f(x)=a_{d}x^{d}+a_{d-1}x^{d-1}+\ldots+a_{0}
\]
as a product 
\[
f\left(x\right)=a_{d}\left(x-s_{1}\right)\text{·}\cdots\text{·}\left(x-s_{d}\right).
\]
I.e., if the $r_{i}$'s are the roots of $f(x)$ and the $l_{i}$
are the multiplicities of the roots, then each root $r_{i}$ occurs
in the list 
\[
s_{1},s_{2},\ldots,s_{d}
\]
exactly $l_{i}$ times. 

\subparagraph{Exercise:}

For some small values of $d$, multiply out the right-hand-side of
the equality
\[
a_{d}x^{d}+a_{d-1}x^{d-1}+\ldots+a_{0}=a_{d}\left(x-s_{1}\right)\text{·}\cdots\text{·}\left(x-s_{d}\right)
\]
so that you can give a formula for each quantity $\frac{a_{i}}{a_{d}}$
as a function of the quantities $s_{1},s_{2},\ldots,s_{d}$. Can you
guess the general formula for all values of $d$? Those formulas are
called the elementary symmetric functions of $s_{1},s_{2},\ldots,s_{d}$.
They will figure in an important way later on in this story.

\section{Toward a proof of the Fundamental Theorem of Algebra}

The Fundamental Thereom of Algebra is critical to much in algebra,
and the proof is often waved away as being beyond the scope of college
mathematics courses. In order to motivate the proof which we propose
to explain in its entirety in what follows, we start with an example
that highlights necessary components of the proof, while giving some
concreteness.

\section*{How many roots does $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$ have?}

Consider the following polynomial 
\[
p(x)=x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
how many roots does it have? Does it help if $r_{1},r_{2},r_{3}$
are real numbers? Let's prove that $p(x)$ has three roots in the
complex field. We will take as a given that we know that, as $x$
goes to $+\infty$, $p(x)$ becomes positive and, as $x$ goes to
$-\infty$, $p(x)$ becomes negative. A slightly more complicated
fact is the fact that $p(x)$ is a continuous function of $x$, which
is often informally explained as the fact that you can draw the graph
of 
\[
y=p\left(x\right)
\]
without lifting your pencil from the page. So as a consquence of the
fact that $p(x)$ is a continuous, your pencil cannot go from negative
$y$ to positive $y$ without crossing a place where $y=p\left(x\right)=0$.
That is, there is a real number $x$ where $p\left(x\right)=0$. So
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
has a real root $s_{1}$. As we have shown earlier, this means that
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}=\left(x-a_{1}\right)\left(x^{2}+bx+c\right).
\]
But now, again as we have already shown, this means that $x^{2}+bx+c$
can be factors into linear factors using the quadratic formula.

Suppose now that we don't know that the coefficients of $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$
are real but just lie in some field $\mathbb{F}$, so the operations
of addition, subtraction, multiplication, and division hold. And suppose
we know that its roots lie in that same field. Let's call these roots
$a_{1},a_{2},a_{3}$.

Typically one would rewrite 
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
as 
\[
(x-a_{1})(x-a_{2})(x-a_{3}).
\]

Why can polynomials be written as linear factors? Recall the previous
section on the relationship between prime numbers and irreducible
factors of polynomials. It turns out that it always is possible to
factor a polynomial in $\mathcal{\mathbb{F}}\left[x\right]$ if $\mathbb{F}$
is a large enough field. 

\subsection*{Field Extension}
The idea of a field extension boils down to increasing the set of numbers one is able to use, while maintaining the properties of addition and multiplication.  %For example the Pythagoreans worked in the field of rational values, therefore when they came across $\sqrt{2}$ they did not believe it could be a solution.  We can now extend our field to all real values and now $\sqrt{2}$ is a perfectly fine solution.

\subsubsection*{5 hour Clock Arithmetic}
The basics of field extension can be most easily understood in the world of clock (modular) arithmetic.  Consider an algebraic structure that consists of five elements, $\{0,1,2,3,4\}$.  It is possible to add in this structure,
for example, $1+1=2$, $1+2=3$, and $0+1=1$ thus 0 is the additive
identity. But what about $1+4=$? Any answer could be correct. Mathematicians
like to play games, so they pick the situation that will continue
the game. They decide to make $1+4=0$. In other words in this 
field the number 5 is mapped to the number 0, this implies the number
6 would be mapped to 1, and $7 \rightarrow 2$, $8 \rightarrow 3$, $9\rightarrow 4, \ldots$. Using these mappings and the induction they imply I can add any values in this structure and still have values in this structure. For example addition in 5-clock arithmetic looks like:
\[
2+4=1
\]
\[
3+4=2
\]
\[
1+2+3+4=0
\]
\[
3+3+3+3+4=1
\]
so on and so forth.  The last equation in the series highlights repeated addition, I can use the model of multiplication as repeated addition to understand multiplication.
\begin{align*}
3+3+3+3+4 & =1\\
4\cdot3+4 & =1\\
4 \cdot (3+1) & =1\text{ Using distributive property}\\
4\cdot4 & =1
\end{align*}
If $4\cdot4=1$, then multiplying both sides by 4, I get 
\begin{align*}
4\cdot4\cdot4 & =4\\
1\cdot4 & =4
\end{align*}
There is a multiplicative identity in this structure, it is the number 1. 


Let's take a closer look at multiplication of twos.  
\begin{align*}
2\cdot0=0\\
2\cdot1=2\\
2\cdot2=4\\
2\cdot3=1 \\
2 \cdot 4 = 3 \\
\end{align*}

I have a multiplicative inverse for 2 it is 3. I.e., $2 \cdot 3=1$.  Likewise there is a multiplicative inverse for $1^{-1}=1$, $3^{-1}=2$, $4^{-1}=4$.  I will use the multiplicative inverse to understand division in this system.\\

For example suppose I wanted to find what $2\div4 =?$ is in 5-clock arithmetic.  I can rephrase this as a multiplication problem, $ 2 = ? \cdot 4$.  Then I know 4 has a multiplicative inverse namely itself, so I can multiply the previous equation by 4, $2 \cdot 4 = ? \cdot 4 \cdot 4$.  $2\cdot 4=3$ in this clock 5 arithmetic and $4 \cdot 4 =1$, therefore $3=?$.  So $2\div4=3$ in this system and I can divide, because I have a multiplicative inverse.

A more efficient method for computing division exists.  In fact it was the Egyptians who were one of the first peoples to record this more efficient method of division.
Similar to the above process they framed it as a multiplication problem with the multiplier (or
multiplicand)\footnote{Why does it not matter if it is the multiplier or multiplicand that
is missing?} missing. But then they used a multiplication table  to deduce the solution.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% Overview of division
%
%The Egyptians then used their times tables to deduce the the answer.
%At its core this is what division is, multiplication in reverse, multiplication
%in-reverse, multiplication in-verse. We invert multiplication, this
%is why when dividing fractions we flip and multiply, division \underline{is}
%multiplicative inverse.  Thus if a set of numbers has a multiplicative inverse, then division is defined for that set of numbers.\\

From here it will help if we have a times
table to reference: 
\begin{center}
\global\long\def\arraystretch{1.3}
 \setlength{\doublerulesep}{0pt} %
\begin{tabular}{r||2|2|2|2|2}
$\times$  & 0  & 1  & 2  & 3  & 4 \tabularnewline
\hline 
\hline 
0  & 0  & 0  & 0  & 0  & 0 \tabularnewline
\hline 
1  & 0  & 1  & 2  & 3  & 4 \tabularnewline
\hline 
2  & 0  & 2  & 4  & 1  & 3 \tabularnewline
\hline 
3  & 0  & 3  & 1  & 4  & 2 \tabularnewline
\hline 
4  & 0  & 4  & 3  & 2  & 1 \tabularnewline
\hline 
\end{tabular}
\par\end{center}

What the Egyptians did was re-frame the division problem as multiplication, the read of the answer.  For example, $3\div3=?$ re-framed becomes $3 = ? \cdot 3$.  Now I look at the times table and see that 4 times 3 gives me 3, therefore $?=4$ and $3 \div 3=4$.  Remember division is multiplication inverted.  In other words, each division problem is really just a multiplication problem in reverse.  Next I will show you a structure that does not have a multiplicative inverse.

%\begin{align*}
%2 \div 1 = ? \rightarrow 2 = 1 \cdot ?  \Rightarrow ?=2 \\
%2 \div 2 = ? \rightarrow 2  = 2 \cdot ? \Rightarrow ?=1 \\
%2 \div 3 = ? \rightarrow 2 = 3 \cdot ? \Rightarrow ?=4 \\
%2 \div 4 = ? \rightarrow 2 = 4  \cdot ? \Rightarrow ?=3 \\
%\end{align*}

\subsubsection*{4 hour clock arithmetic}
Consider a structure whose set of numbers consist of $\{0,1,2,3\}$.
Similar to the 5-clock structure it is possible to add these numbers, for example, $1+1=2$, $1+2=3$, $0+1=1$ and likewise $1+3=0$.  In this structure I map the number 4 to 0, $5 \rightarrow 1, 6 \rightarrow 2,\ldots$etc.  I can also consider multiplication take for example:
\[
2+2+2+3=1
\]
Again I can utilize repeated addition to understand multiplication.
\begin{align*}
2+2+2+3 & =1\\
2\cdot3+3 & =1\\
(2+1)\cdot3 & =1\\
3\cdot3 & =1
\end{align*}
This system has a multiplicative identity.  Does it have a multiplicative inverse, can we divide.  From here it will be helpful to reference a times table in this 4-clock structure:

\begin{center}
\global\long\def\arraystretch{1.3}
 \setlength{\doublerulesep}{0pt} %
\begin{tabular}{r||2|2|2|2}
$\times$  & 0  & 1  & 2  & 3 \tabularnewline
\hline 
\hline 
0  & 0  & 0  & 0  & 0 \tabularnewline
\hline 
1  & 0  & 1  & 2  & 3 \tabularnewline
\hline 
2  & 0  & 2  & 0  & 2 \tabularnewline
\hline 
3  & 0  & 3  & 2  & 1 \tabularnewline
\hline 
\end{tabular}
\par\end{center}

Suppose I wanted to find $3\div2=?$.  Previously I used the multiplicative inverse to find the solution to a division problem, so I'll try that same strategy again.  I'll re-frame the problem as a multiplicative one $3 = ? \cdot 2$, now I want to multiply both sides by the multiplicative inverse of 2, however I'm not able to, because there is no number, which I can multiply 2 by and get 1 in this 4-clock structure.  Therefore because I do not have a multiplicative inverse I cannot define division.




\subsubsection*{Applying the idea of field extension to polynomials}

Suppose that we are in the real numbers with any polynomial of the
form 
\[
x^{3}+x^{2}+x+1
\]
and I want to factor this polynomial. Thus I need to find the splitting
field. It looks like $x=-1$ might be a root. So I use long division\footnote{Why can we use long division here? Because we are in a field, if we
are in a field, then long division is defined. The field we are in
is the field of polynomials with integer coefficients.} to find 
\[
x^{3}+x^{2}+x+1=(x+1)(x^{2}+1)
\]
Look at the $x^{2}+1$ term it is tempting to say it has \char`\"{}no
solution in the reals'', which is technically correct. But there
is another way to frame it. I can apply the previous logic to polynomials,
I.e., 
\[
x^{2}+1=0
\]
set the un-factorable term to 0, from that it follows that 
\[
x^{2}=-1.
\]
This would be a field extension. For example multiplication in this
field extension, $\mathbb{Z}[x]/(x^{2}+1)$\footnote{Commonly called $\mathbb{Z}[x]/(x^{2}+1)$, which means the field
of polynomials with integer coefficients modular $x^{2}+1$} is. 
\begin{align*}
(a+bx)(c+dx) & =(a+bx)c+(a+bx)dx\\
 & =ac+bcx+adx+bdx^{2}\\
 & =(ac+bdx^{2})+(ad+bc)x\\
 & =(ac-bd)+(ad+bc)x
\end{align*}
Where the last step used the identity $x^{2}=-1$. In some ways working
from this place one might say we are building the complex numbers
from the real numbers. But the reason I am doing this, is so I can
split any polynomial up (find the splitting field). 

%I am extending
%the field as a quotient ring generated by $x^{2}+1$. $x^{2}+1$ is
%also know as an irreducible factor, and when it acts as the divisor
%in the quotient ring, it is known as the ideal.

%\paragraph*{Example}
%
%What is the splitting field for 
%\[
%x^{4}+x^{3}+x^{2}+1?
%\]
%Using the previous field extension, i.e., the quotient ring generated
%by $x^{2}+1=0$. 
%\begin{align*}
%x^{4}+x^{3}+x^{2}+1 & =x^{4}+x^{3}+0\\
% & =x^{3}(x+1)
%\end{align*}
%Lastly, recall $x^{2}=-1$ this implies $x^{3}=-x$. Therefore: 
%\[
%x^{4}+x^{3}+x^{2}+1=-x(x+1)
%\]
%in our new field $\mathbb{Z}/(x^{2}+1)$. I have split the polynomial
%into linear factors, this is the splitting field, and in this field
%0 and -1 are the roots of the polynomial.\\



\subsubsection*{Splitting field of $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$}

I can rewrite 
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
as 
\[
(x-a_{1})(x-a_{2})(x-a_{3})
\]
because we look at it in a field in which it is possible, just as
I did in the above examples. \\

Suppose that $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$ is irreducible in the field of polynomials with real-valued coefficients, $\mathbb{R}[x]$.  Then what I want to do is construct a clock arithmetic where
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3} \equiv 0,
\]
I'll call this new field $\mathbb{G}[y]$.  In this new field the function $f(y)=y^{3}-r_{1}y^{2}+r_{2}y-r_{3}$ has a root at $f(x)=x^{3}-r_{1}x^{2}+r_{2}x-r_{3}=0$ therefore $x$ is root of $y^{3}-r_{1}y^{2}+r_{2}y-r_{3}$.\footnote{Since $x$ is not divisible by $y^{3}-r_{1}y^{2}+r_{2}y-r_{3}$, it cannot be a multiple of it.} For the sake of convenience, let $x=a_1$.  I then reduce the degree by long division thus $y^{3}-r_{1}y^{2}+r_{2}y-r_{3}$ becomes 
\[
(y-a_1)(y^2+by+c)
\]
where $a$ and $c$ are some values in $\mathbb{G}[x]$.  I complete the square on the quadratic term to find the other remaining two roots.
\begin{align*}
y^2+by+c &= 0 \\
\left(y+\frac{b}{2}\right)^2 - \frac{b^2}{4}+c &= 0 \\
\left(y+\frac{b}{2}\right)^2 &= \frac{b^2}{4} - c \\
\left(y+\frac{b}{2}\right)^2 &= \frac{b^2-4c}{4} \\
\abs{y+ \frac{b}{2}} &= \sqrt{\frac{b^2-4c}{4}} \\
y + \frac{b}{2} &= \pm \sqrt{\frac{b^2-4c}{4}} \\
y + \frac{b}{2} &= \pm \frac{\sqrt{b^2-4c}}{2} \\
y &= \frac{-b}{2} \pm \frac{\sqrt{b^2-4c}}{2} \\
y &= \frac{-b \pm \sqrt{b^2-4c}}{2}
\end{align*}

If $\sqrt{b^2-4c}$ is reducible in $\mathbb{G}$ then I'm done, let $a_2 = \frac{-b + \sqrt{b^2-4c}}{2}$ and $a_3 = \frac{-b - \sqrt{b^2-4c}}{2}$, therefore my splitting field is:
\[
(y-a_1)(y-a_2)(y-a_3).
\]
If $\sqrt{b^2-4c}$ is not reducible, then I need to expand the field again.  This time I set
\[
y - \frac{-b + \sqrt{b^2-4c}}{2} \equiv 0
\]
I'll call this new field $\mathbb{J}\left[\sqrt{b^2-4c}\right]$.  Now $\sqrt{b^2-4c}$ is a root in this field.  Surely $x$ is still a root, because $\mathbb{J}\left[\sqrt{b^2-4c}\right]$ contains $\mathbb{G}[x]$, since we built $\mathbb{J}$ off $\mathbb{G}$. Therefore on root is $a_2 =\frac{-b + \sqrt{b^2-4c}}{2}$ and the other is $a_3 = \frac{-b - \sqrt{b^2-4c}}{2}$.  I have now successfully split my polynomial 
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
which belonged to the field of polynomials with real valued coefficients into
\[
\left(z-x \right) \left(z-\frac{-b + \sqrt{b^2-4c}}{2}\right)\left(z - \frac{-b - \sqrt{b^2-4c}}{2} \right)
\]
which belongs to the field $\mathbb{J}\left[\sqrt{b^2-4c}\right]$ which is quite possibly very different than the field of polynomials with real valued coefficients.





%What we want to do first is factor $x^3 -r_1 x^2 +r_2 x - r_3$ as far as possible, suppose $x^3 -r_1 x^2 +r_2 x - r_3$ is as far as we can factor it in our field.   Then $$x^3 -r_1 x^2 +r_2 x - r_3 =0$$
%This then implies that $$x^3 -r_1 x^2 = r_3- r_2 x$$
%Thus $$x^3 -r_1 x^2 +r_2 x - r_3 = r_3- r_2 x +r_2 x - r_3 = 0$$ it works!  
%Let $$f(x) = x^n + b_{n-1}x^{n-1} + \cdots + b_1 x + b_0$$ be our ideal (like $x^2+1$), with $n \leq 3$  we know $a_1$ is a root, therefore:
%\begin{align*}
%a_1^n + b_{n-1}a_1^{n-1} + \cdots + b_1 a_1 + b_0 &=0 \\
%a_1^n  &= -(b_{n-1}a_1^{n-1} + \cdots + b_1 a_1 + b_0) \\
%a_1^na_1^{3-n} &= -(b_{n-1}a_1^{n-1} + \cdots + b_1 a_1 + b_0)a_1^{3-n} \\
%&= -(b_{n-1}a_1^{3-1} + \cdots + b_1 a_1^{3-n+1} + b_0 a_1^{3-n}) \\
%&= -(b_{n-1}a_1^{2} + \cdots + b_1 a_1^{4-n} + b_0 a_1^{3-n})
%\end{align*}
%In this case we would take $n=0,1,2$

%in this case we know the roots, so let's just do long division, which gives us:
%$$x^4+x^3+x^2+1 = (x-a_1)(x^3 + (a_1+1)x^2 + (a_1^2+a_1+1) + (a_1^3+a_1^2+a_1))$$

\mbox{}\\

In other words, I keep building larger and larger fields until I have split my original polynomial into linear terms.  This new field $\mathbb{J}$ may look very different than my original field, for example $\sqrt{2}$ might actually have a solution in my new field.  Now I have split 
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
into 
\[
(x-a_{1})(x-a_{2})(x-a_{3})
\]
and $a_{1},a_{2},a_{3}$ are elements of this field $\mathbb{J}$.\\

I can't say that $a_{1},a_{2},a_{3}$ are my three roots, because
they may not even be complex valued, let alone real-valued.

\subsubsection*{Recap and relationship with prime numbers}
To recap what I did:\\

{\color{red} TBD}




I know that $\mathbb{J}$ is a field thus all the usual properties of addition
and multiplication hold. Therefore there is nothing holding me back from distributing and multiplying: 
\[
(x-a_{1})(x-a_{2})(x-a_{3})=x^{3}-(a_{1}+a_{2}+a_{3})x^{2}+(a_{1}a_{2}+a_{1}a_{3}+a_{2}a_{3})x-a_{1}a_{2}a_{3}
\]

Something really cool is going on!

\subsection*{Symmetric Polynomials}

I am going to look at 
\[
x^{3}-(a_{1}+a_{2}+a_{3})x^{2}+(a_{1}a_{2}+a_{1}a_{3}+a_{2}a_{3})x-a_{1}a_{2}a_{3}
\]
more thoroughly. Assigning the following: 
\begin{align*}
e_{1} & =a_{1}+a_{2}+a_{3}\\
e_{2} & =a_{1}a_{2}+a_{1}a_{3}+a_{2}a_{3}\\
e_{3} & =a_{1}a_{2}a_{3}
\end{align*}

The $e_{1},e_{2},e_{3}$ I just designated are called elementary symmetric
polynomials. Viéte found formulas that draw a connection between a
polynomial's coefficients and its roots. We are going to leverage
those formulas now. It has been shown that Viéte's formulas may be
applied with polynomials with coefficients in any field.{\color{red}\emph{This
is correct right? if Vietes works in most integral domains then it
surely will work in a field}}

\paragraph*{Example:}

Using Viéte's formulas to rewrite 
\[
a_{1}^{2}+a_{2}^{2}+a_{3}^{2}
\]
as the previously defined elementary symmetric polynomials.

I will need the sum of squares: 
\[
e_{1}^{2}=a_{1}^{2}+a_{2}^{2}+a_{3}^{2}+2a_{1}a_{2}+2a_{1}a_{3}+2a_{2}a_{3}
\]
Now I need to remove the cross terms, $e_{2}$ should work nicely
for that 
\[
e_{1}^{2}-2e_{2}=a_{1}^{2}+a_{2}^{2}+a_{3}^{2}+2a_{1}a_{2}+2a_{1}a_{3}+2a_{2}a_{3}-2(a_{1}a_{2}+a_{1}a_{3}+a_{2}a_{3})
\]
after zeroing out terms I arrive at 
\[
e_{1}^{2}-2e_{2}=a_{1}^{2}+a_{2}^{2}+a_{3}^{2}
\]

Returning to the polynomial in question, $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$,
the elementary symmetric polynomials $e_{1},e_{2},e_{3}$ are the
same as the coefficients, $r_{1},r_{2},r_{3}$ respectively. This
means I can relate the roots in our strange field $\mathbb{F}$ with
the coefficients of the original polynomial. Continuing the polynomial
I can put the car back together. Since the $r_{1},r_{2},r_{3}$ are
real valued, then $e_{1},e_{2},e_{3}$ must be real valued. But I
still can't make the leap to $a_{1},a_{2},a_{3}$ being real valued,
for example suppose 
\begin{align*}
a_{1}=1-i\\
a_{2}=1+i\\
a_{3}=1
\end{align*}
Then the elementary symmetric polynomials would become 
\begin{align*}
e_{1}=3\\
e_{2}=4\\
e_{3}=2
\end{align*}

The elementary symmetric polynomials are real, however the roots are
not necessarily real valued. Thus I cannot yet say I have three real
valued roots.

In order to sort this root problem out, I \char`\"{}need'' to use
induction on a new equation. This new equation will have the following
form: 
\[
G_{s}(x)=(x-a_{1}-a_{2}-sa_{1}a_{2})(x-a_{1}-a_{3}-sa_{1}a_{3})(x-a_{2}-a_{3}-sa_{2}a_{3})
\]
where the $a_{1},a_{2},a_{3}$ are the roots in the splitting field,
$\mathbb{F}$ of the original polynomial and $s$ is a real number.
When I expand $G_{s}(x)$ we get: %$$ G_s(x) = x^2 - x(a_1 + 2a_2 + a_3 + s (a_1 a_2 + a_2 a_3))  + a_1 a_2 + a_1 a_3 + a_2 a_3 + a_2^2 + s(a_1 a_2^2 + 2a_1 a_2 a_3 + a_2^2 a_3) + s^2 a_1 a_2^2 a_3$$

{\color{red} to be fixed} \begin{flalign*} Gs(x) \&= x3 \\
 \&\hphantom{{}=x} {\color{green} - x2 ( 2 a1 + 2 a2 + 2 a3 + a1
a2 s + a1 a3 s + a2 a3 s )} \&\\
 \&\hphantom{{}=x+x} {\color{blue} + x( a1 a2 a32 s2 + a1 a22 a3
s2 + a12 a2 a3 s2 + a1 a22 s + a1 a32 s + a2 a32 s + a12 a2 s + a12
a3 s + a22 a3 s} \\
 \&\hphantom{{}=x+x+x+x} {\color{blue} + 6 a1 a2 a3 s + a12 + a22
+ a32 + 3 a1 a2 + 3 a1 a3 + 3 a2 a3 )} \&\\
 \&\hphantom{{}=x+x+x} {\color{red} -a12 a22 a32 s3 - 2 a1 a22
a32 s2 - 2 a12 a2 a32 s2 - 2 a12 a22 a3 s2 - a12 a22 s - a12 a32 s
- a22 a32 s} \\
 \&\hphantom{{}=x+x+x+x} {\color{red} - 3 a1 a2 a32 s - 3 a1 a22
a3 s - 3 a12 a2 a3 s - a1 a22 - a1 a32 - a2 a32 - a12 a2 - a12 a3
- a22 a3 - 2 a1 a2 a3 } \end{flalign*}

Notice how I could switch all the $a_{1}$ with $a_{2}$ (likewise
with $a_{1}$ and $a_{3}$ or $a_{2}$ and $a_{3}$) and I would end
up with an equivalent equation. Polynomials of this form are called
symmetric and $G_{s}(x)$ is symmetric in the coefficients $a_{1},a_{2},a_{3}$.
Since $G_{s}(x)$ is symmetric it can be shown that I can then write
$G_{s}(x)$ in terms of its elementary symmetric polynomials.\footnote{This is known as the Fundamental Theorem of Symmetric Polynomials,
the proof of which is a bit beyond this paper.} Recall that the elementary symmetric polynomials are 
\begin{align*}
e_{1}=a_{1}+a_{2}+a_{3}\\
e_{2}=a_{1}a_{2}+a_{1}a_{3}+a_{2}a_{3}\\
e_{3}=a_{1}a_{2}a_{3}
\end{align*}

Using the elementary symmetric polynomials I can rewrite {\color{red}
to be fixed} \begin{flalign*} Gs(x) \&= x3 \\
 \&\hphantom{{}=x} {\color{green}- x2 ( 2 e1 + e2 s )} \&\\
 \&\hphantom{{}=x+x} {\color{blue}+ x( e1 e3 s2 + e1 e2 s + e12
- 2e2 + 3 e2 ) }\&\\
 \&\hphantom{{}=x+x+x} {\color{red}-e32 s3 - 2 e2 e3 s2 - (e22
- 2e1e3) s - 3 e1 e3 s - e1 e2 + e3} \end{flalign*}

The coefficients of the elementary symmetric polynomials are real
valued. Furthermore the elementary symmetric polynomials are real
valued {\color{red}\emph{WHY?.. attempt}}. Recall that the $e_{1},e_{2},e_{3}$
relate to the original polynomial, 
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
Not only did $e_{1},e_{2},e_{3}$ relate to the roots, but they related
to the coefficients, i.e., 
\begin{align*}
e_{1}=r_{1}\\
e_{2}=r_{2}\\
e_{3}=r_{3}
\end{align*}

Remember that $r_{1},r_{2},r_{3}$ are real valued, $\mathbb{R}$.
Now $G_{s}(x)$ may be rewritten in terms of the elementary symmetric
polynomials, which are equivalent to the coefficients in the original
polynomial. Thus the coefficients of $G_{s}(x)$ must be real valued.

\subsection*{Induction}

Now I will abuse induction in preparation for the coming proof. Pretend
for the time being the original polynomial $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$
has degree $n=2^{k}m$ where $k>0$ and is the largest integer such
that $n=2^{k}m$ is true and $m$ is odd. I will show that for any
value $k$ there is at least one complex root. I will do this by inducting
on $k$.

\subsubsection*{Base Case}

For the base case $k=0$, the polynomial is odd, the end behavior
of odd polynomials with positive leading coefficient is $x\rightarrow-\infty$
the value of the polynomial $\rightarrow-\infty$ and as $x\rightarrow\infty$
the value of the polynomial $\rightarrow\infty$. Recall that a polynomial
is continuous, there are no jumps, therefore the polynomial must cross
the $x-$axis at some place in between $(-\infty,\infty)$ therefore
there is at least one complex root. This argument also takes care
of any polynomials with odd degree. I will elaborate on this argument
in the generalized proof.

\subsubsection*{Induction Step}

Now for the induction step, I use the induction hypothesis that every
polynomial with real coefficients and degree $n=2^{k-1}m'$ (where
$m'$ is odd) has at least one complex root.\\

The original polynomial has degree $n=2^{k}m$, but the degree of
$G_{s}(x)$ is $2^{k-1}m(n-1)$ (this is proven in the generalized
case). For the specific case $n=3$, which implies that $k=0$ and
$m=3$ thus the degree of $G_{s}(x)$ is $2^{-1}3(2)=3$.\footnote{Technically the following does not hold, because $m'$ is not odd}
Since $G_{s}(x)$ is of the form $2^{k-1}m'$, we can apply the induction
hypothesis. Thus $G_{s}(x)$ has at least one complex root.\\

I do not know if this complex root is 
\[
a_{1}+a_{2}+sa_{1}a_{2}
\]
or 
\[
a_{1}+a_{3}+sa_{1}a_{3}
\]
or 
\[
a_{2}+a_{3}+sa_{2}a_{3}
\]

For now pretend that $a_{1}+a_{2}+sa_{1}a_{2}$ is complex (the rigor
is left for the generalized proof). Recall that $s$ is real valued,
$s\in\mathbb{R}$. If $s=0$ then $a_{1}+a_{2}$ must be complex.
What if $s\neq0$? If the sum $a_{1}+a_{2}$ is real for a complex
number, then the imaginary parts must zero out, then in order for
$a_{1}+a_{2}+sa_{1}a_{2}$ to be complex $a_{1}a_{2}$ must be complex.
Recall that $s$ is a parameter and it is real valued, so the fact
that $a_{1}+a_{2}+sa_{1}a_{2}$ is complex does not depend on $s$.
Thus the sum $a_{1}+a_{2}$ is complex and the product $a_{1}a_{2}$
is complex. Now I need to show that $a_{1}$ is complex and $a_{2}$
is complex.\\

Consider the equation 
\[
x^{2}-(a_{1}+a_{2})x+a_{1}a_{2}
\]
the roots of this equation are $a_{1}$ and $a_{2}$, since the degree
of this equation is of the form $2^{k-1}m'$ by the induction hypothesis
$x^{2}-(a_{1}+a_{2})x+a_{1}a_{2}$ has at least one complex root.
Thus $a_{1}$ or $a_{2}$ is complex. So I have at least one complex
root. What about the other three?

\subsection*{The three roots}

Suppose that $a_{1}$ is indeed the complex root. Then I can factor
$a_{1}$ out of our original equation $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$.
When I do this I arrive at 
\[
(x-a_{1})(x^{2}-\rho_{1}x-\rho_{2})
\]
Now I can apply the same process I went through and find that $x^{2}-\rho_{1}x-\rho_{2}$
has a \char`\"{}complex''\footnote{Complex is in quotes, because the imaginary part might be zero}
root, suppose this time it is $a_{3}$, I can factor out this term
arriving at: 
\[
(x-a_{1})(x-a_{3})(x-\gamma)
\]
Again I apply this process to $x-\gamma$ and I find that $\gamma=a_{2}$
is complex and I have a full set of roots.

The previous steps for identifying how many roots $x^{3}-r_{1}x^{2}+r_{3}x-r_{3}$
has, are equivalent to the general proof for the Fundamental Theorem
of Algebra.

% now work through the fact that g is symmetric then elementary symmetric, then use the fact that there are two roots in the complex blah blah

% does this proof only work for polynomials greater than a certain degree, find out.

\section*{The Fundamental Theorem of Algebra}

The Fundamental Theorem of Algebra can be expressed as any of the
following three statements: 
\begin{enumerate}
\item That every single variable polynomial of degree $n$ has exactly $n$
roots (counting multiplicity). 
\item Any polynomial with complex coefficients greater than degree 0 has
at least one complex root. 
\item The field of complex numbers is closed under algebra. 
\end{enumerate}
These statements are all equivalent, recall a field\footnote{A field is an algebraic structure with a form of addition, subtraction,
multiplication and division, satisfying the commutative and distributive
properties} $K$ is called \emph{algebraically closed} if every non-constant
polynomial $f(c)\in K[x]$ has a root in $K$.

So a field, $K$ is algebraically closed if the roots of every non-constant
polynomial (e.g. $x^{2}+1$) are in $K$.

%\paragraph*{Comprehension check:} With only rereading the definition and explanation of algebraic closure, think of an example of why the field of integers is not closed under algebra.\footnote{$x^2+1$ would work because the roots of $x^2+1$ are $x=\pm i$}

From the definition of algebraically closed it follows that \textbf{statement
2} and \textbf{statement 3} are identical, in other words \textbf{statement
3} is just shorthand for stating \textbf{statement 2}. Remember that
a complex number with no imaginary part is a real number.

\textbf{Statement 1} is how the Fundamental Theorem of Algebra is
typically stated in the secondary setting and it really is a corollary
of \textbf{statement 2}. As in the previous section I can factor out
the complex number I find. This leaves a linear term multiplying a
polynomial which is one degree less than our original. I can then
find the complex root of this new polynomial, and factor it out. I
can repeat this process, but I can only do it $n$ times, thus I get
$n$ roots.

\section*{Algebraic proof of The Fundamental Theorem of Algebra}

I will attempt to prove that every non-constant polynomial with complex
coefficients has a complex root in a manner that does not require
rigorous mathematical study.\footnote{Many thanks to \url{https://www.artofproblemsolving.com/wiki/index.php?title=Fundamental_Theorem_of_Algebra##Algebraic_Proof}}

%It is necessary to assume the following
%\begin{enumerate}
%\item Every odd degree polynomial with real coefficients has at least one real root.\footnote{One may prove this with the Intermediate Value Theorem} \emph{Include example picture, proof using IVT in appendix}
%\item Every polynomial of degree two with complex coefficients has a complex root.\footnote{Proven by the fact that every polynomial with real coefficients has a complex root} \emph{Include example picture}
%\end{enumerate}
%\paragraph*{Lemma} 

\subsection*{Set up}

Think of a polynomial with complex coefficients (e.g. $x^{2}+4x+2$,
or $(1+2i)x+3$), now think of another. Now think of every polynomial
with complex coefficients and let's let $C(x)$ represent one of them,
we don't know which, but it is one of them.\\


\subsubsection*{Complex number review}

Recall that to take the conjugate of a complex number you just switch
the addition or subtraction on the imaginary part (e.g. if $z=1+2i$
the the conjugate of $z$ is $\bar{z}=1-2i$). Something neat happens
when you multiply a complex number with its conjugate. Let's let $z=a+bi$
be any complex number, then the conjugate is $\bar{z}=a-bi$ and if
we multiply them together we get 
\[
z\bar{z}=a^{2}+abi-abi-(bi)^{2}
\]
which becomes 
\[
z\bar{z}=a^{2}+b^{2}
\]
which is a real number, i.e., the imaginary part is 0 (or there is
no imaginary part). Furthermore it is a positive real number!\\


\subsubsection*{Creating a polynomial with real coefficients}

So back to our polynomial with complex coefficients, $C(x)$ if we
take $R(x)=C(x)\bar{C(x)}$ (which is $C(x)$ multiplied by its conjugate)
then $R(x)$ will be a polynomial with real coefficients, and the
roots of $C(x)$ will also be the roots of $R(x)$. To see this think
about how we find roots, one way is to factor, so if we factor $C(x)$
into a bunch of linear terms, say $c_{1}(x)c_{2}(x)\cdots c_{n}(x)$
then we could factor $R(x)$ into 
\[
c_{1}(x)c_{2}(x)\cdots c_{n}(x)\overline{c_{1}(x)c_{2}(x)\cdots c_{n}(x)}=c_{1}(x)c_{2}(x)\cdots c_{n}(x)\bar{c_{1}(x)}\bar{c_{2}(x)}\cdots\bar{c_{n}(x)}
\]
. Since, the roots of $C(x)$ are also the roots of $R(x)$, showing
that every polynomial with real coefficients has a complex root will
complete the proof!

\subsubsection*{Set up summary}

To summarize the above, since the goal is to show that any polynomial
with complex coefficients has a complex root then I can build a polynomial
with real coefficients by multiplying my polynomial with complex coefficients
with its conjugate. Then if I show that this new polynomial with real
coefficients has a complex root, I'm done.

\subsection*{Proof by Induction}

Suppose the degree of $R(x)$ is $d=2^{n}q$, where $q$ is odd and
$n$ is a natural number, $\mathbb{N}$. %$R(x)$ will always have an even degree, because the degree of $d(R(x))=d(C(x))*2$.  
Then lets induct on $n$, in other words, we show that the first case
is true, then we show that each case follows from the previous, therefore
each case must be true.

\pagebreak{}

\subsubsection*{Base Case}

\begin{wrapfigure}{r}{8cm}%
\definecolor{qqwuqq}{rgb}{0.,0.39215686274509803,0.} \begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=0.8cm,y=0.8cm]
\draw[->,color=black] (-4.604132231404959,0.) -- (4.784297520661155,0.);
\foreach \x in {-4.,-3.,-2.,-1.,1.,2.,3.,4.}
\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt);
\draw[->,color=black] (0.,-2.9905785123966946) -- (0.,4.249090909090907);
\foreach \y in {-2.,-1.,1.,2.,3.,4.}
\draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt);
\clip(-4.604132231404959,-2.9905785123966946) rectangle (4.784297520661155,4.249090909090907);
\draw[line width=1.2pt,color=qqwuqq,smooth,samples=100,domain=-4.604132231404959:4.784297520661155] plot(\x,{((\x)+2.0)*((\x)-1.0)*(\x)});
\begin{scriptsize}
\draw[color=qqwuqq] (-2.157851239669423,-2.7757024793388436) node {};
\end{scriptsize}
\end{tikzpicture} \end{wrapfigure}%

If we look at the base case $n=0$, then $R(x)$ is degree $d=q$,
where $q$ is odd. Recall that the leading coefficient of a polynomial
will determine the overall slope of the equation. The leading coefficient
for $R(x)$ must be positive, because the leading coefficient is the
coefficient of the highest degreed term in $C(x)$ multiplied with
its conjugate, the product of which will be positive. So the polynomial
$R(x)$ will \char`\"{}start'' somewhere in quadrant three (i.e.,
$\Lim{x\rightarrow-\infty}R(x)\rightarrow-\infty$), so when $x$
is negative and large enough, $R(x)<0$. Then as we traverse the function
$R(x)$ it will rise up and \char`\"{}end'' in quadrant one (i.e.,
$\Lim{x\rightarrow\infty}R(x)\rightarrow\infty$), so when $x$ is
positive and large enough, $R(x)>0$.\\

By combining this analysis with the fact that this function is continuous
we \emph{know} this function will intersect the x-axis. The mathematical
justification for \emph{knowing} is by implicating the intermediate
value theorem, which states that a continuous function, $f$, with
an interval $[a,b]$ as its domain, takes values $f(a)$ and $f(b)$
at each end of the interval, then it also takes any value between
$f(a)$ and $f(b)$ at some point within the interval. For the sake
of completeness the corollary that applies beautifully here is \textbf{Bolzano's
theorem}. Bolzano's theorem states that if a continuous function has
values of opposite sign inside an interval (which we have, i.e., at
some point $x<0,R(x)<0$ and also $x>0,R(x)>0$), then it has a root
in that interval. For proofs of the intermediate value theorem and
continuity of polynomials see the appendix.

\subsubsection*{Induction Step}

Now for the fun part. Suppose again that $d=2^{n}q$, where $q$ is
odd and $n>0$, however lets \char`\"{}believe\char`\"{} that the
theorem has been proven when the degree is $2^{n-1}q'$ where $q'$
is odd. Therefore, we can use the \char`\"{}fact\char`\"{} (induction
hypothesis) that any polynomial less than or equal to degree $2^{n-1}q'$
has a complex root.\\

Let's start by splitting $R(x)$ into all its linear terms like we
did previously, we will also put a factor $a$ out front. This $a$
would correspond to the coefficient of $x^{n}$. This step requires
that we \char`\"{}find\char`\"{} a field, $\mathbb{F}$ over which
we can split the polynomial into its linear factors.\footnote{For completeness, we are finding the smallest field extension of $\mathbb{C}$,
such that $R(x)$ decomposes into linear factors.} This just means, we might not have the tools to take this polynomial
apart where we currently are. So we go to our friends house that has
the tools (find the field) and take it a part there. In math jargon
this would look like: \\

\[
c_{1}(x)c_{2}(x)\cdots c_{n}(x)\bar{c_{1}(x)}\bar{c_{2}(x)}\cdots\bar{c_{d}(x)}
\]
where the $c_{i}(x)$ are linear functions in the field, $\mathbb{F}$.
Now let $x_{1}$ be the root of $c_{1}(x)$ in the field, $\mathbb{F}$,
so on and so forth. So the roots of $R(x)$ in the field $\mathbb{F}$
are $x_{1},\ldots,x_{d}$. If this field, $\mathbb{F}$ were the field
of polynomials with real valued coeffcients, $\mathbb{R}[x]$ we would
be done, but its not. So now we need to figure out how to relate these
roots we found back to the real or complex field. In order to do that
we use the same trickery we used previously.\\

Let $s$ be an arbitrary real number and let $y_{s,i,j}=x_{i}+x_{j}+sx_{i}x_{j}$,
where $1\leq i<j\leq d$. Now define: 
\[
G_{s}(x)=(x-y_{s,1,2})(x-y_{s,1,3})\cdots(x-y_{s,1,d})(x-y_{s,2,3})\cdots(x-y_{s,2,d})\cdots(x-y_{s,d-1,d})
\]
This may be written succinctly as: 
\[
G_{s}(x)=\Pi_{1\leq i<j\leq d}(x-y_{s,i,j})
\]

The $G_{s}(x)$ we saw previously was constructed in the exact same
manner. It is important to know that the coefficients of $G_{s}$
are symmetric in $x_{1},x_{2},\ldots,x_{d}$. This means that the
variables could be interchanged in any way and we would not need to
change the coefficients, in fact, we would have the same polynomial!\footnote{For further investigation read about symmetric polynomials.}
In other words the coefficient of $x_{1}$ would be the same as the
coefficient as $x_{d}$, likewise the coefficient of $xx_{1}$ would
be the same as $xx_{d}$, so on and so forth. It might help to think
of Pascal's triangle. Let's look at a case when $d=2$.

\subsubsection*{Example of $G_{s}(x)$ when $d=2$ }

This example is very similar to the previous example section, but
is included to help concretize the proof. In this example we will
derive $G_{s}(x)$ from the original polynomial 
\[
f(x)=x^{2}+bx+c
\]
where $b,c$ are real valued.

Following the layout of the proof first we find the splitting field
over which $f(x)$ splits. From which we get 
\[
f(x)=(x-x_{1})(x-x_{2})
\]
At this point we don't know what type of numbers $x_{1}$ or $x_{2}$
are. They could be complex, they could be something else altogether.
Let's now look at what $G_{s}(x)$ looks like when $d=2$. Since 
\[
G_{s}(x)=(x-y_{s,1,2})
\]
let's first create $y_{s,1,2}$: 
\[
y_{s,1,2}=x_{1}+x_{2}+sx_{1}x_{2}
\]
so 
\[
G_{s}(x)=x-x_{1}-x_{2}-sx_{1}x_{2}
\]

There are many important features to notice about $G_{s}(x)$. $G_{s}(x)$
is a polynomial in $x$, its degree is one, so it is one degree lower
than our original polynomial $f(x)$, this feature will come in handy
when we use our induction hypothesis. However in order to use our
induction hypothesis we need to know that the coefficients of $G_{s}(x)$
are real valued. The trick we use here is rather clever.

Recall that $f(x)=(x-x_{1})(x-x_{2})$, which if we distribute twice
we get 
\[
f(x)=x^{2}+x(-x_{1}-x_{2})+x_{1}x_{2}
\]
further 
\[
f(x)=x^{2}+bx+x
\]
Therefore

\begin{align*}
b=-x_{1}-x_{2}\\
c=x_{1}x_{2}
\end{align*}

Well we know that $b,c\in\mathbb{R}$ are real valued. Thus $-x_{1}-x_{2},x_{1}x_{2}$
must be real valued. This does not necessarily mean that $x_{1}\in\mathbb{R}$
or that $x_{2}\in\mathbb{R}$ are real valued. For example if $x_{1}=1+i$
and $x_{2}=1-i$ then $-x_{1}-x_{2}=-2$ and $x_{1}x_{2}=2$. But
it does mean that the coefficents of 
\[
G_{s}(x)=x-(x_{1}+x_{2})+x_{1}x_{2}
\]
namely $x_{1}+x_{2}$ and $x_{1}x_{2}$ are real valued. Here you
saw a concrete version but the fact that $x_{1}+x_{2}$ and $x_{1}x_{2}$
are real valued stems from Viéte's formulas, which relate the coefficents
of a polynomial to functions of the polynomial. These functions are
what we have been calling the elementary symmetric polynomials. Now
let's return to the proof.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% This is incorrect!
%To help with this idea of symmetric coefficients, when $d=3$ we have
%$$
%G_s(x) = (x-x_1-x_2-sx_1x_2)(x-x_2-x_3-sx_2x_3)
%$$
%which expands to
%$$
%x^2+x_2x_3+x_1x_3 + x_1x_2 + x_2^2 -xx_3 - 2x x_2-x x_1 + 2sx_1x_2x_3 + sx_2^2 x_3 + s x_1 x_2^2 - s x x_2 x_3 - s x x_1 x_2 + s^2 x_1 x_2^2 x_3
%$$
%Notice how we could interchange $x_1$ and $x_2$ and the coefficients would remain the same.  In this example the coefficients are all real, but we can't assume the same will occur when $d>3$.\\
%The same analysis that applied in the example when $d=2$ and $d=3$ applies when $d>3$, we reduce the degree of the polynomial using $G_s(x)$, check that the coefficients are real then use the induction hypothesis.\\
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% explanation of induction
%In order to do that we need to use induction\footnote{This step is not completely necessary for this example, however understanding this process will help immensely when we turn towards the proof for any polynomial}.    Remember induction is like climbing a ladder, we show that the first rung is true, then we show that if a previous rung existed then the next one does as well.  Suppose the degree of our polynomial was one this will act as our base case.  A polynomial of degree one is a non-horizontal line, which will cross the $x-$axis once, therefore we will have one root for a polynomial of degree one.  Next we will use the induction hypothesis that a polynomial of degree two has two complex/real roots to show that our third degree polynomial has three complex/real roots.
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\subsubsection*{Coefficents of $G_{s}(x)$ are real}

Now back to: 
\[
G_{s}(x)=(x-y_{s,1,2})(x-y_{s,1,3})\cdots(x-y_{s,1,d})(x-y_{s,2,3})\cdots(x-y_{s,2,d})\cdots(x-y_{s,d-1,d})
\]
we need to the coefficients of $G_{s}(x)$ to be real and we need
the degree to be $2^{n-1}q'$, where $q'$ is odd. %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% Delete ?
%We have some motivation to believe that the coefficients of $G_s(x)$ are real.  We will have to take on belief that they are indeed real.\footnote{For the curious, to the best of my knowledge it has not be proven that we may do this.  What the proofs so far claim is that the coefficients of $G_s(x)$ may be viewed as elementary symmetric polynomials.  These polynomials have special formulas which Fran\c{c}ois Vi\'ete discovered in the late 1500's, which enable one to rewrite them as real numbers.  However these formulas only apply to the reals or complex numbers, not necessarily some field extension of the complex field, which is where we are using them.}  
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In order to use our induction step we need the coefficients of $G_{s}(x)$
to be real. Recall that the induction step states: 
\begin{center}
\fbox{\begin{minipage}[c]{40em}%
Polynomials of degree $2^{n-1}q'$ with real valued coefficients have
at least one complex root. %
\end{minipage}} 
\par\end{center}

%In the two examples we saw a concrete version of $G_s(x)$ in these example the terms when multiplied out were composed of symmetric polynomials in the roots, the $x_i$'s.  

Symmetry is going to play an important role, in enabling us to be
able to apply the induction hypothesis. Recall that symmetry in this
situation is when we are able to alternate the coefficents and maintain
equality.

\paragraph*{Symmetric example}

\[
x+y
\]
is symmetric, if I replace $x$ with $y$ and $y$ with $x$ the expression
$y+x$ is equivalent. Likewise 
\[
x+y+z
\]
is also symmetric, we may replace $x$ with $z$ and $z$ with $x$
(or any other combination of variables) and maintain equivalancey.
However 
\[
x^{2}+y
\]
is not symmetric alternating the variables results in 
\[
y^{2}+x\neq x^{2}+y
\]
The left hand side is not equivalent to the right hand side. Let's
take a quick jaunt back to our example and see how this and the induction
step apply there.

\paragraph*{Return to the example for $d=2$}

Before we move on let's make sense of this step with a concrete example.
We want to use the induction hypothesis which loosely stated for this
example is: \\

\emph{Any polynomial with real valued coefficents and degree \char`\"{}less
than'' the original polynomial has at least one complex root.}\footnote{Less than is put in quotations, because stictly speaking this is not
true, we are inducting on the even part of the degree, the $2^{n}$
part. As long as the new polynomial has an $n'<n$ we can utilize
our induction hypothesis.}\\

Let's check everything our induction hypothesis requires. 
\begin{enumerate}
\item Real valued coefficients: Because the $G_{s}(x)$ is symmetric the
coefficents may be related to the coefficents of the original polynomial,
which are real. 
\item Lesser degree: $G_{s}(x)$ is constructed such that it will be \char`\"{}less
than'' the degree of the orginal polynomial.\footnote{see previous footnote} 
\end{enumerate}
Since the criterion for the induction hypothesis is satisfied we know
that $G_{s}(x)$ has at least one complex root, so 
\[
-(x_{1}+x_{2})+sx_{1}x_{2}
\]
must be complex. In order to figure out whether $x_{1}$ or $x_{2}$
is complex we need to analyze $-(x_{1}+x_{2})+sx_{1}x_{2}$. In identifying
that $-(x_{1}+x_{2})+sx_{1}x_{2}$ is complex we did not consider
$s$ at all. Thus, whether or not $x_{1}$ or $x_{2}$ is complex
does not depend on $s$. So if $s=0$ then 
\[
-(x_{1}+x_{2})
\]
must be complex. So $x_{1}+x_{2}$ must be complex. Now let's assume
$x_{1}=u+iv$ and $x_{2}=z+iw$, then if $x_{1}+x_{2}$ is real valued
we must have $iv=iw$ in other words the imaginary parts must cancel
out. Then: 
\begin{align*}
x_{1}x_{2} & =(u+iv)(z+iw)\\
 & =(u+iv)(z-iv)\\
 & =uz-ivu+izu-i^{2}v^{2}\\
 & =uz+v^{2}+i(vz-uv)
\end{align*}
which is complex unless $vz=uv\Rightarrow z=u$ for $v\neq0$.

In order to identify if $x_{1}$ or $x_{2}$ is our complex root we
would need to use the induction hypothesis again, this will be detailed
further in the paper.

\paragraph*{Back to the proof}

We have a very similar situation for the generalized proof. $G_{s}(x)$
is constructed so that the coefficients are symmetric. This stems
from the fact that 
\[
y_{s,i,j}=x_{i}+x_{j}+sx_{i}x_{j}
\]
is symmetric in the $x_{i},x_{j}$. If we then multiply all the combinations
of $x-y_{s,i,j}$, where $i<j$ then we will have polynomial that
is symmetric in all the $x_{i}$'s.

Since the coefficients are symmetric then we can express the additive
and multiplciative combinations of the $x_{1},x_{2},\ldots x_{d}$
as elementary symmetric polynomials\footnote{By the Fundamental Theorem of Symmetric Polynomials},
just as we did in the examples. The elementary symmetric polynomials
relate to the original polynomial and are equivalent to the coefficients
of $R(x)$. The coefficients of $R(x)$ are real, therefore the elementary
symmetic polynomials are real. Likewise the additive and multiplciative
combinations of the $x_{1},x_{2},\ldots x_{d}$ are also real.\\

Put another way we related the coefficients of $G_{s}(x)$ to the
coefficents of our original polynomial, $R(x)$. Since the coefficents
of $R(x)$ are real, the coefficients of $G_{s}(x)$ must be real.
Note that the coefficents of $G_{s}(x)$ are additive and multiplicative
combinations of $x_{1},x_{2},\ldots x_{d}$. These additive and multiplicative
combinations are what are known as the elementary symmetric polynomials.\\

So we have show that the coefficents of $G_{s}(x)$ are real. In order
to use the induction hypothesis we need to figure out the degree of
$G_{s}(x)$.

\subsubsection*{Degree of $G_{s}(x)$}

Let's try counting how many terms there are in $G_{s}(x)$.

There are $d-1$ terms that have $i=1$, then there are $d-2$ terms
that have $i=2$, because $i<j$. So there are $d-3$ terms that have
$i=3$ and so on. 
\[
G_{s}(x)=\underbrace{(x-y_{s,1,2})(x-y_{s,1,3})\cdots(x-y_{s,1,d})}_{d-1}\underbrace{(x-y_{s,2,3})\cdots(x-y_{s,2,d})}_{d-2}\cdots(x-y_{s,d-1,d})
\]

Thus we have a decreasing sequence, since we are trying to find the
degree of $G_{s}(x)$ we take the $d-1$ terms and add them to $d-2$
terms, so on and so forth. This gives us a familiar series: 
\[
1+2+3+4+\cdots+(d-2)+(d-1)
\]

The trick for finding the sum of this sequence is often attributed
to Gauss. First define: 
\[
S_{n}=1+2+3+4+\cdots+(d-2)+(d-1)
\]

then rewrite $S_{n}$ as 
\[
S_{n}=(d-1)+(d-2)+\cdots+4+3+2+1
\]

add $S_{n}$ to itself, but we need to be clever here, take notice
of how the additive terms were grouped 
\[
2S_{n}=(1+(d-1))+(2+(d-2))+(3+(d-3))+\cdots((d-2)+2)+((d-1)+1)
\]

Perform the arithmetic inside each set of parenthesis 
\[
2S_{n}=d+d+d+\cdots+d+d
\]

Recall that there were $d-1$ terms in $S_{n}$ and we did nothing
to alter that since we paired up each component so: 
\[
2S_{n}=\underbrace{d+d+d+\cdots+d+d}_{d-1}
\]

A model to make multiplication understandable is that multiplication
is repeated addition, let's apply that model here: 
\[
2S_{n}=d(d-1)
\]

Which leaves us with our result: 
\[
S_{n}=\frac{d(d-1)}{2}
\]

So the sum of this series is $\frac{d(d-1)}{2}$. Recall that 
\[
d=2^{n}q
\]
so 
\[
\frac{d(d-1)}{2}=\frac{2^{n}q(d-1)}{2}=2^{n-1}q(d-1)
\]
but $q(d-1)$ is an odd number so 
\[
2^{n-1}q(d-1)=2^{n-1}q'
\]

Thus $G_{s}(x)$ has real coefficents and a degree less than or equal
to $2^{n-1}q'$, therefore we can apply the induction hypothesis.

\subsection*{Our Complex root}

Since the degree of $G_{s}(x)$ is $2^{n-1}q'$ and it has real coefficents
the induction hypothesis applies: 
\begin{center}
\fbox{\begin{minipage}[c]{40em}%
Polynomials of degree $2^{n-1}q'$ with real valued coefficients have
at least one complex root. %
\end{minipage}} 
\par\end{center}

Now we can assume $G_{s}(x)$ has at least one complex root in order
to show that $R(x)$ has at least one complex root. Without a loss
of generality let us say that the root occurs when $i=r$, $j=t$,
therefore 
\[
y_{s,r,t}=x_{r}+x_{t}+sx_{r}x_{t}
\]
is complex for some $s$. So suppose 
\[
s=0
\]
then 
\[
x_{r}+x_{t}
\]
must be complex. Now Suppose 
\[
x_{r}+x_{t}
\]
is real, that would mean that the imaginary parts cancel out, but
when we take 
\[
x_{r}x_{t}
\]
the imaginary parts would not cancel out, therefore $x_{r}x_{t}$
is also complex for some $s$. Since both $x_{r}+x_{t}$ and $x_{r}x_{t}$
are complex numbers. To show that $x_{r}$ or $x_{t}$ is complex
we consider the equation 
\[
x^{2}-(x_{r}+x_{t})x+x_{r}x_{t}
\]
Notice that the coefficents of this equation are complex. So let's
define them as such 
\begin{align*}
x_{r}+x_{t}=u+iv\\
x_{r}x_{t}=w+iz
\end{align*}
where $u,v,z,w$ are real valued. Substituting these values into the
equation gives us: 
\[
x^{2}-(u+iv)x+w+iz
\]
Let's apply the quadratic formula to this equation and see what we
get. 
\begin{align*}
x & =\frac{u+iv\pm\sqrt{(u+iv)^{2}-4(w+iz)}}{2}\\
 & =\frac{u+iv\pm\sqrt{u^{2}+2iuv-v^{2}-4w-4iz}}{2}\\
 & =\frac{u+iv\pm\sqrt{\underbrace{u^{2}-v^{2}-4w}_{p}+i(\underbrace{2uv-4z}_{q})}}{2}\\
 & =\frac{u+iv\pm\sqrt{p+iq}}{2}\\
 & =\frac{u+iv\pm(p'+iq')}{2}\\
\end{align*}

Thus the roots of $x^{2}-(u+iv)x+w+iz=x^{2}-(x_{r}+x_{t})x+x_{r}x_{t}$
are complex. Notice in the third step we set $u^{2}-v^{2}-4w=p$ and
$2uv-4z=q$. Further a critical and subtle part of the above is that
the square root of complex numbers is complex, i.e if $\sqrt{p+iq}$
is complex then there exist some $p',q'$ such that $p'+iq'=\sqrt{p+iq}$.
A proof of this is included in the appendix.\\

Great the roots of $x^{2}-(x_{r}+x_{t})x+x_{r}x_{t}$ are complex,
but how does that help us in figuring out if $x_{r}$ or $x_{t}$
is complex. Well let's now find the roots of $x^{2}-(x_{r}+x_{t})x+x_{r}x_{t}$
\begin{align*}
x & =\frac{x_{r}+x_{t}\pm\sqrt{(x_{r}+x_{t})^{2}-4x_{r}x_{t}}}{2}\\
 & =\frac{x_{r}+x_{t}\pm\sqrt{x_{r}^{2}+2x_{r}x_{t}+x_{t}^{2}-4x_{r}x_{t}}}{2}\\
 & =\frac{x_{r}+x_{t}\pm\sqrt{x_{r}^{2}-2x_{r}x_{t}+x_{t}^{2}}}{2}\\
 & =\frac{x_{r}+x_{t}\pm\sqrt{(x_{r}-x_{t})^{2}}}{2}\\
 & =\frac{x_{r}+x_{t}\pm(x_{r}-x_{t})}{2}\\
 & =x_{r},x_{t}
\end{align*}

So the roots of $x^{2}-(x_{r}+x_{t})x+x_{r}x_{t}$ are non other than
$x_{r}$ and $x_{t}$, which must be complex.

\subsection*{Conclusion}

There is a lot involved in this proof, so let's recap what has been
done. We started with some polynomial $C(x)$ this polynomial had
complex coefficents, we multiplied $C(x)$ with its conjugate $\bar{C(x)}$,
which left us with a polynomial with real coefficents, we called this
polynomial $R(x)$. Now our goal was to show that this polynomial
$R(x)$ had at least one complex root. We found that complex root
by inducting on the even part of the degree of the polynomial $R(x)$.
Induction enabled us to assume that polynomials of \char`\"{}lesser''
degree do have at least one complex root. We were able to construct
a polynomial, $G_{s}(x)$ that not only satisfied the induction hypothesis,
but whose roots were the roots of $R(x)$. So when we find $G_{s}(x)$'s
complex root, we also found $R(x)$ complex root.

\subsection*{Every single variable polynomial of degree $n$ has exactly $n$
roots}

I stated earlier that the statement:

\fbox{\begin{minipage}[c]{30em}%
Every single variable polynomial of degree $n$ has exactly $n$ roots %
\end{minipage}}\\

Is really a corollary of what I will be referring to as the Fundamental
Thereom of Algebra, which is:\\

\emph{Any polynomial with complex coefficents greater than degree
zero has at least one complex root}.\\

Let's prove this using the Fundamental Thereom of Algebra.

Let 
\[
f_{0}(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{1}x+a_{0}
\]
where the $a_{i}$ are real (it still holds if they are complex).
we know by the Fundamental Thereom of Algebra that $f_{0}(x)$ has
at least one complex root, let's call it $r_{1}$. Then we can factor
$r_{1}$ out of $f_{0}(x)$ using long division. We know this step
is legitamate, because in both the field of reals and complex we can
divide polynomials by linear factors. In other words since they are
fields, we can divide.\\

Let's denote the new function we obtain when we divide $f_{0}(x)$
by $(x-r_{1})$ as $f_{1}(x)$. Further when we compute this division
we will have no remainder. If we did have a remainder then 
\[
f_{0}(r_{1})=\underbrace{(r_{1}-r_{1})f_{1}(r_{1})}_{\text{zero}}+\underbrace{\text{remainder}}_{\text{not zero}}
\]
would not equal zero. Also the degree of $f_{0}(x)$ was $n$, therefore
the degree of $f_{1}(x)$ must be $n-1$, since 
\[
\underbrace{f_{0}(x)}_{\text{degree }n}=\underbrace{(x-r_{1})}_{\text{degree }1}f_{1}(x)
\]
since exponents add $f_{1}(x)$ must have degree $n-1$.\\

Now we apply the Fundamental Thereom of Algebra to $f_{1}(x)$ this
gives us a root we will call $r_{2}$. We divide $f_{1}(x)$ by $(x-r_{2})$
to get $f_{2}(x)$. Again the remainder must be zero and the degree
of $f_{2}(x)$ will be $n-2$.\\

It's important to see that we have found two roots and we've reduced
the polynomial by two degrees. Therefore if we repeat this process
$n$ times we will have exactly $n$ roots and we will have reduced
the polynomial to degree $n-n=0$, for which no roots will exist.
Thus we have found exactly $n$ roots for a polynomial of degree $n$.

%$y_{s,1,1} = 2x_1 + s x_1^2$ and $y_{s,1,2} = x_1 +x_2 + s x_1 x_2$ and $y_{s,2,2} = 2x_2 + s x_2^2$.  Therefore 
%\begin{align*}
%G_s(x) &= (x-y_{s,1,1})(x-y_{s,1,2})(x-y_{s,2,2}) \\
%&= x^3 \\
%& \hspace{1em} -(s(x_1^2+x_1 x_2+x_2^2 \\
%& \hspace{10em} +3(x_1+x_2))x^2 \\
%& \hspace*{1em} + (s^2(x_1 x_2^3 + x_1^3 x_2) \\
%& \hspace{9em} + s(x_1^3 +x_2^3 \\
%& \hspace{15em} + 5(x_1 x_2^2 + x_1^2 x_2)) \\
%& \hspace{23em} +2(x_1^2+x_2^2) + 8x_1 x_2)x \\
%& \hspace*{1em} -s^3 x_1^3 x_2^3 \\
%& \hspace{5em} - 3s^2(x_1^2x_2^3 + x_1^3 x_2^2) \\
%& \hspace{14em} -2s(x_1 x_2^3 + 4x_1^2 x_2^2 + x_1^3 x_2) \\
%& \hspace{26em} -4(x_1 x_2^2 + x_1^2 x_2)
%\end{align*}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% Maybe include? probably not
%\section*{So What? An argument for the Fundamental Theorem of Algebra}  
%The Fundamental Theorem of Algebra states that there is at least one complex root.  What if there was not?  In other words let's suppose that every equation has a solution, and the solution is not in the complex numbers, where would it be?
%\subsection*{A tour of different fields}
%\emph{Take readers on a tour of ordered and non-ordered fields}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\begin{appendices}

\section*{The Mapping $\Phi$}

Let $f(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{0}$, where $a_{i}\in\mathbb{Z}$,
for $i=0,1,2,\ldots,n$.\\

Define: 
\[
\Phi_{b}(f(x))=\int f(x)\delta(b-x)dx=f(b)
\]
where $\delta$ is the dirac delta, and $b$ is the base of the integer
representation we are mapping into.

So if we were mapping into the base-10 representation of the integers,
denoted $\mathbb{Z}_{10}$, we would have: 
\[
\Phi_{10}(f(x))=\int f(x)\delta(10-x)dx=f(10)
\]
It should be stated that $\Phi$ maps the polynomials with integer
coefficients to the integers.

\subsection*{$\Phi$ preserves addition and multiplication}

%In order to show that $\Phi$ is a ring homomorphism it is necessary to show that it is closed under addition and multiplication, \emph{but I also need to show $\Phi(1_F) = 1$}.

Given two polynomials with integer coefficients $f_{1},f_{2}$.

\paragraph*{Preserves addition}

\begin{align*}
\Phi_{b}(f_{1}(x)+f_{2}(x)) & =\int(f_{1}(x)+f_{2}(x))\delta(b-x)dx\\
 & =\int f_{1}(x)\delta(b-x)dx+\int f_{2}(x)\delta(b-x)dx\\
 & =f_{1}(b)+f_{2}(b)\\
 & =\Phi_{b}(f_{1}(x))+\Phi_{b}(f_{2}(x))
\end{align*}


\paragraph{Preserves multiplication}

\begin{align*}
\Phi_{b}(f_{1}(x)f_{2}(x)) & =\int f_{1}(x)f_{2}(x)\delta(b-x)dx\\
 & =f_{1}(b)f_{2}(b)-\int f'_{1}(x)g_{2}(x)dx\text{ By integrating by parts }\\
 & =f_{1}(b)f_{2}(b)\\
 & =\Phi_{b}(f_{1}(x))\Phi_{b}(f_{2}(x))
\end{align*}

Where $\int f'_{1}(x)g_{2}(x)dx=0$ because $g_{2}(b)=f_{2}(b)$\footnote{$f_{2}(b)\in\mathbb{Z}$}
when $x=b$, but $g_{2}(x)$ is zero everywhere else, so the integral
has measure zero.

\section*{Polynomials are continuous}

Below is a proof that polynomials are continuous. In other words,
in the real plane a polynomial function is continuous at every point,
therefore it is continuous on every interval in $\mathbb{R}$.

We need the product rule for limits. Let $f,g$ be real valued $\Lim{x\rightarrow c}f(x)=l$
and $\Lim{x\rightarrow c}g(x)=k$. Then: $\Lim{x\rightarrow c}(f(x)g(x))=lk$

We will also need to remember the combined sum rule for limits: Let
$f,g$ be real valued $\Lim{x\rightarrow c}f(x)=l$ and $\Lim{x\rightarrow c}g(x)=k$.
Let $\lambda,\kappa\in\mathbb{R}$. Then $\Lim{x\rightarrow c}(\lambda f(x)+\kappa g(x))=\lambda l+\kappa k$

Consider the function $l(x)=x$. Then $\Lim{x\rightarrow c}l(x)=c$.
Then by applying the product rule for limits to $\Lim{x\rightarrow c}l(x)l(x)=\Lim{x\rightarrow c}x^{2}=c^{2}$.
We can continue applying this rule so that for any value $d\in\mathbb{N}$
we have $\Lim{x\rightarrow c}x^{d}=c^{d}$. Let $P(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{1}x+a_{0}$.
Now by applying the combined sum rule to $P(x)$ we get $\Lim{x\rightarrow c}P(x)=\Lim{x\rightarrow c}a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{1}x+a_{0}=a_{n}c^{n}+a_{n-1}c^{n-1}+\cdots+a_{1}c+a_{0}=P(c)$.
Therefore $P(x)$ is continuous for any value $c$.

%Using induction, first prove that a polynomial of degree 1 is continuous.  Let $m,b \in \mathbb{R}$, and let $f$ be a real function defined as $f(x) = m x + b$.  Then we want to show that $f$ is continuous at every real number $c \in \mathbb{R}$.

%Assume $m \neq 0$, let $\epsilon > 0$, and $\delta = \frac{\epsilon}{|m|}$.  Then whenever $|x-c| < \delta$.
%\begin{align*}
%|f(x)-f(c)| &= |mx+b -mc -b| \\
%&= |m(x-c)| \\
%&= |m||x-c| \\
%&< |m| \delta \\
%= \epsilon
%\end{align*}
%Therefore we have found a $\delta$ for a given $\epsilon$ so that $|f(x)-f(c)|< \epsilon$ whenever $|x-c| < \delta$.  The case when $m=0$, follows similar, but simpler logic.

\section*{Proof of Intermediate Value Theorem}

Below is a proof of the Intermediate Value Theorem.

Consider a function f

\section*{Proof the square root of a complex number is complex}

\footnote{Thanks to \url{http://math.stackexchange.com/questions/883030/proof-for-complex-numbers-and-square-root}}
Let $w=s(\cos(\theta)+i\sin(\theta))$ be a non-zero complex number
such that $w^{2}=z$. Therefore: 
\[
w^{2}=(s(\cos(\theta)+i\sin(\theta)))^{2}=s^{2}(\cos(2\theta)+i\sin(2\theta))=r(\cos(\theta')+i\sin(\theta')
\]
by De Moivre's theorem.

$\cos(2\theta)=\cos(\theta')$ so $2\theta=\pm\theta'+2n\pi$, where
$n$ is an integer

Similarly $\sin(2\theta)=\sin(\theta')$ so $2\theta=\pm\theta'+2l\pi$,
where $l$ is an integer.

\emph{TBD}

\end{appendices} 
\end{document}





%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% old 5hr clock arithmetic stuff
%
%Let's pick the two's column again, what is $2\div2$ rewriting this
%becomes $2\cdot?=2$ which looking up in the table is one. What about
%$2\div1$? Rephrasing as multiplication is $2\cdot?=1$ thus two divided
%by one is three. Or phrased another way three is the inverse of two.\\
%
%It was not possible to divide in the ring with four elements. But
%when an element was added making the size of the field prime, division
%was unique (and it could be called a field). This is the idea of a
%field extension. I start with some (playing) field and I make the
%field larger so that it will satisfy some additional properties.
%
