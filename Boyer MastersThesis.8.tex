%% LyX 2.2.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt]{article}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose}
\setcounter{secnumdepth}{0}
\usepackage{wrapfig}
\usepackage{calc}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[unicode=true,
 bookmarks=false,
 breaklinks=false,pdfborder={0 0 1},backref=section,colorlinks=false]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.


%\usepackage{exsheets}
\usepackage[toc,page]{appendix}
%\usepackage{amsthm} 
\usepackage{array}
\mathtoolsset{showonlyrefs} 
\usepackage{graphicx}
\usepackage{listings}
%\usepackage{indentfirst}
%\usepackage{setspace}
%\doublespacing
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{commath}% Used for \abs{}
\usepackage{dcolumn}
\newcolumntype{2}{D{.}{}{2.0}}


\usepackage{pgf}
\usepackage{tikz}\usepackage{mathrsfs}
\usetikzlibrary{arrows}


\newcommand{\ssol}{\vspace{3em}}
\newcommand{\lsol}{\vspace{10em}}
\newcommand{\blnk}{{\underline {\hspace{1.5in}}}}
\newcommand{\Lim}[1]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle \lim_{#1}\;$}}}






\makeatother

\begin{document}

\section{Introduction}
In mathematics, symmetry is often analyzed, utilized, or identified.  A general/laymans definition of symmetry could be reduced to: features of objects which are similar or have an exact correspondence.  This paper sets out to help the reader identify the symmetry between the integers and polynomials with real coefficients.  This will be achieved by first analyzing symmetries between the integers and polynomials with real coefficients.  A function, $\Phi$ will be introduced that allows us to move between polynomials with integer coefficients and the integers.  We will then see parallels between prime numbers and irreducible polynomials, a field extension in clock arithmetic and in the polynomials, and lastly in the fundamental theorem of arithmetic and the fundamental theorem of algebra.  An exposition of an algebraic proof of the fundamental theorem of algebra then utilizes symmetry in order to conclude that that any polynomial with complex coefficients has at least one complex root.  Symmetries are abound in nature and mathematics, this paper seeks to expose one of the beautiful mathematical symmetries at an undergraduate college level.



\section{Comparing the integers and the polynomials with coefficients in the
rational, real or complex number fields}

Suppose I want to solve a polynomial equation 
\[
\frac{a_{d}}{b_{d}}x^{d}+\frac{a_{d-1}}{b_{d-1}}x^{d-1}+\ldots+\frac{a_{0}}{b_{0}}=0
\]
with rational numbers $z_{i}=\frac{a_{i}}{b_{i}}$ (where $a_{i},b_{i}$
are integers) as coefficients. I could turn it into a polynomial equation
with integer coefficients just by multiplying both sides of the above
equation by a common multiple of $\left\{ b_{d},b_{d-1},\ldots,b_{0}\right\} $.
So every root of a polynomial with rational coefficients is a root
of a polynomial with integer coefficients. A deeper fact, called Gauss's
Lemma, says that any factorization of a polynomial 
\[
a_{d}x^{d}+a_{d-1}x^{d-1}+\ldots+a_{0}
\]
with integer coefficients $a_{i}$ that cannot be factored into two
polynomial factors with integer coefficients cannot be factored into
two polynomial factors with rational coefficients. These two facts
led mathematicians to study systems of polynomials with coefficients
in a number system in which you could add, subtract, multiply $and$
divide. They called such a system a $field$. Besides the field of
rational numbers, denoted $\mathbb{Q}$, other examples of fields
are the field $\mathbb{R}$ of real numbers and the field $\mathbb{C}$
of complex numbers. They also noticed that the system $\mathbb{F}\left[x\right]$
of polynomials with coefficients in a field $\mathbb{F}$ behaves
a lot like the most familiar number system, the integers. Both have
unique factorization into prime factors, greatest common divisors,
least common multiples, etc.

\section{The Mapping $\Phi_{b}$}

What I want to do is create a function (also called a mapping) that
takes a polynomial with integer coefficients to the (corresponding)
number system formed by its coefficients. For example I could ask
that the mapping substitute an integer $b$ for every \char`\"{}$x$'', thereby taking the given polynomial with integer coefficients to
an integer.
As an example of this, if $b=10$, the mapping would send $2x^{2}+3x+4\xrightarrow[\Phi_{10}]{}234$.
It is common to use the letter $\mathbb{Z}$ to denote the integer
number system and $\mathbb{Z}\left[x\right]$ to denote the ring of
polynomials with integer coefficients.

\subparagraph{Example}

We would then denote the function (mapping) described above as 
\[
\begin{array}{c}
\Phi_{10}:\mathbb{Z}[x]\rightarrow\mathbb{Z}\\
f(x)\mapsto f(10).
\end{array}
\]
In other words, if $f(x)=5x^{4}+4x^{3}+2x+1$,

\[
\Phi_{10}(f(x))=f(10)=5(10)^{4}+4(10)^{3}+2(10)+1=54021.
\]

More generally 
\[
\Phi_{b}(f(x))=f(b)
\]


\subsection*{Properties of $\Phi_{b}$ }

The following proofs use two polynomials $f_{1},f_{2}$, with coefficients
in any of the number systems $\mathbb{Z}$, $\mathbb{Q}$, $\mathbb{R}$
or $\mathbb{C}$. 
\begin{itemize}
\item $\Phi_{b}$ preserves addition 
\end{itemize}
\emph{Proof:} 
\begin{align*}
\Phi_{b}(f_{1}(x)+f_{2}(x)) & =f_{1}(b)+f_{2}(b)\\
 & =\Phi_{b}(f_{1}(x))+\Phi_{b}(f_{2}(x))
\end{align*}

\begin{itemize}
\item $\Phi_{b}$ preserves multiplication 
\end{itemize}
\emph{Proof:} 
\begin{align*}
\Phi_{b}(f_{1}(x)f_{2}(x)) & =f_{1}(b)f_{2}(b)\\
 & =\Phi_{b}(f_{1}(x))\Phi_{b}(f_{2}(x))
\end{align*}

\begin{itemize}
\item A polynomial maps to zero if and only if the polynomial is divisible
by $x-b$ 
\end{itemize}
\emph{Proof:}

Given $f(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{0}$, and a number
$r$ such that 
\[
f(r)=0.
\]

By long division of polynomials 
\[
a_{n}x^{n}+\cdots+a_{0}=(x-r)\text{·}(c_{n-1}x^{n-1}+\cdots c_{0})+\text{constant}
\]
Substituting $x=r$ into the both sides of the equation above, 
\[
0=0+\text{constant}.
\]
So by necessity $\text{constant}=0$, therefore 
\[
f(x)=(x-r)(c_{n-1}x^{n-1}+\cdots c_{0}).
\]


\subsection*{A rough comparison of prime factorization}

This section will compare prime factorization in the ring of integers
and the ring of polynomials with coefficients in a field. Prime factorization
is decomposing something into its constituent primes. Among the integers
we have the \emph{the fundamental theorem of arithmetic}, which says
that every positive integer has a unique prime factorization. In math
speak this states that any positive integer $k\geq2$, $k$ may be
rewritten as 
\[
k=p_{1}^{l_{1}}p_{2}^{l_{2}}\cdots p_{n}^{l_{n}}
\]
where the $p_{i}$'s are the $n$ distinct prime factors, each of
order $l_{i}$. One way to understand this is to think of it as taking
a positive integer, breaking it up into several unique parts, multiplying
those parts together and getting the same positive integer back. \\

Do the polynomials with coefficients in a field $\mathbb{F}$ have
a similar analogue? In this set-up, polynomials of degree zero, that
is, polynomials with only constant terms $a_{0}$, play the role of
$\text{\textpm1}$ in the integers, so that prime factors are always
polynomials of degree $d\geq1$. In grade school, polynomials are
often factored by breaking up the polynomial into several terms, each
of which can't be broken up further. Similarly these factored terms
may be multiplied together to retrieve the original polynomial.

For example in $\mathbb{Q}\left[x\right]$ 
\[
x^{2}-1=\left(x+1\right)\left(x-1\right)
\]
but 
\[
x^{2}-2
\]
is \char`\"{}prime'' in $\mathbb{Q}\left[x\right]$ since $\sqrt{2}$
is not in the number system $\mathbb{Q}$. Since $x^{2}-2$ is a polynomial
not an integer instead of prime it is more common to say irreducible.
In $\mathbb{R}\left[x\right]$, $x^{2}-2$ is reducible since $\sqrt{2}$ is an element of the reals, $\mathbb{R}$, i.e., 
\[
x^{2}-2=\left(x+\sqrt{2}\right)\left(x-\sqrt{2}\right).
\]
Hoover the prime factorization of $f(x)=x^{2}+1$ in $\mathbb{R}\left[x\right]$
is $x^{2}+1$. Since the square of any real number is greater or equal
to zero, 
\[
x^{2}+1
\]
is irreducible in $\mathbb{R}\left[x\right]$. However in the system of
polynomials $\mathbb{C}\left[x\right]$, $x^{2}-1$ factors as 
\[
x^{2}+1=\left(x+i\right)\text{·}\left(x-i\right).
\]

Mathematicians decided that a polynomial $f\left(x\right)$ in $\mathbb{F}\left[x\right]$
is called irreducible if the polynomial cannot be written as a product
\[
f\left(x\right)=g\left(x\right)\text{·}h\left(x\right)
\]
where $g\left(x\right)$ and $h\left(x\right)$ are polynomials in
the same system $\mathbb{F}\left[x\right]$ and the degrees of $g\left(x\right)$
and $h\left(x\right)$ are both less than the degree of $f\left(x\right)$.
In other words, $f\left(x\right)$ is irreducible if division by any
polynomial $g\left(x\right)$ in the same system $\mathbb{F}\left[x\right]$
always leaves a remainder. So the only polynomials that divide an
irreducible polynomial are the irreducible polynomial itself and the
constant polynomials $a_{0}$. Remind you of anything?

\subparagraph{Exercise:}

If you don't know already, try to figure out how to do prime factorization
in any system $\mathbb{F}\left[x\right]$ of polynomials where $\mathbb{F}$
is one of our fields. \\

The 'miracle' is that every polynomial $f(x)$ in $\mathbb{R}\left[x\right]$
can be factored into irreducible factors of degrees $1$ and $2$.
But then each irreducible factor 
\[
ax^{2}+bx+c
\]
of $f(x)$ with $a$, $b$ and $c$ real can be considered as a polynomial
in $\mathbb{C}\left[x\right]$. (Remember that real numbers are also
complex numbers, it's just that their imaginary part is zero). It
is then possible to factor $ax^{2}+bx+c$ in $\mathbb{C}\left[x\right]$
by the quadratic formula, 
\[
a\left(x-\frac{-b+\sqrt{b^{2}-4ac}}{2a}\right)\left(x-\frac{-b-\sqrt{b^{2}-4ac}}{2a}\right).
\]
In fact, any polynomial $f(x)$ in $\mathbb{R}\left[x\right]$ be
factored completely in $\mathbb{C}\left[x\right]$ as 
\[
f(x)=a_{d}\text{·}(x-r_{1})^{l_{1}}(x-r_{2})^{l_{2}}\cdots(x-r_{i})^{l_{i}},
\]
the same is true for any $f(x)$ in $\mathbb{C}\left[x\right]$. We
know this fact as the \emph{Fundamental Theorem of Algebra}. The Fundamental
Theorem of Algebra states that the only irreducible polynomials in
$\mathbb{C}\left[x\right]$ are the polynomials of degree one! So,
roughly speaking, every $d$-th degree polynomial in $\mathbb{C}\left[x\right]$
has $d$ complex roots. These facts considered together with long
division of polynomials enable use to rewrite the polynomial 
\[
f(x)=a_{d}x^{d}+a_{d-1}x^{d-1}+\ldots+a_{0}
\]
as a product 
\[
f\left(x\right)=a_{d}\left(x-s_{1}\right)\text{·}\cdots\text{·}\left(x-s_{d}\right).
\]
I.e., if the $r_{i}$'s are the roots of $f(x)$ and the $l_{i}$
are the multiplicities of the roots, then each root $r_{i}$ occurs
in the list 
\[
s_{1},s_{2},\ldots,s_{d}
\]
exactly $l_{i}$ times.

\subparagraph{Exercise:}

For some small values of $d$, multiply out the right-hand-side of
the equality 
\[
a_{d}x^{d}+a_{d-1}x^{d-1}+\ldots+a_{0}=a_{d}\left(x-s_{1}\right)\text{·}\cdots\text{·}\left(x-s_{d}\right)
\]
so that you can give a formula for each quantity $\frac{a_{i}}{a_{d}}$
as a function of the quantities $s_{1},s_{2},\ldots,s_{d}$. Can you
guess the general formula for all values of $d$? Those formulas are
called the elementary symmetric functions of $s_{1},s_{2},\ldots,s_{d}$.
They will figure in an important way later on in this story.

\section{Toward a proof of the Fundamental Theorem of Algebra}

The Fundamental Thereon of Algebra is critical to much in algebra,
and the proof is often waved away as being beyond the scope of college
mathematics courses. In order to motivate the proof we start
with an example that highlights necessary components of the proof,
while giving some concreteness.

\section*{How many roots does $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$ have?}

Consider the following polynomial 
\[
p(x)=x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
how many roots does it have? Does it help if $r_{1},r_{2},r_{3}$
are real numbers? Let's suppose that and prove that $p(x)$ has three
roots in the complex field. We will take as a given that we know that,
as $x$ goes to $+\infty$, $p(x)$ becomes positive and, as $x$
goes to $-\infty$, $p(x)$ becomes negative. A slightly more complicated
fact is the fact that $p(x)$ is a continuous function of $x$, which
is often informally explained as the fact that you can draw the graph
of 
\[
y=p\left(x\right)
\]
without lifting your pencil from the page. So as a consequence of
the fact that $p(x)$ is continuous, your pencil cannot go from negative
$y$ to positive $y$ without crossing a place where $y=p\left(x\right)=0$.
That is, there is a real number $x$ where $p\left(x\right)=0$. So
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
has a real root $s_{1}$. As we have shown earlier, this means that
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}=\left(x-a_{1}\right)\left(x^{2}+bx+c\right).
\]
But now, again as had already been shown, this means that $x^{2}+bx+c$
can be factors into linear factored in $\mathbb{C}\left[x\right]$
using the quadratic formula.

Suppose now that we don't know that the coefficients of $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$
are real but just lie in some field $\mathbb{F}$, so the operations
of addition, subtraction, multiplication, and division hold. And suppose
we know that its roots lie in that same field. Let's call these roots
$a_{1},a_{2},a_{3}$.

Typically one would rewrite 
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
as 
\[
(x-a_{1})(x-a_{2})(x-a_{3}).
\]

Why can polynomials be written as linear factors? Recall the previous
section on the relationship between prime numbers and irreducible
factors of polynomials. It turns out that it always is possible to
factor a polynomial in $\mathcal{\mathbb{F}}\left[x\right]$ if $\mathbb{F}$
is a large enough field.

\subsection*{Field Extension}

The idea of a field extension boils down to increasing the set of
numbers one is able to use, while maintaining the properties of addition,
subtraction and multiplication and division. For example the Pythagoreans
worked in the field of rational numbers, therefore when they came
across $\sqrt{2}$ they did not believe it could be a solution. We
can now extend our field to all real values and now $\sqrt{2}$ is
a perfectly fine solution.



\subsubsection*{5-hour Clock Arithmetic}

The basics of field extension can be most easily understood by seeing
first how the integers can be turned into a field through the world
of clock (modular) arithmetic. Consider an algebraic structure that
consists of five elements, $\{0,1,2,3,4\}$. I will call this a 5-clock
arithmetic, because its arithmetic is just like what we use for the
hour hand on the clock, except that there are only five hours in our
'day,' that is, the set of hours has only five elements. It is possible
to add in this structure, for example, $1+1=2$, $1+2=3$, and $0+n=n$
thus 0 is the additive identity. But what about $4+1=$? On the 5-hour
clock we are forced to make $4+1=0$ since the hour after four o'clock
is the place where the hour-hand starts over . In other words in this
field the number 5 is mapped to the number 0, this implies the number
6 would be mapped to 1, and $7\rightarrow2$, $8\rightarrow3$, $9\rightarrow4,\ldots$.
Using these mappings and induction I can add any values in this structure
and still have values in this structure. For example addition in 5-clock
arithmetic looks like: 
\[
2+4=1
\]
\[
3+4=2
\]
\[
1+2+3+4=0
\]
\[
3+3+3+3+4=1
\]
so on and so forth. The last equation in the series highlights repeated
addition, I can use this model of multiplication (repeated addition)
to understand how multiplication functions in 5-clock arithmetic.
\begin{align*}
3+3+3+3+4 & =1\\
4\cdot3+4 & =1\\
4\cdot(3+1) & =1\text{ using the distributive property}\\
4\cdot4 & =1
\end{align*}
If $4\cdot4=1$, then multiplying both sides by 4, I get 
\begin{align*}
\underbrace{4\cdot4}_{=1}\cdot4 & =4\\
1\cdot4 & =4
\end{align*}
There is a multiplicative identity in this structure, it is the number
1.

Let's take a closer look at multiplication of twos. 
\begin{align*}
2\cdot0=0\\
2\cdot1=2\\
2\cdot2=4\\
2\cdot3=1\\
2\cdot4=3\\
\end{align*}

I have a multiplicative inverse for 2 it is 3. I.e., $2\cdot3=1$,
therefore $2^{-1}=3$. Likewise there is a multiplicative inverse
for $1,3,4$ namely $1^{-1}=1$, $3^{-1}=2$, $4^{-1}=4$. I will
use the multiplicative inverse to understand division in this system.\\

What does $2\div4=?$ in 5-clock arithmetic?  Rephrasing as a multiplication
problem, $2=?\cdot4$. I know 4 has a multiplicative inverse namely
itself, so I can multiply the previous equation by 4, 
\[
2\cdot4=?\cdot4\cdot4
\]
$2\cdot4=3$ and $4\cdot4=1$ in this clock 5 arithmetic, therefore
$3=?$. So 
\[
2\div4=3
\]
I can divide, because I have a multiplicative inverse.

A more efficient method for computing division exists. In fact it
was the Egyptians who were one of the first peoples to record this
more efficient method of division. Similar to the above process they
framed it as a multiplication problem with the multiplier (or multiplicand)\footnote{Why does it not matter if it is the multiplier or multiplicand that
is missing?} missing. But then they used a multiplication table to deduce the
solution.



From here it will help if we have a times-table to reference: 
\begin{center}
\global\long\def\arraystretch{1.3}
 \setlength{\doublerulesep}{0pt} %
\begin{tabular}{r||2|2|2|2|2}
$\times$  & 0  & 1  & 2  & 3  & 4 \tabularnewline
\hline 
\hline 
0  & 0  & 0  & 0  & 0  & 0 \tabularnewline
\hline 
1  & 0  & 1  & 2  & 3  & 4 \tabularnewline
\hline 
2  & 0  & 2  & 4  & 1  & 3 \tabularnewline
\hline 
3  & 0  & 3  & 1  & 4  & 2 \tabularnewline
\hline 
4  & 0  & 4  & 3  & 2  & 1 \tabularnewline
\hline 
\end{tabular}
\par\end{center}

What the Egyptians did was re-frame the division problem as multiplication,
then read off the answer. For example, $3\div3=?$ re-framed as multiplication
becomes $3=?\cdot3$. Now I look at the times table and see that 4
times 3 gives me 3, therefore $?=4$ and $3\div3=4$. Remember division
is multiplication inverted. In other words, each division problem
is really just a multiplication problem in reverse. Next I will show
you a structure that does not have a multiplicative inverse.


\subsubsection*{4-hour clock arithmetic}

Consider a structure whose set of numbers consists of four elements
$\{0,1,2,3\}$, I will call this a 4-clock arithmetic. Similar to
the 5-clock structure it is possible to add these numbers, for example,
$1+1=2$, $1+2=3$, $0+1=1$ and likewise $1+3=0$. In this structure
I map the number 4 to 0, $5\rightarrow1,6\rightarrow2,\ldots$etc.
I can also consider multiplication take for example: 
\[
2+2+2+3=1
\]
Again I can utilize repeated addition to understand multiplication.
\begin{align*}
2+2+2+3 & =1\\
2\cdot3+3 & =1\\
(2+1)\cdot3 & =1\\
3\cdot3 & =1
\end{align*}
This system has a multiplicative identity. Does it have a multiplicative
inverse, can we divide? From here it will be helpful to reference
a times table in this 4-clock structure: 
\begin{center}
\global\long\def\arraystretch{1.3}
 \setlength{\doublerulesep}{0pt} %
\begin{tabular}{r||2|2|2|2}
$\times$  & 0  & 1  & 2  & 3 \tabularnewline
\hline 
\hline 
0  & 0  & 0  & 0  & 0 \tabularnewline
\hline 
1  & 0  & 1  & 2  & 3 \tabularnewline
\hline 
2  & 0  & 2  & 0  & 2 \tabularnewline
\hline 
3  & 0  & 3  & 2  & 1 \tabularnewline
\hline 
\end{tabular}
\par\end{center}

Suppose I wanted to find $3\div2=?$. Previously I used the multiplicative
inverse to find the solution to a division problem, so I'll try that
same strategy again. I'll re-frame the problem as a multiplicative
one $3=?\cdot2$, now I want to multiply both sides by the multiplicative
inverse of 2, however I'm not able to, because there is no number,
which I can multiply 2 by and get 1 in this 4-clock structure.  There does not exist a solution in the $4-$clock arithmetic to $3\div2$, for more on this see the appendix on Euclidean division. 

The reason that we get a field modulo $5$ but we don't get a field
modulo $4$ is that $4=2\text{·}2$ whereas $5$ is a prime number,
that is, it is not the product of two integers smaller than itself.
The important thing to take away from this is that clock or modulo
arithmetic gives us a number system in which we can add, subtract,
multiply and divide if and only if the modulus or number of hours
on the clock is a prime (also called irreducible) number.

\subsubsection*{Applying the idea of field extension to polynomials}

Suppose that we are in the real numbers with any polynomial of the
form 
\[
x^{3}+x^{2}+x+1
\]
and I want to factor this polynomial. It looks like $x=-1$ might
be a root. So I use long division to find 
\[
x^{3}+x^{2}+x+1=(x+1)(x^{2}+1)
\]
The polynomial $x^{2}+1$ cannot be factored in $\mathbb{R}[x]$,
just like the number $5$ cannot be factored in the number system
$\mathbb{Z}.$ Just as I found that setting the prime number $5$
equal to zero led to the fact that every other number in the system
has a multiplicative inverse and so the $5$-hour clock system is
a field, I can apply the same logic to polynomials in $\mathbb{R}\left[x\right]$,
i.e., I can set 
\[
x^{2}+1=0
\]
which then implies that I must set all polynomial multiples of $x^{2}+1$
equal to zero and see what happens. Well, if $g\left(x\right)\in\mathbb{R}\left[x\right]$
is any polynomial with real coefficients, then I can do long division
with remainder 
\[
g\left(x\right)\div\left(x^{2}+1\right)=?
\]
to get 
\[
g\left(x\right)=h\left(x\right)\text{·}\left(x^{2}+1\right)+r\left(x\right)
\]
where $r\left(x\right)$ is a polynomial of degree less than two.
This equation says that in our system where $x^{2}+1=0$, 
\[
g\left(x\right)=r\left(x\right).
\]
So every element in this number system can be represented by a polynomial
of degree less than two, just like every integer can be represented
in $5$-clock arithmetic by either $0$, $1$, $2$, $3$, or $4$.

If $r\left(x\right)=0$ then $g\left(x\right)$ is a multiple of $x^{2}+1$
and so is also zero. So if $g\left(x\right)$ is not zero in this
system, either $r\left(x\right)$ is a polynomial of degree one or
a non-zero constant. If $r\left(x\right)$ is a non-zero constant
polynomial $a_{0}\in\mathbb{R}$, then it has a multiplicative inverse
in this system, namely $a_{0}^{-1}\in\mathbb{R}$. If $r\left(x\right)$
is a polynomial of degree one, then I can do long division with remainder
\[
\left(x^{2}+1\right)\div r\left(x\right)=
\]
to get 
\[
\left(x^{2}+1\right)=k\left(x\right)\text{·}r\left(x\right)+b_{0}
\]
where $b_{0}$ is a constant polynomial. Notice that $b_{0}\neq0$
since, if it were zero, $x^{2}+1$ would be factorable in $\mathbb{R}\left[x\right]$.
So in this system 
\[
0=k\left(x\right)\text{·}r\left(x\right)+b_{0}
\]
but we already know that $g\left(x\right)=r\left(x\right)$ so 
\[
0=k\left(x\right)\text{·}g\left(x\right)+b_{0}.
\]
So dividing both sides by $-b_{0}$ we get 
\[
0=\left(-b_{0}^{-1}\text{·}k\left(x\right)\right)\text{·}g\left(x\right)-1,
\]
that is $\left(-b_{0}^{-1}\text{·}k\left(x\right)\right)$ is the
multiplicative inverse of $g\left(x\right)$. The notation for this
system is 
\[
\mathbb{R}[x]/(x^{2}+1).
\]

In short, set the un-factorable term to 0, from that it follows that
the polynomial 
\[
x\in\mathbb{F}=\mathbb{R}[x]/(x^{2}+1)
\]
is a root of the polynomial 
\[
y^{2}+1\in\mathbb{F}\left[y\right]
\]
since, substitution $x\in\mathbb{F}$ for $y$, we get $x^{2}+1\in\mathbb{F}$
and in $\mathbb{F}$, $x^{2}+1=0$. This would be a field extension
of $\mathbb{R}$, namely $\mathbb{F}$ is a field and $\mathbb{F}$
contains the field $\mathbb{R}$.

For example multiplication in this field extension, $\mathbb{R}[x]/(x^{2}+1)$\footnote{Commonly called $\mathbb{R}[x]/(x^{2}+1)$, which means the field
of polynomials with real coefficients modulo $x^{2}+1$} is 
\begin{align*}
(a+bx)(c+dx) & =(a+bx)c+(a+bx)dx\\
 & =ac+bcx+adx+bdx^{2}\\
 & =(ac-bd+bd\left(x^{2}+1\right))+(ad+bc)x\\
 & =(ac-bd)+(ad+bc)x
\end{align*}
Usually we replace the letter $x$ by the letter $i$ and call this
system the complex numbers. So we have obtained the complex numbers
as a field extension of the real number system. We are doing this
because it will turn out that I can split any polynomial up in this
way (i.e. find the splitting field).

\subsubsection*{Splitting field of $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$}


Suppose that $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$ is irreducible in some
system of polynomials $\mathbb{F}[x]$ with coefficients in some field
$\mathbb{F}$ about which I know nothing. ($\mathbb{F}$ can't be
the field of real numbers because we have seen above that any polynomial
of degree three in $\mathbb{R}[x]$ has at least one real root and
so can be factored in $\mathbb{R}[x]$.)

What I want to do is construct a clock arithmetic where 
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}\equiv0,
\]
I'll call this new field $\mathbb{G}$ and write $\mathbb{G}[y]$
for the system of polynomials with coefficients in $\mathbb{G}$ .
As we saw above, the polynomial $f(y)=y^{3}-r_{1}y^{2}+r_{2}y-r_{3}$
has the root $x\in\mathbb{G}$ since $f(x)=x^{3}-r_{1}x^{2}+r_{2}x-r_{3}\equiv0$
therefore $x$ is root of $y^{3}-r_{1}y^{2}+r_{2}y-r_{3}$.\footnote{Since $x$ is not divisible by $y^{3}-r_{1}y^{2}+r_{2}y-r_{3}$, it
cannot be a multiple of it.} For the sake of notational convenience, write $x=s_{1}$. I can then
reduce the degree of my initial polynomial by long division by $y-s_{1}$
so that $y^{3}-r_{1}y^{2}+r_{2}y-r_{3}$ becomes 
\[
(y-s_{1})(y^{2}+by+c)
\]
where $s_{1}$, $b$ and $c$ are some values in $\mathbb{G}[x]$.
I complete the square on the quadratic term to find the other remaining
two roots. 
\begin{align*}
y^{2}+by+c & =0\\
\left(y+\frac{b}{2}\right)^{2}-\frac{b^{2}}{4}+c & =0\\
\left(y+\frac{b}{2}\right)^{2} & =\frac{b^{2}}{4}-c\\
\left(y+\frac{b}{2}\right)^{2} & =\frac{b^{2}-4c}{4}\\
\abs{y+\frac{b}{2}} & =\sqrt{\frac{b^{2}-4c}{4}}\\
y+\frac{b}{2} & =\pm\sqrt{\frac{b^{2}-4c}{4}}\\
y+\frac{b}{2} & =\pm\frac{\sqrt{b^{2}-4c}}{2}\\
y & =\frac{-b}{2}\pm\frac{\sqrt{b^{2}-4c}}{2}\\
y & =\frac{-b\pm\sqrt{b^{2}-4c}}{2}
\end{align*}
If we can solve the equation 
\[
y^{2}-\left(b^{2}-4c\right)=0
\]
for some $y=s_{0}\in\mathbb{G}$ then for simplification we write
\[
s_{0}=\sqrt{b^{2}-4c}.
\]
Then $y^{2}+by+c$ can be factored in $\mathbb{G}\left[y\right]$
and I'm done. Just let $s_{2}=\frac{-b+s_{0}}{2}$ and $s_{3}=\frac{-b-s_{0}}{2}$,
therefore my splitting field $\mathbb{G}$ since I have the factorization
\[
y^{3}-r_{1}y^{2}+r_{2}y-r_{3}=(y-s_{1})(y-s_{2})(y-s_{3}).
\]
in $\mathbb{G}\left[x\right]$.\\

If $y^{2}+by+c$ is not reducible, i.e., if I cannot solve $y^{2}-(b^{2}-4c)$
in $\mathbb{G}[y]$, then I need to expand the field again. This time
I set 
\[
y^{2}+by+c\equiv0
\]
and call this new field 
\[
\mathbb{J}=\mathbb{G}\left[x\right]/\left(y^{2}+by+c\right).
\]
Then just like before $y\in\mathbb{G}$ is a root of the polynomial
\[
z^{2}+bz+c\in\mathbb{J}\left[z\right].
\]
Again for convenience set $y=s_{2}$. Thus after long division of
$(z^{2}+bz+c)\div(z-s_{2})$, I will have a first degree polynomial,
so I will not need to repeat the procedure again.

To summarize: I split 
\[
z^{3}-r_{1}z^{2}+r_{2}z-r_{3}
\]
into 
\[
(z-s_{1})(z-s_{2})(z-s_{3})
\]
and $s_{1},s_{2},s_{3}$ are elements of this field $\mathbb{J}$.
This new field $\mathbb{J}$ may look very different than my original
field, $\mathbb{F}$. We may have had to make two field extensions
\[
\mathbb{F}\subseteq\mathbb{G}\subseteq\mathbb{J}
\]
in order to be able to factor $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$ completely.

\subsection*{Overview}

The previous process should be reminiscent of prime factorization.
In both instances we are decomposing something into pieces, which
cannot be decomposed anymore. In regards to prime factorization we
had prime numbers. In regards to the splitting field we have irreducible
terms. The main difference is that in prime factorization all the
possible prime numbers are available. Whereas in the splitting field,
I have to go looking for the pieces. But there is a place where all
the possible roots are available and that is where we are headed!

\subsection*{Symmetric Polynomials}
A symmetric polynomial is a polynomial whose variables can be interchanged in any way without affecting the polynomial.  For example,
\[
x+y
\]
is symmetric, if I replace $x$ with $y$ and $y$ with $x$ the expression
$y+x$ is equivalent. Likewise 
\[
x+y+z
\]
is also symmetric, we may replace $x$ with $z$ and $z$ with $x$
(or any other combination of variables) and maintain equivalency.
However 
\[
x^{2}+y
\]
is not symmetric alternating the variables results in 
\[
y^{2}+x\neq x^{2}+y
\]
The left hand side is not equivalent to the right hand side.


The polynomial $(z-s_1)(z-s_2)(z-s_3)$ is symmetric, this is important because symmetric polynomials have a special property.  I know that in $\mathbb{J}[z]$ all the usual properties
of addition and multiplication hold (except multiplicative inverse). Therefore there is nothing holding
me back from distributing and multiplying: 
\[
(z-s_{1})(z-s_{2})(z-s_{3})=z^{3}-(s_{1}+s_{2}+s_{3})z^{2}+(s_{1}s_{2}+s_{1}s_{3}+s_{2}s_{3})z-s_{1}s_{2}s_{3}
\]
Inspecting the previous equation notice how $s_{1}$ and $s_{2}$ could interchange
positions and maintain an equivalent expression, likewise with $s_{3}$.
So each of the coefficients on the right-hand side 
\begin{align*}
e_{1}\left(s_{1},s_{2},s_{3}\right) & =s_{1}+s_{2}+s_{3}\\
e_{2}\left(s_{1},s_{2},s_{3}\right) & =s_{1}s_{2}+s_{1}s_{3}+s_{2}s_{3}\\
e_{3}\left(s_{1},s_{2},s_{3}\right) & =s_{1}s_{2}s_{3}
\end{align*}
is a polynomial whose value is unchanged if the three variables $s_{1},s_{2},s_{3}$
are permuted in any way. These polynomials 
\[
p\left(s_{1},s_{2},s_{3}\right)
\]
are symmetric.

The three polynomials 
\begin{align*}
e_{1} & =s_{1}+s_{2}+s_{3}\\
e_{2} & =s_{1}s_{2}+s_{1}s_{3}+s_{2}s_{3}\\
e_{3} & =s_{1}s_{2}s_{3}
\end{align*}
are called elementary symmetric polynomials in the three variables
$s_{1},s_{2},s_{3}$ . In the same way, the $n$ coefficients of the
polynomial 
\[
(x-s_{1})(x-s_{2})\cdots(x-s_{n})=x^{n}-e_{1}(s_{1},\ldots,s_{n})x^{2}\pm e_{n-1}(s_{1},\ldots,s_{n})x\mp e_{n}(s_{1},\ldots,s_{n})
\]
are called the elementary symmetric polynomials in the $n$ variables
$s_{1},\ldots,s_{n}$.

Viéte proved that every symmetric polynomial 
\[
p\left(s_{1},\ldots,s_{n}\right)
\]
can be written as a polynomial 
\[
q\left(e_{1},\ldots,e_{n}\right)
\]
whose ``variables' are elementary symmetric polynomials in the variables
$s_{1},\ldots,s_{n}$. Viéte discovered that the necessary formulas
to prove this only utilize the basic properties of addition, subtraction,
multiplication, and division, so that they apply for coefficients
in any of the fields we might be interested in. Said otherwise, if
the coefficients of $p\left(s_{1},\ldots,s_{n}\right)$ lie in some
number system $\mathbb{S}$, then the coefficients in the polynomial
$q\left(e_{1},\ldots,e_{n}\right)$ lie in the same number system
$\mathbb{S}$.

\paragraph*{Example:}

Using Viéte's formulas to rewrite 
\[
s_{1}^{2}+s_{2}^{2}+s_{3}^{2}
\]
as the previously defined elementary symmetric polynomials.

First notice how $s_{1}^{2}+s_{2}^{2}+s_{3}^{2}$ is symmetric in
$s_{1},s_{2},s_{3}$. I can interchange any of the variables and I'll
have an equivalent expression.

Next I will need the sum of squares: 
\[
e_{1}^{2}=s_{1}^{2}+s_{2}^{2}+s_{3}^{2}+2s_{1}s_{2}+2s_{1}s_{3}+2s_{2}s_{3}
\]
Now I need to remove the cross terms, $e_{2}$ should work nicely
for that 
\[
e_{1}^{2}-2e_{2}=s_{1}^{2}+s_{2}^{2}+s_{3}^{2}+2s_{1}s_{2}+2s_{1}s_{3}+2s_{2}s_{3}-2(s_{1}s_{2}+s_{1}s_{3}+s_{2}s_{3})
\]
after zeroing out terms I arrive at 
\[
e_{1}^{2}-2e_{2}=s_{1}^{2}+s_{2}^{2}+s_{3}^{2}.
\]
\mbox{%
%
}\\

Returning to the polynomial in question, $z^{3}-r_{1}z^{2}+r_{2}z-r_{3}$,
the elementary symmetric polynomials $e_{1},e_{2},e_{3}$ are the
same as the coefficients, $r_{1},r_{2},r_{3}$ respectively. Let's
suppose the coefficients $r_{i}$ are real. This means I can relate
the roots in our strange field $\mathbb{J}$ with the coefficients
of the original polynomial that lie in $\mathbb{R}$. Since the $r_{1},r_{2},r_{3}$
lie in $\mathbb{R}$, i.e. the $e_{1},e_{2},e_{3}$ lie in $\mathbb{R}$.
But I still can't make the leap to $s_{1},s_{2},s_{3}$ lying in $\mathbb{R}$.
For example suppose 
\begin{align*}
s_{1}=1-i\\
s_{2}=1+i\\
s_{3}=1
\end{align*}
Then the elementary symmetric polynomials would become 
\begin{align*}
e_{1}=3\\
e_{2}=4\\
e_{3}=2
\end{align*}

The elementary symmetric polynomials are real, however only one of
the three roots is real valued. \\

In our proof of the Fundamental Theorem of Algebra, we will need to
use induction on polynomial equations whose coefficients are symmetric
polynomials. For example, if we start with 
\[
z^{3}-r_{1}z^{2}+r_{2}z-r_{3}=(z-s_{1})(z-s_{2})(z-s_{3})
\]
with the left-hand side in $\mathbb{R}\left[z\right]$ and the right-hand
side in $\mathcal{\mathbb{J}}\left[z\right]$, we will want to form
the polynomial 
\[
G_{t}(z)=(z-s_{1}-s_{2}-ts_{1}s_{2})(z-s_{1}-s_{3}-ts_{1}s_{3})(z-s_{2}-s_{3}-ts_{2}s_{3}).
\]
When I expand $G_{t}(z)$ I will get a polynomial in the variable
$z$ whose coefficients are polynomials in $s_{1},s_{2},s_{3}$ and
those coefficient polynomials in the $s_{1},s_{2},s_{3}$ are symmetric
polynomials in the $s_{1},s_{2},s_{3}$ and their coefficients are
in the number system 
\[
\mathbb{S}=\mathbb{Z}\left[s\right].
\]
Let's check. Here is what we get:



{\color{red} The equation which would be below is commented out because Lyx messes it up. Uncommnet on final compile}

%\begin{flalign*}
%G_s(x) &= x^3 \\
%&\hphantom{{}=x} {\color{green} - x^2 ( 2 a_1 + 2 a_2 + 2 a_3 + a_1 a_2 s + a_1 a_3 s + a_2 a_3 s )} &\\
%&\hphantom{{}=x+x} {\color{blue} + x( a_1 a_2 a_3^2 s^2 + a_1 a_2^2 a_3 s^2 + a_1^2 a_2 a_3 s^2 + a_1 a_2^2 s  + a_1 a_3^2 s  + a_2 a_3^2 s  + a_1^2 a_2 s  }  \\
%&\hphantom{{}=x+x+x+x} {\color{blue} + a_1^2 a_3 s  + a_2^2 a_3 s + 6 a_1 a_2 a_3 s  + a_1^2  + a_2^2  + a_3^2  + 3 a_1 a_2  + 3 a_1 a_3  + 3 a_2 a_3 )} &\\
%&\hphantom{{}=x+x+x} {\color{red} -a_1^2 a_2^2 a_3^2 s^3   - 2 a_1 a_2^2 a_3^2 s^2 - 2 a_1^2 a_2 a_3^2 s^2 - 2 a_1^2 a_2^2 a_3 s^2  - a_1^2 a_2^2 s - a_1^2 a_3^2 s - a_2^2 a_3^2 s} \\
%&\hphantom{{}=x+x+x+x} {\color{red} - 3 a_1 a_2 a_3^2 s - 3 a_1 a_2^2 a_3 s - 3 a_1^2 a_2 a_3 s  - a_1 a_2^2 - a_1 a_3^2 - a_2 a_3^2 - a_1^2 a_2 - a_1^2 a_3}\\
%&\hphantom{{}=x+x+x+x} {\color{red} - a_2^2 a_3 - 2 a_1 a_2 a_3 }
%\end{flalign*}


Notice how I could switch all the $s_{1}$ with $s_{2}$ (likewise
with $s_{1}$ and $s_{3}$ or $s_{2}$ and $s_{3}$) and I would end
up with an equivalent equation. Thus the polynomial viewed as a function
of $s_{1},s_{2},s_{3}$ is symmetric and $G_{t}(z)$ is symmetric
in the coefficients $s_{1},s_{2},s_{3}$. Since $G_{t}(z)$ is symmetric
then I can use Viéte's formulas and rewrite $G_{t}(z)$ in terms of
its elementary symmetric polynomials.\footnote{This is known as the Fundamental Theorem of Symmetric Polynomials,
the proof of which is a bit beyond this paper.} Recall that the elementary symmetric polynomials are 
\begin{align*}
e_{1} & =s_{1}+s_{2}+s_{3}\\
e_{2} & =s_{1}s_{2}+s_{1}s_{3}+s_{2}s_{3}\\
e_{3} & =s_{1}s_{2}s_{3}
\end{align*}

Using the elementary symmetric polynomials I can rewrite




{\color{red} The equation which would be below is commented out because Lyx messes it up.  Uncommnet on final compile}

%\begin{flalign*}
%G_s(x) &= x^3 \\
%&\hphantom{{}=x} {\color{green}- x^2 ( 2 e_1 + e_2 s )} &\\
%&\hphantom{{}=x+x} {\color{blue}+ x( e_1 e_3 s^2 + e_1 e_2 s  + e_1^2 - 2e_2  + 3 e_2 ) }&\\
%&\hphantom{{}=x+x+x} {\color{red}-e_3^2 s^3   - 2 e_2 e_3 s^2  - (e_2^2 - 2e_1e_3) s  - 3 e_1 e_3 s  - e_1 e_2 + e_3}
%\end{flalign*}






The coefficients
of the elementary symmetric polynomials are real valued. Furthermore
the elementary symmetric polynomials are real valued. Recall that
the $e_{1},e_{2},e_{3}$ relate to the original polynomial, 
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
Not only did $e_{1},e_{2},e_{3}$ relate to the roots, but they related
to the coefficients, i.e., 
\begin{align*}
e_{1}=r_{1}\\
e_{2}=r_{2}\\
e_{3}=r_{3}
\end{align*}

Remember that $r_{1},r_{2},r_{3}$ are real valued, $\mathbb{R}$.
Now $G_{t}(z)$ may be rewritten as a polynomial in the elementary
symmetric polynomials with coefficients in $\mathbb{S}=\mathbb{Z}\left[s\right]$,
which are equivalent to the coefficients in the original polynomial.
Thus the coefficients of $G_{t}(z)$ must be real valued, that is,
must lie in $\mathbb{R}\left[s\right].$

\subsubsection*{Complex number review}

Recall that to take the conjugate of a complex number you just switch
the addition or subtraction on the imaginary part (e.g. if $z=1+2i$
then the conjugate of $z$ is $\bar{z}=1-2i$). So a complex number
$a+bi$ is in fact a real number if and only if $b=0$, that is, if
and only if
\[
a+bi=\overline{a+bi}.
\]
Also, in the complex numbers, the sum of conjugates equals the conjugate
of the sum and the product of conjugates equals the conjugate of the
product.

Something neat also happens when you multiply a complex number with
its conjugate. Let's let $z=a+bi$ be any complex number, then the
conjugate is $\bar{z}=a-bi$ and if we multiply them together we get
\[
z\bar{z}=a^{2}+abi-abi-(bi)^{2}
\]
which becomes 
\[
z\bar{z}=a^{2}+b^{2}
\]
which is a real number, i.e., the imaginary part is 0 (or there is
no imaginary part). Furthermore it is a positive real number.

\section*{The Fundamental Theorem of Algebra}

So now we are ready to begin our proof of the Fundamental Theorem
of Algebra, namely the theorem that says that any polynomial of degree
$d>0$ with complex coefficients has at least one complex root. 

\subsection*{Reduction to polynomials with real coefficients}

We start with any polynomial of degree $d$ with coefficients which
are complex numbers. We can always divide through by the coefficient
of $x^{d}$ to reduce our polynomial to one of the form
\[
p\left(x\right)=x^{d}+a_{d-1}x^{d-1}+\ldots+a_{1}x+a_{0}\in\mathbb{C}\left[x\right].
\]
We need to show that there is a complex number $z_{1}$ that is a
root of this polynomial. We start by forming the polynomial
\[
P\left(x\right)=\left(x^{d}+a_{d-1}x^{d-1}+\ldots+a_{1}x+a_{0}\right)\left(x^{d}+\overline{a_{d-1}}x^{d-1}+\ldots+\overline{a_{1}}x+\overline{a_{0}}\right).
\]
Multiplying out the right-hand side we get
\[
\begin{array}{c}
P\left(x\right)=x^{2d}+\left(a_{d-1}+\overline{a_{d-1}}\right)x^{2d-1}+\\
\ldots\\
+\left(a_{1}\overline{a_{0}}+a_{0}\overline{a_{1}}\right)+a_{0}\overline{a_{0}}.
\end{array}
\]
Now each coefficient in $P\left(x\right)$ has the property that its
conjugate is itself\textendash that just follows from the fact that
the conjugate of a sum is the sum of the conjugates and the conjugate
of a product is the product of the conjugates. So all the coefficients
in the polynomial $P\left(x\right)$ are real! Also, it suffices to
find a complex root $z_{1}$ of $P\left(x\right)$ since such a $z_{1}$
is either a root of 
\[
x^{d}+a_{d-1}x^{d-1}+\ldots+a_{1}x+a_{0}
\]
so we can put $z_{0}=z_{1}$ or it is a root of 
\[
x^{d}+\overline{a_{d-1}}x^{d-1}+\ldots+\overline{a_{1}}x+\overline{a_{0}}.
\]
But in this second case
\[
\begin{array}{c}
z_{1}^{d}+\overline{a_{d-1}}z_{1}^{d-1}+\ldots+\overline{a_{1}}z_{1}+\overline{a_{0}}=0\\
\overline{z_{1}^{d}+\overline{a_{d-1}}z_{1}^{d-1}+\ldots+\overline{a_{1}}z_{1}+\overline{a_{0}}}=\overline{0}=0\\
\overline{z_{1}}^{d}+a_{d-1}\overline{z_{1}}^{d-1}+\ldots+a_{1}\overline{z_{1}}+a_{0}=0
\end{array}
\]
so we can put $z_{0}=\overline{z_{1}}$ . So it suffices to show that
every polynomial with real coefficients has a complex root.

\subsection*{Roots of polynomials with real coefficients}

Now suppose that we have any polynomial 
\[
x^{d}+r_{d-1}x^{d-1}+\ldots+r_{1}x+r_{0}
\]
with real coefficients. We can always factor
\[
d=2^{k}\text{·}m
\]
with $m$ odd. We will show that for any value $k$ there is at least
one complex root of this polynomial. We will do this by inducting
on $k$.

\subsubsection*{Base Case}

For the base case $k=0$, the polynomial has odd degree, and, as we
have explored above, the end behavior of odd polynomials with positive
leading coefficient is $x\rightarrow-\infty$ the value of the polynomial
$\rightarrow-\infty$ and as $x\rightarrow\infty$ the value of the
polynomial $\rightarrow\infty$. Recall that a polynomial is continuous,
there are no jumps, therefore the polynomial must cross the $x-$axis
at some place in between $(-\infty,\infty)$ therefore there is at
least one real root. (We give a more rigorous proof in the Appendix.)
This real root of course counts as our complex root since every real
number is a complex number. This argument takes care of any polynomials
with odd degree. 

\subsubsection*{Induction Step}

Now for the induction step, where we will use the induction hypothesis
that every polynomial with real coefficients and degree $d=2^{k-1}m'$
(where $m'$ is odd) has at least one complex root. 

As we have seen above, there is $some$ field $\mathbb{F}\supseteq\mathbb{C}\supseteq\mathbb{R}$
so that we can factor 
\[
x^{d}+r_{d-1}x^{d-1}+\ldots+r_{1}x+r_{0}=\left(x-x_{1}\right)\text{·}\ldots\text{·}\left(x-x_{d}\right)
\]
 with all the $x_{i}\in\mathbb{F}$.

Here comes the ingenious step! Let $s$ be an arbitrary real number
and let $y_{s,i,j}=x_{i}+x_{j}+sx_{i}x_{j}$, where $1\leq i<j\leq d$.
Now define: 
\[
G_{s}(x)=(x-y_{s,1,2})(x-y_{s,1,3})\cdots(x-y_{s,1,d})(x-y_{s,2,3})\cdots(x-y_{s,2,d})\cdots(x-y_{s,d-1,d})
\]
This may be succinctly written as: 
\[
G_{s}(x)=\Pi_{1\leq i<j\leq d}(x-y_{s,i,j})
\]
The coefficients of $x$ in $G_{s}(x)$ are polynomials in $x_{1},\ldots,x_{d}$
whose coefficients are in the number system $\mathbb{Z}\left[s\right]$.
But each of those coefficients is a symmetric polynomial in $x_{1},\ldots,x_{d}$
since it doesn't change under any permutation of the $x_{i}$ . So
by Viéte's theorem, each coefficient of $x$ in $G_{s}(x)$ is a polynomial
in the elementary symmetric functions in $x_{1},\ldots,x_{d}$ with
coefficients in $\mathbb{Z}\left[s\right]$. But those elementary
elementary symmetric functions in $x_{1},\ldots,x_{d}$ are just $r_{d-1},\ldots,r_{0}$!
And $r_{d-1},\ldots,r_{0}$ are all real numbers! So each coefficient
of $x$ in $G_{s}(x)$ lies in the number system $\mathbb{R}\left[s\right]$.

But what is the degree of $G_{s}(x)$? That is, how many terms there
are in $G_{s}(x)$? We get exactly one term for each way of choosing
two distinct numbers out of the set $\left\{ 1,\ldots,d\right\} $.
You may recognize this as the 'choose number' $\left(\begin{array}{c}
d\\
2
\end{array}\right)$. If not, here's a way to calculate it. There are $d-1$ terms that
have $i=1$, then there are $d-2$ terms that have $i=2$, because
$i<j$. So there are $d-3$ terms that have $i=3$ and so on. 
\[
G_{s}(x)=\underbrace{(x-y_{s,1,2})(x-y_{s,1,3})\cdots(x-y_{s,1,d})}_{d-1}\underbrace{(x-y_{s,2,3})\cdots(x-y_{s,2,d})}_{d-2}\cdots \underbrace{(x-y_{s,d-1,d}}_{1})
\]

So the degree of $G_{s}(x)$ is
\[
\left(d-1\right)+\left(d-2\right)+\ldots+2+1.
\]
But
\[
\begin{array}{c}
\left(d-1\right)+\left(d-2\right)+\ldots+2+1+\\
1+2+\ldots\left(d-2\right)+\left(d-1\right)\\
=d\text{·}\left(d-1\right)
\end{array}
\]
so that the degree of $G_{s}(x)$ is
\[
\frac{d\text{·}\left(d-1\right)}{2}=\frac{2^{k}\text{·}m\text{·}\left(2^{k}\text{·}m-1\right)}{2}=2^{k-1}\text{·}m\text{·}\left(2^{k}\text{·}m-1\right).
\]
But $m$ is odd and, since $k>0$, $\left(2^{k}\text{·}m-1\right)$
is also odd and so $m'=m\text{·}\left(2^{k}\text{·}m-1\right)$ is
also odd. So, by the induction hypothesis, for any fixed real number
$s$, the polynomial $G_{s}(x)\in\mathbb{R}\left[x\right]$ has a
complex root! So, for each real number $s$, at least one of the $y_{s,i,j}$
must be a complex number!

I do not know which 
\[
x_{i}+x_{j}+sx_{i}x_{j}
\]
is complex for a given $s$ but there are infinitely many real numbers
$s$ and only finitely many pairs $ij$ so there must be some $ij$
such that
\[
x_{i}+x_{j}+sx_{i}x_{j}
\]
 for an infinite number of real numbers $s$. Pick two of those, say
$s'$ and $s''$ . So we have a system of two linear equations 
\[
\begin{array}{c}
\left(x_{i}+x_{j}\right)+s'x_{i}x_{j}=z'\in\mathbb{C}\\
\left(x_{i}+x_{j}\right)+s''x_{i}x_{j}=z''\in\mathbb{C}
\end{array}
\]
in two unknowns $b=\left(x_{i}+x_{j}\right)$ and $c=x_{i}x_{j}$.
Solving the system of two linear equations in two unknowns, we conclude
that since $s'$, $s''$, $z'$, and $z''$ all lie in the complex
number system, so do the unknowns $b=\left(x_{i}+x_{j}\right)$ and
$c=x_{i}x_{j}$.  
{\color{red} Was my version incorrect?}
Finally consider the quadratic equation 
\[
x^{2}-(x_{i}+x_{j})x+x_{i}x_{j}=x^{2}-b\text{·}x+c=0
\]
with complex coefficients. Applying the quadratic formula, the solutions
are
\[
x=\frac{b\text{\textpm}\sqrt{b^{2}-4c}}{2}.
\]
On the other hand
\begin{align*}
x^{2}-(x_{i}+x_{j})x+x_{i}x_{j}&=\left(x-x_{i}\right)\left(x-x_{j}\right)\\
&= \left(x- \frac{b\text{\textpm}\sqrt{b^{2}-4c}}{2}\right)\left(x-\frac{b\mp\sqrt{b^{2}-4c}}{2}\right)
\end{align*}
So
\[
\begin{array}{c}
x_{i}=\frac{b\pm\sqrt{b^{2}-4c}}{2}\\
x_{j}=\frac{b\mp\sqrt{b^{2}-4c}}{2}.
\end{array}
\]
So to show that $x^{d}+r_{d-1}x^{d-1}+\ldots+r_{1}x+r_{0}$ has a
complex root, we only need to show that the square root of a complex
number $z=b^{2}-4c$ is again a complex number. 


\subsubsection*{Proof the square root of a complex number is complex}

Write $z$ in polar coordinates as $z=r\text{·}(\cos(\theta)+i\sin(\theta))$.
We seek a complex number $w$ such that $w^{2}=z$. Using the sum
of angles formulae from trigonometry, we compute 
\[
\begin{array}{c}
\left(r^{1/2}\text{·}(\cos(\theta/2)+i\sin(\theta/2))\right)^{2}=\\
\left(r^{1/2}\right)^{2}\text{·}((\cos(\theta/2)+i\sin(\theta/2)))^{2}=\\
r\text{·}\left(\left(\cos^{2}\left(\theta/2\right)-\sin^{2}\left(\theta/2\right)\right)+2i\sin(\theta/2)\text{·}\cos(\theta/2)\right)\\
r\text{·}(\cos(\theta)+i\sin(\theta))=z.
\end{array}
\]
So the complex number
\[
w=r^{1/2}\text{·}(\cos(\theta/2)+i\sin(\theta/2))
\]
is the square root of the complex number $z$.

So we have finished the proof of the Fundamental Theorem of Algebra.
Namely we have shown that every polynomial with coefficients which
are complex numbers
\[
p\left(x\right)=x^{d}+a_{d-1}x^{d-1}+\ldots+a_{1}x+a_{0}\in\mathbb{C}\left[x\right]
\]
has at least one root that is a complex number. So we are almost done
with the story. But we can say a bit more.

\subparagraph{Every single variable polynomial of degree $d$ in $\mathbb{C}\left[x\right]$
has exactly $d$ roots}

Let's prove this as a corollary of the Fundamental Theorem of Algebra.
Set: 
\[
f_{0}(x)=a_{d}x^{d}+a_{d-1}x^{d-1}+\cdots+a_{1}x+a_{0}
\]
where the $a_{i}$ are complex. We know by the Fundamental Theorem
of Algebra that $f_{0}(x)$ has at least one complex root, let's call
it $r_{1}$. But since $\mathbb{C}$ is a field, we can divide polynomials
in $\mathbb{C}\left[x\right]$ by the linear polynomial $x-r_{1}$
so that we can write
\[
f_{0}(x)=(x-r_{1})\text{·}\underbrace{f_{1}(x)}_{\textrm{\text{quotient}}}+\underbrace{b_{0}}_{\textrm{remainder}}.
\]
Substituting we have
\[
0=f_{0}\left(r_{1}\right)=\left(r_{1}-r_{1}\right)f_{1}\left(r_{1}\right)+a_{0}=a_{0}.
\]
So in fact
\[
f_{0}(x)=(x-r_{1})\text{·}f_{1}\left(x\right)
\]
with $f_{1}\left(x\right)\in\mathbb{C}\left[x\right].$\\

The degree of $f_{0}(x)$ was $d$, therefore the degree of $f_{1}(x)$
must be $d-1$, since 
\[
\underbrace{f_{0}(x)}_{\text{degree }n}=\underbrace{(x-r_{1})}_{\text{degree }1}f_{1}(x)
\]
and exponents add.\\

Now we apply the Fundamental Theorem of Algebra to $f_{1}(x)$ this
gives us a root we will call $r_{2}$. We divide $f_{1}(x)$ by $(x-r_{2})$
to get $f_{2}(x)$. Again the remainder must be zero and the degree
of $f_{2}(x)$ will be $d-2$. Now we have found two roots and we've
reduced the polynomial by two degrees. Therefore if we repeat this
process $d$ times we will have exactly $d$ roots and we will have
reduced the polynomial to degree $d-d=0$, for which no roots will
exist. Thus we have found exactly $d$ roots for a polynomial of degree
$d$. In fact in $\mathbb{C}\left[x\right]$ we have the complete
factorization 
\[
p\left(x\right)=x^{d}+a_{d-1}x^{d-1}+\ldots+a_{1}x+a_{0}=\left(x-r_{1}\right)\text{·}\cdots\text{·}\left(x-r_{d}\right).
\]
Another way to say this is that the only irreducible polynomials in
$\mathbb{C}\left[x\right]$ are linear polynomials $ax+b$.\pagebreak{}


\subsection*{Conclusion}

The original polynomial $C(x)$ had complex coefficients, which when
multiplied by its conjugate $\bar{C(x)}$ produced a polynomial with
real coefficients, we called this polynomial $R(x)$. The goal was
to show that this polynomial $R(x)$ had at least one complex root.
We found that complex root by inducting on the even part of the degree
of the polynomial $R(x)$. Induction enabled us to assume that polynomials
of \char`\"{}lesser'' degree do have at least one complex root. We
were able to construct a polynomial, $G_{t}(z)$ that not only satisfied
the induction hypothesis, but whose roots were the roots of $R(x)$.
So when we found $G_{t}(z)$'s complex root, we also found $R(x)$'s
complex root. We then used this result to reduce the degree of the
initial polynomial, and find the complex roots. Using this result
we were able to find $n$ roots for a $n^{\text{th}}$ degree polynomial.
In other words, we were able to decompose any polynomial into its
constituent parts. Just as any integer may be decomposed into its
prime factors, polynomials may be completely decomposed into linear
factors in the complex plane. So the Fundamental Theorem of Arithmetic
is to Integers, as the Fundamental Theorem of Algebra is to polynomials.

{\color{red} I was thinking I should find a way to include the mapping $\Phi$, just to tie everything together}

\begin{appendices}

\section*{The Mapping $\Phi$}

Let $f(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{0}$, where $a_{i}\in\mathbb{Z}$,
for $i=0,1,2,\ldots,n$.\\

Define: 
\[
\Phi_{b}(f(x))=\int f(x)\delta(b-x)dx=f(b)
\]
where $\delta$ is the Dirac delta, and $b$ is the base of the integer
representation we are mapping into.

So if we were mapping into the base-10 representation of the integers,
denoted $\mathbb{Z}_{10}$, we would have: 
\[
\Phi_{10}(f(x))=\int f(x)\delta(10-x)dx=f(10)
\]
It should be stated that $\Phi$ maps the polynomials with integer
coefficients to the integers.

\subsection*{$\Phi$ preserves addition and multiplication}

%In order to show that $\Phi$ is a ring homomorphism it is necessary to show that it is closed under addition and multiplication, \emph{but I also need to show $\Phi(1_F) = 1$}.

Given two polynomials with integer coefficients $f_{1},f_{2}$.

\paragraph*{Preserves addition}

\begin{align*}
\Phi_{b}(f_{1}(x)+f_{2}(x)) & =\int(f_{1}(x)+f_{2}(x))\delta(b-x)dx\\
 & =\int f_{1}(x)\delta(b-x)dx+\int f_{2}(x)\delta(b-x)dx\\
 & =f_{1}(b)+f_{2}(b)\\
 & =\Phi_{b}(f_{1}(x))+\Phi_{b}(f_{2}(x))
\end{align*}


\paragraph{Preserves multiplication}

\begin{align*}
\Phi_{b}(f_{1}(x)f_{2}(x)) & =\int f_{1}(x)f_{2}(x)\delta(b-x)dx\\
 & =f_{1}(b)f_{2}(b)-\int f'_{1}(x)g_{2}(x)dx\text{ By integrating by parts }\\
 & =f_{1}(b)f_{2}(b)\\
 & =\Phi_{b}(f_{1}(x))\Phi_{b}(f_{2}(x))
\end{align*}

Where $\int f'_{1}(x)g_{2}(x)dx=0$ because $g_{2}(b)=f_{2}(b)$\footnote{$f_{2}(b)\in\mathbb{Z}$}
when $x=b$, but $g_{2}(x)$ is zero everywhere else, so the integral
has measure zero.

\section{Division}
Division in the language of abstract algebra is often defined as the solution, $s$ to the equation $a \cdot s = t$.\footnote{Technically this would be the definition for left division, right division could be defined as the solution, $k$ to the equation $k \cdot a = t$.} This is similar to how the first peoples framed division.  When we use the word division we often think of "chunking'' values or items, yet considering division in this way is only true for integers.  In which case when we compute division we are calculating the quotient and the remainder.  The reason we know we are able to do this for any integer is attributable to Euclidean division (a theorem).

\subsection{Euclidean Division}
Euclidean division is a theorem that states:\footnote{Burton, David M. (2010). Elementary Number Theory. McGraw-Hill. pp. 17?19. ISBN 978-0-07-338314-9.}

Given two integers $a$ and $b$ with $b \neq 0$, there exists unique integer $q$ and $r$ such that 
\[
a= bq+r \text{ and } 0 \leq r \leq |b|
\]

This theorem tells us that for any integer division problem, not only does a quotient and remainder exist but there is a unique quotient and remainder.  This is very important, in the $4-$clock arithmetic we saw a specific instance where a solution did not exist.  It is possible to find a non unique solution in the $4-$clock arithmetic as well.  This theorem is often taken for granted because we have had so much experience with division of integers, when contrasted with $4-$clock arithmetic its importance is highlighted.

This theorem does not state how to perform the division.  That is the topic of division algorithms


\subsubsection{Division Algorithm}
Many division algorithms exist, the one with which most individuals are familiar with is long division, which is commonly confused as division.  Other algorithms exist one of which is remarkably simple and often overlooked.  For example, if a model for multiplication is repeated addition then a model for division be repeated subtraction.

\paragraph{Example:}  Divide 21 by 4 using repeated subtraction.  Subtract 4 from 21, $21-4=17$, repeat with the new result $17-4=13$, repeat $13-4=9$, $9-4=5$, $5-4=1$, stop because $4>1$.  So 21 divided by 4 has a remainder 1, and the quotient is 5, because we subtracted 4 five times.

There are many other algorithms for computing the quotient and remainder in a division problem.  All of which rely on Euclidean division.

\section{Odd degree real valued polynomials with real coefficients have a real root}
In the base case of the proof of the fundamental theorem of algebra it is argued that odd degree polynomials with real coefficients have a real root.  A rigorous proof is provided here.  First we offer some preliminaries: we define what it means to be continuous as well as the property that a non-empty set with an upper bound must have a least upper bound is given.  From here we then prove that polynomials are continuous.  Then by invoking the previously stated property we can show that the least upper bound is the root of the polynomial.


\subsection{Preliminaries}
\subsubsection{Definition of a real valued function which is continuous at a point}
A real valued function, $f$ is continuous at some point $c$, where $c$ is real-valued if the limit of $f(x)$ as $x$ approached $c$ is equal to $f(c)$\footnote{Lang, Serge (1997), Undergraduate analysis, Undergraduate Texts in Mathematics (2nd ed.), Berlin, New York: Springer-Verlag, ISBN 978-0-387-94841-6, section II.4}, i.e.,
\[
\lim_{x \rightarrow c} f(x) = f(c)
\]


\subsubsection{Bounding values}
An upper bound of a set is an element that is greater than or equal to every element of the set.  For example in the $5-$clock arithmetic set, $\{0,1,2,3,4\}$ an upper bound is 4, but 5 is also an upper bound.  The set of upper bounds, in the integers is the set, $U=\{4, 5, 6, \ldots \}$.  This is where the least upper bound comes in.  In order to differentiate between all the possible upper bounds; the least upper bound is defined to be the smallest of all the previous upper bounds.  If we consider the previous example then the smallest value of $U$ is 4, this  least upper bound of the previously stated $5-$ clock arithmetic is 4.  \\

\subsubsection{Least upper bound property}
The least upper bound property is a statement that if certain sets have an upper bound then they must have a least upper bound.  In terms of real values the least upper bound property is often stated as:\\

Any non-empty set of real numbers that has an upper bound must have a least upper bound in the real numbers.

\subsubsection{Bolzano-Weierstrass theorem}
We will also need to invoke the famous Bolzano-Wierstrass theorem, which states that every sequence $s_i$ of real numbers in a closed interval must have a convergent subsequence.  Phrased another way, if a set of real values, $S$ has an upper bound, then it has a least upper bound, $c$.  The value $c$ must be a limit point of the sequence $s_i$.




\subsection{Proof that odd degree real valued polynomials with real coefficients have a real root}
First we will prove that polynomials are continuous.  We will then show that odd degree polynomials have an upper bound, which implies that they have a least upper bound.  From this it will follow that the least upper bound is the root of the function.

\subsubsection*{Polynomials are continuous}

Below is a proof that polynomials are continuous. In other words,
in the real plane a polynomial function is continuous at every point,
therefore it is continuous on every interval in $\mathbb{R}$.

We need the product rule for limits. Let $f,g$ be real valued $\Lim{x\rightarrow c}f(x)=l$
and $\Lim{x\rightarrow c}g(x)=k$. Then: $\Lim{x\rightarrow c}(f(x)g(x))=lk$

We will also need to remember the combined sum rule for limits: Let
$f,g$ be real valued $\Lim{x\rightarrow c}f(x)=l$ and $\Lim{x\rightarrow c}g(x)=k$.
Let $\lambda,\kappa\in\mathbb{R}$. Then $\Lim{x\rightarrow c}(\lambda f(x)+\kappa g(x))=\lambda l+\kappa k$

Consider the function $l(x)=x$. Then $\Lim{x\rightarrow c}l(x)=c$.
Then by applying the product rule for limits to $\Lim{x\rightarrow c}l(x)l(x)=\Lim{x\rightarrow c}x^{2}=c^{2}$.
We can continue applying this rule so that for any value $d\in\mathbb{N}$
we have $\Lim{x\rightarrow c}x^{d}=c^{d}$. Let $P(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{1}x+a_{0}$.
Now by applying the combined sum rule to $P(x)$ we get $\Lim{x\rightarrow c}P(x)=\Lim{x\rightarrow c}a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{1}x+a_{0}=a_{n}c^{n}+a_{n-1}c^{n-1}+\cdots+a_{1}c+a_{0}=P(c)$.
Therefore $P(x)$ is continuous for any value $c$.


\subsubsection{The least upper bound $b$ for an odd degree polynomial}
Recall in the proof for the fundamental theorem of algebra we created the polynomial $R(x)$, by multiplying a polynomial with complex coefficients with its conjugate; this ensured that the polynomial $R(x)$ had a positive leading coefficient.  Therefore in the polynomial $P(x) = a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{1}x+a_{0}$, where $n$ is odd, we only consider the case where $a_n>0$.


Recall:
\[\lim_{x \rightarrow -\infty} x^d \rightarrow -\infty 
\]
and
\[
\lim_{x \rightarrow -\infty} 1 + \frac{r_{d-1}}{x^{d-1}} + \cdots +\frac{r_1}{x} + \frac{r_0}{x^d} = 1
\]

Thus:
\begin{align*}
\lim_{x \rightarrow -\infty} P(x) &= \lim_{x \rightarrow -\infty}  x^d + r_{d-1}x^{d-1} + \cdots +r_1x +r_0 \\
&= \lim_{x \rightarrow -\infty} x^d \left( 1 + \frac{r_{d-1}}{x^{d-1}} + \cdots +\frac{r_1}{x} + \frac{r_0}{x^d} \right) \\
&= \lim_{x \rightarrow -\infty} P(x) \rightarrow - \infty
\end{align*}

I have shown that $\lim_{x \rightarrow - \infty} P(x) \rightarrow - \infty$ and that $\lim_{x \rightarrow  \infty} P(x) \rightarrow \infty$.  Now we can consider the set $B$ of all $x$ values, where $f(x)<0$, i.e., $B = \{x \in \mathbb{R} | f(x)<0 \}$.  We know that $B$ is not the empty set, because $\lim_{x \rightarrow - \infty} P(x) \rightarrow - \infty$.  Further we know $B$ has an upper bound, because for some $x$ large enough $f(x)>0$, because $\lim_{x \rightarrow  \infty} P(x) \rightarrow \infty$.  Since $B$ has an upper bound, it must therefore have a least upper bound, call this value $b$.

\subsubsection{Putting it all together}

It has been proven that an odd degree polynomial with a positive leading coefficient, $P(x)$ is continuous and has a least upper bound, $b$.  Since $P(x)$ has a least upper bound and is real valued, by the Bolzano-Weirstrass theorem there exists a sequence, $b_i$ converging to $b$ from below, such that $f(b_i)<0$.  Since $f$ is continuous at all points, it must be continuous at $b$, the limit at $b$, must equal the function evaluated at $b$ therefore $\lim_{x \rightarrow b} P(x) = P(b)$.  But as we just saw there is a sequence converging to $b$ such that $f(b_i) < 0$, therefore it is accurate to state $P(b) \leq 0$.  On the flip side, for any $x>b$, $P(x)>0$.  So by the Bolzano-Weierstrass theorem there exists a sequence $c_i$ converging to $b$ from above such that $f(c_i)>0$.  But again $P(x)$ is continuous at $b$ and since there is a sequence of values such that $f(c_i)>0$ converging to $b$, it is accurate to state $P(b) \geq 0$.  Since $P(b) \leq 0$ and $P(b) \geq 0$, the only situation that satisfies these two equations is $P(b) = 0$, thus $b$ is a root of $P(x)$ 




\end{appendices} 
\end{document}
