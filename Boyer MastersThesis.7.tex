%% LyX 2.2.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt]{article}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose}
\setcounter{secnumdepth}{0}
\usepackage{wrapfig}
\usepackage{calc}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[unicode=true,
 bookmarks=false,
 breaklinks=false,pdfborder={0 0 1},backref=section,colorlinks=false]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.


%\usepackage{exsheets}
\usepackage[toc,page]{appendix}
%\usepackage{amsthm} 
\usepackage{array}
\mathtoolsset{showonlyrefs} 
\usepackage{graphicx}
\usepackage{listings}
%\usepackage{indentfirst}
%\usepackage{setspace}
%\doublespacing
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{commath}% Used for \abs{}
\usepackage{dcolumn}
\newcolumntype{2}{D{.}{}{2.0}}


\usepackage{pgf}
\usepackage{tikz}\usepackage{mathrsfs}
\usetikzlibrary{arrows}


\newcommand{\ssol}{\vspace{3em}}
\newcommand{\lsol}{\vspace{10em}}
\newcommand{\blnk}{{\underline {\hspace{1.5in}}}}
\newcommand{\Lim}[1]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle \lim_{#1}\;$}}}


%\usepackage{mcode}

 % Turns off automatic numbering of sections.

%\SetupExSheets{headings=block}




















\makeatother

\begin{document}

\section{Comparing the integers and the polynomials with coefficients in the
rational, real or complex number fields}

Suppose I want to solve a polynomial equation 
\[
\frac{a_{d}}{b_{d}}x^{d}+\frac{a_{d-1}}{b_{d-1}}x^{d-1}+\ldots+\frac{a_{0}}{b_{0}}=0
\]
with rational numbers $z_{i}=\frac{a_{i}}{b_{i}}$ (where $a_{i},b_{i}$
are integers) as coefficients. I could turn it into a polynomial equation
with integer coefficients just by multiplying both sides of the above
equation by a common multiple of $\left\{ b_{d},b_{d-1},\ldots,b_{0}\right\} $.
So every root of a polynomial with rational coefficients is a root
of a polynomial with integer coefficients. A deeper fact, called Gauss's
Lemma, says that any factorization of a polynomial 
\[
a_{d}x^{d}+a_{d-1}x^{d-1}+\ldots+a_{0}
\]
with integer coefficients $a_{i}$ that cannot be factored into two
polynomial factors with integer coefficients cannot be factored into
two polynomial factors with rational coefficients. These two facts
led mathematicians to study systems of polynomials with coefficients
in a number system in which you could add, subtract, multiply $and$
divide. They called such a system a $field$. Besides the field of
rational numbers, denoted $\mathbb{Q}$, other examples of fields
are the field $\mathbb{R}$ of real numbers and the field $\mathbb{C}$
of complex numbers. They also noticed that the system $\mathbb{F}\left[x\right]$
of polynomials with coefficients in a field $\mathbb{F}$ behaves
a lot like the most familiar number system, the integers. Both have
unique factorization into prime factors, greatest common divisors,
least common multiples, etc.

\section{The Mapping $\Phi_{b}$}

What I want to do is create a function (also called a mapping) that
takes a polynomial with integer coefficients to the (corresponding)
number system formed by its coefficients. For example I could ask
that the mapping substitute an integer $b$ for every \char`\"{}$x$'', thereby taking the given polynomial with integer coefficients to
an integer.
As an example of this, if $b=10$, the mapping would send $2x^{2}+3x+4\xrightarrow[\Phi_{10}]{}234$.
It is common to use the letter $\mathbb{Z}$ to denote the integer
number system and $\mathbb{Z}\left[x\right]$ to denote the ring of
polynomials with integer coefficients.

\subparagraph{Example}

We would then denote the function (mapping) described above as 
\[
\begin{array}{c}
\Phi_{10}:\mathbb{Z}[x]\rightarrow\mathbb{Z}\\
f(x)\mapsto f(10).
\end{array}
\]
In other words, if $f(x)=5x^{4}+4x^{3}+2x+1$,

\[
\Phi_{10}(f(x))=f(10)=5(10)^{4}+4(10)^{3}+2(10)+1=54021.
\]

More generally 
\[
\Phi_{b}(f(x))=f(b)
\]


\subsection*{Properties of $\Phi_{b}$ }

The following proofs use two polynomials $f_{1},f_{2}$, with coefficients
in any of the number systems $\mathbb{Z}$, $\mathbb{Q}$, $\mathbb{R}$
or $\mathbb{C}$. 
\begin{itemize}
\item $\Phi_{b}$ preserves addition 
\end{itemize}
\emph{Proof:} 
\begin{align*}
\Phi_{b}(f_{1}(x)+f_{2}(x)) & =f_{1}(b)+f_{2}(b)\\
 & =\Phi_{b}(f_{1}(x))+\Phi_{b}(f_{2}(x))
\end{align*}

\begin{itemize}
\item $\Phi_{b}$ preserves multiplication 
\end{itemize}
\emph{Proof:} 
\begin{align*}
\Phi_{b}(f_{1}(x)f_{2}(x)) & =f_{1}(b)f_{2}(b)\\
 & =\Phi_{b}(f_{1}(x))\Phi_{b}(f_{2}(x))
\end{align*}

\begin{itemize}
\item A polynomial maps to zero if and only if the polynomial is divisible
by $x-b$ 
\end{itemize}
\emph{Proof:}

Given $f(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{0}$, and a number
$r$ such that 
\[
f(r)=0.
\]

By long division of polynomials 
\[
a_{n}x^{n}+\cdots+a_{0}=(x-r)\text{·}(c_{n-1}x^{n-1}+\cdots c_{0})+\text{constant}
\]
Substituting $x=r$ into the both sides of the equation above, 
\[
0=0+\text{constant}.
\]
So by necessity $\text{constant}=0$, therefore 
\[
f(x)=(x-r)(c_{n-1}x^{n-1}+\cdots c_{0}).
\]


\subsection*{A rough comparison of prime factorization}

This section will compare prime factorization in the ring of integers
and the ring of polynomials with coefficients in a field. Prime factorization
is decomposing something into its constituent primes. Among the integers
we have the \emph{the fundamental theorem of arithmetic}, which says
that every positive integer has a unique prime factorization. In math
speak this states that any positive integer $k\geq2$, $k$ may be
rewritten as 
\[
k=p_{1}^{l_{1}}p_{2}^{l_{2}}\cdots p_{n}^{l_{n}}
\]
where the $p_{i}$'s are the $n$ distinct prime factors, each of
order $l_{i}$. One way to understand this is to think of it as taking
a positive integer, breaking it up into several unique parts, multiplying
those parts together and getting the same positive integer back. \\

Do the polynomials with coefficients in a field $\mathbb{F}$ have
a similar analogue? In this set-up, polynomials of degree zero, that
is, polynomials with only constant terms $a_{0}$, play the role of
$\text{\textpm1}$ in the integers, so that prime factors are always
polynomials of degree $d\geq1$. In grade school, polynomials are
often factored by breaking up the polynomial into several terms, each
of which can't be broken up further. Similarly these factored terms
may be multiplied together to retrieve the original polynomial.

For example in $\mathbb{Q}\left[x\right]$ 
\[
x^{2}-1=\left(x+1\right)\left(x-1\right)
\]
but 
\[
x^{2}-2
\]
is \char`\"{}prime'' in $\mathbb{Q}\left[x\right]$ since $\sqrt{2}$
is not in the number system $\mathbb{Q}$. Since $x^{2}-2$ is a polynomial
not an integer instead of prime it is more common to say irreducible.
In $\mathbb{R}\left[x\right]$, $x^{2}-2$ is reducible since $\sqrt{2}$ is an element of the reals, $\mathbb{R}$, i.e., 
\[
x^{2}-2=\left(x+\sqrt{2}\right)\left(x-\sqrt{2}\right).
\]
Howver the prime factorization of $f(x)=x^{2}+1$ in $\mathbb{R}\left[x\right]$
is $x^{2}+1$. Since the square of any real number is greater or equal
to zero, 
\[
x^{2}+1
\]
is irreducible in $\mathbb{R}\left[x\right]$. However in the system of
polynomials $\mathbb{C}\left[x\right]$, $x^{2}-1$ factors as 
\[
x^{2}+1=\left(x+i\right)\text{·}\left(x-i\right).
\]

Mathematicians decided that a polynomial $f\left(x\right)$ in $\mathbb{F}\left[x\right]$
is called irreducible if the polynomial cannot be written as a product
\[
f\left(x\right)=g\left(x\right)\text{·}h\left(x\right)
\]
where $g\left(x\right)$ and $h\left(x\right)$ are polynomials in
the same system $\mathbb{F}\left[x\right]$ and the degrees of $g\left(x\right)$
and $h\left(x\right)$ are both less than the degree of $f\left(x\right)$.
In other words, $f\left(x\right)$ is irreducible if division by any
polynomial $g\left(x\right)$ in the same system $\mathbb{F}\left[x\right]$
always leaves a remainder. So the only polynomials that divide an
irreducible polynomial are the irreducible polynomial itself and the
constant polynomials $a_{0}$. Remind you of anything?

\subparagraph{Exercise:}

If you don't know already, try to figure out how to do prime factorization
in any system $\mathbb{F}\left[x\right]$ of polynomials where $\mathbb{F}$
is one of our fields. \\

The 'miracle' is that every polynomial $f(x)$ in $\mathbb{R}\left[x\right]$
can be factored into irreducible factors of degrees $1$ and $2$.
But then each irreducible factor 
\[
ax^{2}+bx+c
\]
of $f(x)$ with $a$, $b$ and $c$ real can be considered as a polynomial
in $\mathbb{C}\left[x\right]$. (Remember that real numbers are also
complex numbers, it's just that their imaginary part is zero). It
is then possible to factor $ax^{2}+bx+c$ in $\mathbb{C}\left[x\right]$
by the quadratic formula, 
\[
a\left(x-\frac{-b+\sqrt{b^{2}-4ac}}{2a}\right)\left(x-\frac{-b-\sqrt{b^{2}-4ac}}{2a}\right).
\]
In fact, any polynomial $f(x)$ in $\mathbb{R}\left[x\right]$ be
factored completely in $\mathbb{C}\left[x\right]$ as 
\[
f(x)=a_{d}\text{·}(x-r_{1})^{l_{1}}(x-r_{2})^{l_{2}}\cdots(x-r_{i})^{l_{i}},
\]
the same is true for any $f(x)$ in $\mathbb{C}\left[x\right]$. We
know this fact as the \emph{Fundamental Theorem of Algebra}. The Fundamental
Theorem of Algebra states that the only irreducible polynomials in
$\mathbb{C}\left[x\right]$ are the polynomials of degree one! So,
roughly speaking, every $d$-th degree polynomial in $\mathbb{C}\left[x\right]$
has $d$ complex roots. These facts considered together with long
division of polynomials enable use to rewrite the polynomial 
\[
f(x)=a_{d}x^{d}+a_{d-1}x^{d-1}+\ldots+a_{0}
\]
as a product 
\[
f\left(x\right)=a_{d}\left(x-s_{1}\right)\text{·}\cdots\text{·}\left(x-s_{d}\right).
\]
I.e., if the $r_{i}$'s are the roots of $f(x)$ and the $l_{i}$
are the multiplicities of the roots, then each root $r_{i}$ occurs
in the list 
\[
s_{1},s_{2},\ldots,s_{d}
\]
exactly $l_{i}$ times.

\subparagraph{Exercise:}

For some small values of $d$, multiply out the right-hand-side of
the equality 
\[
a_{d}x^{d}+a_{d-1}x^{d-1}+\ldots+a_{0}=a_{d}\left(x-s_{1}\right)\text{·}\cdots\text{·}\left(x-s_{d}\right)
\]
so that you can give a formula for each quantity $\frac{a_{i}}{a_{d}}$
as a function of the quantities $s_{1},s_{2},\ldots,s_{d}$. Can you
guess the general formula for all values of $d$? Those formulas are
called the elementary symmetric functions of $s_{1},s_{2},\ldots,s_{d}$.
They will figure in an important way later on in this story.

\section{Toward a proof of the Fundamental Theorem of Algebra}

The Fundamental Thereom of Algebra is critical to much in algebra,
and the proof is often waved away as being beyond the scope of college
mathematics courses. In order to motivate the proof we start
with an example that highlights necessary components of the proof,
while giving some concreteness.

\section*{How many roots does $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$ have?}

Consider the following polynomial 
\[
p(x)=x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
how many roots does it have? Does it help if $r_{1},r_{2},r_{3}$
are real numbers? Let's suppose that and prove that $p(x)$ has three
roots in the complex field. We will take as a given that we know that,
as $x$ goes to $+\infty$, $p(x)$ becomes positive and, as $x$
goes to $-\infty$, $p(x)$ becomes negative. A slightly more complicated
fact is the fact that $p(x)$ is a continuous function of $x$, which
is often informally explained as the fact that you can draw the graph
of 
\[
y=p\left(x\right)
\]
without lifting your pencil from the page. So as a consequence of
the fact that $p(x)$ is continuous, your pencil cannot go from negative
$y$ to positive $y$ without crossing a place where $y=p\left(x\right)=0$.
That is, there is a real number $x$ where $p\left(x\right)=0$. So
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
has a real root $s_{1}$. As we have shown earlier, this means that
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}=\left(x-a_{1}\right)\left(x^{2}+bx+c\right).
\]
But now, again as had already been shown, this means that $x^{2}+bx+c$
can be factors into linear factored in $\mathbb{C}\left[x\right]$
using the quadratic formula.

Suppose now that we don't know that the coefficients of $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$
are real but just lie in some field $\mathbb{F}$, so the operations
of addition, subtraction, multiplication, and division hold. And suppose
we know that its roots lie in that same field. Let's call these roots
$a_{1},a_{2},a_{3}$.

Typically one would rewrite 
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
as 
\[
(x-a_{1})(x-a_{2})(x-a_{3}).
\]

Why can polynomials be written as linear factors? Recall the previous
section on the relationship between prime numbers and irreducible
factors of polynomials. It turns out that it always is possible to
factor a polynomial in $\mathcal{\mathbb{F}}\left[x\right]$ if $\mathbb{F}$
is a large enough field.

\subsection*{Field Extension}

The idea of a field extension boils down to increasing the set of
numbers one is able to use, while maintaining the properties of addition,
subtraction and multiplication and division. For example the Pythagoreans
worked in the field of rational numbers, therefore when they came
across $\sqrt{2}$ they did not believe it could be a solution. We
can now extend our field to all real values and now $\sqrt{2}$ is
a perfectly fine solution.


{\color{red} I feel like I should rewrite the parts on division using the division algorithm and the existence/uniqueness of the quotient and remainder in euclidean division (for integers and polynomials?) Thoughts?}

\subsubsection*{5-hour Clock Arithmetic}

The basics of field extension can be most easily understood by seeing
first how the integers can be turned into a field through the world
of clock (modular) arithmetic. Consider an algebraic structure that
consists of five elements, $\{0,1,2,3,4\}$. I will call this a 5-clock
arithmetic, because its arithmetic is just like what we use for the
hour hand on the clock, except that there are only five hours in our
'day,' that is, the set of hours has only five elements. It is possible
to add in this structure, for example, $1+1=2$, $1+2=3$, and $0+n=n$
thus 0 is the additive identity. But what about $4+1=$? On the 5-hour
clock we are forced to make $4+1=0$ since the hour after four o'clock
is the place where the hour-hand starts over . In other words in this
field the number 5 is mapped to the number 0, this implies the number
6 would be mapped to 1, and $7\rightarrow2$, $8\rightarrow3$, $9\rightarrow4,\ldots$.
Using these mappings and induction I can add any values in this structure
and still have values in this structure. For example addition in 5-clock
arithmetic looks like: 
\[
2+4=1
\]
\[
3+4=2
\]
\[
1+2+3+4=0
\]
\[
3+3+3+3+4=1
\]
so on and so forth. The last equation in the series highlights repeated
addition, I can use this model of multiplication (repeated addition)
to understand how multiplication functions in 5-clock arithmetic.
\begin{align*}
3+3+3+3+4 & =1\\
4\cdot3+4 & =1\\
4\cdot(3+1) & =1\text{ using the distributive property}\\
4\cdot4 & =1
\end{align*}
If $4\cdot4=1$, then multiplying both sides by 4, I get 
\begin{align*}
\underbrace{4\cdot4}_{=1}\cdot4 & =4\\
1\cdot4 & =4
\end{align*}
There is a multiplicative identity in this structure, it is the number
1.

Let's take a closer look at multiplication of twos. 
\begin{align*}
2\cdot0=0\\
2\cdot1=2\\
2\cdot2=4\\
2\cdot3=1\\
2\cdot4=3\\
\end{align*}

I have a multiplicative inverse for 2 it is 3. I.e., $2\cdot3=1$,
therefore $2^{-1}=3$. Likewise there is a multiplicative inverse
for $1,3,4$ namely $1^{-1}=1$, $3^{-1}=2$, $4^{-1}=4$. I will
use the multiplicative inverse to understand division in this system.\\

What does $2\div4=?$ in 5-clock arithmetic?  Rephrasing as a multiplication
problem, $2=?\cdot4$. I know 4 has a multiplicative inverse namely
itself, so I can multiply the previous equation by 4, 
\[
2\cdot4=?\cdot4\cdot4
\]
$2\cdot4=3$ and $4\cdot4=1$ in this clock 5 arithmetic, therefore
$3=?$. So 
\[
2\div4=3
\]
I can divide, because I have a multiplicative inverse.

A more efficient method for computing division exists. In fact it
was the Egyptians who were one of the first peoples to record this
more efficient method of division. Similar to the above process they
framed it as a multiplication problem with the multiplier (or multiplicand)\footnote{Why does it not matter if it is the multiplier or multiplicand that
is missing?} missing. But then they used a multiplication table to deduce the
solution.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% Overview of division
%The Egyptians then used their times tables to deduce the the answer.
%At its core this is what division is, multiplication in reverse, multiplication
%in-reverse, multiplication in-verse. We invert multiplication, this
%is why when dividing fractions we flip and multiply, division \underline{is}
%multiplicative inverse.  Thus if a set of numbers has a multiplicative inverse, then division is defined for that set of numbers.\\

From here it will help if we have a times-table to reference: 
\begin{center}
\global\long\def\arraystretch{1.3}
 \setlength{\doublerulesep}{0pt} %
\begin{tabular}{r||2|2|2|2|2}
$\times$  & 0  & 1  & 2  & 3  & 4 \tabularnewline
\hline 
\hline 
0  & 0  & 0  & 0  & 0  & 0 \tabularnewline
\hline 
1  & 0  & 1  & 2  & 3  & 4 \tabularnewline
\hline 
2  & 0  & 2  & 4  & 1  & 3 \tabularnewline
\hline 
3  & 0  & 3  & 1  & 4  & 2 \tabularnewline
\hline 
4  & 0  & 4  & 3  & 2  & 1 \tabularnewline
\hline 
\end{tabular}
\par\end{center}

What the Egyptians did was re-frame the division problem as multiplication,
then read off the answer. For example, $3\div3=?$ re-framed as multiplication
becomes $3=?\cdot3$. Now I look at the times table and see that 4
times 3 gives me 3, therefore $?=4$ and $3\div3=4$. Remember division
is multiplication inverted. In other words, each division problem
is really just a multiplication problem in reverse. Next I will show
you a structure that does not have a multiplicative inverse.

%\begin{align*}
%2 \div 1 = ? \rightarrow 2 = 1 \cdot ?  \Rightarrow ?=2 \\
%2 \div 2 = ? \rightarrow 2  = 2 \cdot ? \Rightarrow ?=1 \\
%2 \div 3 = ? \rightarrow 2 = 3 \cdot ? \Rightarrow ?=4 \\
%2 \div 4 = ? \rightarrow 2 = 4  \cdot ? \Rightarrow ?=3 \\
%\end{align*}

\subsubsection*{4-hour clock arithmetic}

Consider a structure whose set of numbers consists of four elements
$\{0,1,2,3\}$, I will call this a 4-clock arithmetic. Similar to
the 5-clock structure it is possible to add these numbers, for example,
$1+1=2$, $1+2=3$, $0+1=1$ and likewise $1+3=0$. In this structure
I map the number 4 to 0, $5\rightarrow1,6\rightarrow2,\ldots$etc.
I can also consider multiplication take for example: 
\[
2+2+2+3=1
\]
Again I can utilize repeated addition to understand multiplication.
\begin{align*}
2+2+2+3 & =1\\
2\cdot3+3 & =1\\
(2+1)\cdot3 & =1\\
3\cdot3 & =1
\end{align*}
This system has a multiplicative identity. Does it have a multiplicative
inverse, can we divide? From here it will be helpful to reference
a times table in this 4-clock structure: 
\begin{center}
\global\long\def\arraystretch{1.3}
 \setlength{\doublerulesep}{0pt} %
\begin{tabular}{r||2|2|2|2}
$\times$  & 0  & 1  & 2  & 3 \tabularnewline
\hline 
\hline 
0  & 0  & 0  & 0  & 0 \tabularnewline
\hline 
1  & 0  & 1  & 2  & 3 \tabularnewline
\hline 
2  & 0  & 2  & 0  & 2 \tabularnewline
\hline 
3  & 0  & 3  & 2  & 1 \tabularnewline
\hline 
\end{tabular}
\par\end{center}

Suppose I wanted to find $3\div2=?$. Previously I used the multiplicative
inverse to find the solution to a division problem, so I'll try that
same strategy again. I'll re-frame the problem as a multiplicative
one $3=?\cdot2$, now I want to multiply both sides by the multiplicative
inverse of 2, however I'm not able to, because there is no number,
which I can multiply 2 by and get 1 in this 4-clock structure. Therefore
because I do not have a multiplicative inverse I cannot define division.

The reason that we get a field modulo $5$ but we don't get a field
modulo $4$ is that $4=2\text{·}2$ whereas $5$ is a prime number,
that is, it is not the product of two integers smaller than itself.
The important thing to take away from this is that clock or modulo
arithmetic gives us a number sytem in which we can add, subtract,
multiply and divide if and only if the modulus or number of hours
on the clock is a prime (also called irreducible) number.

\subsubsection*{Applying the idea of field extension to polynomials}

Suppose that we are in the real numbers with any polynomial of the
form 
\[
x^{3}+x^{2}+x+1
\]
and I want to factor this polynomial. It looks like $x=-1$ might
be a root. So I use long division to find 
\[
x^{3}+x^{2}+x+1=(x+1)(x^{2}+1)
\]
The polynomial $x^{2}+1$ cannot be factored in $\mathbb{R}[x]$,
just like the number $5$ cannot be factored in the number system
$\mathbb{Z}.$ Just as I found that setting the prime number $5$
equal to zero led to the fact that every other number in the system
has a multiplicative inverse and so the $5$-hour clock system is
a field, I can apply the same logic to polynomials in $\mathbb{R}\left[x\right]$,
i.e., I can set 
\[
x^{2}+1=0
\]
which then implies that I must set all polynomial multiples of $x^{2}+1$
equal to zero and see what happens. Well, if $g\left(x\right)\in\mathbb{R}\left[x\right]$
is any polynomial with real coefficients, then I can do long division
with remander 
\[
g\left(x\right)\div\left(x^{2}+1\right)=?
\]
to get 
\[
g\left(x\right)=h\left(x\right)\text{·}\left(x^{2}+1\right)+r\left(x\right)
\]
where $r\left(x\right)$ is a polynomial of degree less than two.
This equation says that in our system where $x^{2}+1=0$, 
\[
g\left(x\right)=r\left(x\right).
\]
So every element in this number system can be represented by a polynomial
of degree less than two, just like every integer can be represented
in $5$-clock arithmetic by either $0$, $1$, $2$, $3$, or $4$.

If $r\left(x\right)=0$ then $g\left(x\right)$ is a multiple of $x^{2}+1$
and so is also zero. So if $g\left(x\right)$ is not zero in this
system, either $r\left(x\right)$ is a polynomial of degree one or
a non-zero constant. If $r\left(x\right)$ is a non-zero constant
poynomial $a_{0}\in\mathbb{R}$, then it has a multiplicative inverse
in this system, namely $a_{0}^{-1}\in\mathbb{R}$. If $r\left(x\right)$
is a polynomial of degree one, then I can do long division with remander
\[
\left(x^{2}+1\right)\div r\left(x\right)=
\]
to get 
\[
\left(x^{2}+1\right)=k\left(x\right)\text{·}r\left(x\right)+b_{0}
\]
where $b_{0}$ is a constant polynomial. Notice that $b_{0}\neq0$
since, if it were zero, $x^{2}+1$ would be factorable in $\mathbb{R}\left[x\right]$.
So in this system 
\[
0=k\left(x\right)\text{·}r\left(x\right)+b_{0}
\]
but we already know that $g\left(x\right)=r\left(x\right)$ so 
\[
0=k\left(x\right)\text{·}g\left(x\right)+b_{0}.
\]
So dividing both sides by $-b_{0}$ we get 
\[
0=\left(-b_{0}^{-1}\text{·}k\left(x\right)\right)\text{·}g\left(x\right)-1,
\]
that is $\left(-b_{0}^{-1}\text{·}k\left(x\right)\right)$ is the
multiplicative inverse of $g\left(x\right)$. The notation for this
system is 
\[
\mathbb{R}[x]/(x^{2}+1).
\]

In short, set the un-factorable term to 0, from that it follows that
the polynomial 
\[
x\in\mathbb{F}=\mathbb{R}[x]/(x^{2}+1)
\]
is a root of the polynomial 
\[
y^{2}+1\in\mathbb{F}\left[y\right]
\]
since, substitution $x\in\mathbb{F}$ for $y$, we get $x^{2}+1\in\mathbb{F}$
and in $\mathbb{F}$, $x^{2}+1=0$. This would be a field extension
of $\mathbb{R}$, namely $\mathbb{F}$ is a field and $\mathbb{F}$
contains the field $\mathbb{R}$.

For example multiplication in this field extension, $\mathbb{R}[x]/(x^{2}+1)$\footnote{Commonly called $\mathbb{R}[x]/(x^{2}+1)$, which means the field
of polynomials with real coefficients modulo $x^{2}+1$} is 
\begin{align*}
(a+bx)(c+dx) & =(a+bx)c+(a+bx)dx\\
 & =ac+bcx+adx+bdx^{2}\\
 & =(ac-bd+bd\left(x^{2}+1\right))+(ad+bc)x\\
 & =(ac-bd)+(ad+bc)x
\end{align*}
Usually we replace the letter $x$ by the letter $i$ and call this
system the complex numbers. So we have obtained the complex numbers
as a field extension of the real number system. We are doing this
because it will turn out that I can split any polynomial up in this
way (i.e. find the splitting field).

%I am extending
%the field as a 'quotient ring' modulo the relation $x^{2}+1$. The set of all polynomial multiples of $x^{2}+1$ is called an ideal in the polynomial ring $\mathbb{R}[x]$ and, since $x^{2}+1$ cannot be factored in $\mathbb{R}[x]$,
%the quotient ring is a field.

%\paragraph*{Example}
%What is the splitting field for 
%\[
%x^{4}+x^{3}+x^{2}+1?
%\]
%Using the previous field extension, i.e., the quotient ring generated
%by $x^{2}+1=0$. 
%\begin{align*}
%x^{4}+x^{3}+x^{2}+1 & =x^{4}+x^{3}+0\\
% & =x^{3}(x+1)
%\end{align*}
%Lastly, recall $x^{2}=-1$ this implies $x^{3}=-x$. Therefore: 
%\[
%x^{4}+x^{3}+x^{2}+1=-x(x+1)
%\]
%in our new field $\mathbb{Z}/(x^{2}+1)$. I have split the polynomial
%into linear factors, this is the splitting field, and in this field
%0 and -1 are the roots of the polynomial.\\

\subsubsection*{Splitting field of $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$}

%In short we can eventually factor
%\[
%x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
%\]
%as 
%\[
%(x-a_{1})(x-a_{2})(x-a_{3})
%\]
%because we look at it in a bigger field in which it is possible, just as
%we did in the above examples. \\

Suppose that $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$ is irreducible in some
system of polynomials $\mathbb{F}[x]$ with coefficients in some field
$\mathbb{F}$ about which I know nothing. ($\mathbb{F}$ can't be
the field of real numbers because we have seen above that any polynomial
of degree three in $\mathbb{R}[x]$ has at least one real root and
so can be factored in $\mathbb{R}[x]$.)

What I want to do is construct a clock arithmetic where 
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}\equiv0,
\]
I'll call this new field $\mathbb{G}$ and write $\mathbb{G}[y]$
for the system of polynomials with coefficients in $\mathbb{G}$ .
As we saw above, the polynomial $f(y)=y^{3}-r_{1}y^{2}+r_{2}y-r_{3}$
has the root $x\in\mathbb{G}$ since $f(x)=x^{3}-r_{1}x^{2}+r_{2}x-r_{3}\equiv0$
therefore $x$ is root of $y^{3}-r_{1}y^{2}+r_{2}y-r_{3}$.\footnote{Since $x$ is not divisible by $y^{3}-r_{1}y^{2}+r_{2}y-r_{3}$, it
cannot be a multiple of it.} For the sake of notational convenience, write $x=s_{1}$. I can then
reduce the degree of my initial polynomial by long division by $y-s_{1}$
so that $y^{3}-r_{1}y^{2}+r_{2}y-r_{3}$ becomes 
\[
(y-s_{1})(y^{2}+by+c)
\]
where $s_{1}$, $b$ and $c$ are some values in $\mathbb{G}[x]$.
I complete the square on the quadratic term to find the other remaining
two roots. 
\begin{align*}
y^{2}+by+c & =0\\
\left(y+\frac{b}{2}\right)^{2}-\frac{b^{2}}{4}+c & =0\\
\left(y+\frac{b}{2}\right)^{2} & =\frac{b^{2}}{4}-c\\
\left(y+\frac{b}{2}\right)^{2} & =\frac{b^{2}-4c}{4}\\
\abs{y+\frac{b}{2}} & =\sqrt{\frac{b^{2}-4c}{4}}\\
y+\frac{b}{2} & =\pm\sqrt{\frac{b^{2}-4c}{4}}\\
y+\frac{b}{2} & =\pm\frac{\sqrt{b^{2}-4c}}{2}\\
y & =\frac{-b}{2}\pm\frac{\sqrt{b^{2}-4c}}{2}\\
y & =\frac{-b\pm\sqrt{b^{2}-4c}}{2}
\end{align*}
If we can solve the equation 
\[
y^{2}-\left(b^{2}-4c\right)=0
\]
for some $y=s_{0}\in\mathbb{G}$ then for simplification we write
\[
s_{0}=\sqrt{b^{2}-4c}.
\]
Then $y^{2}+by+c$ can be factored in $\mathbb{G}\left[y\right]$
and I'm done. Just let $s_{2}=\frac{-b+s_{0}}{2}$ and $s_{3}=\frac{-b-s_{0}}{2}$,
therefore my splitting field $\mathbb{G}$ since I have the factorization
\[
y^{3}-r_{1}y^{2}+r_{2}y-r_{3}=(y-s_{1})(y-s_{2})(y-s_{3}).
\]
in $\mathbb{G}\left[x\right]$.\\

If $y^{2}+by+c$ is not reducible, i.e., if I cannot solve $y^{2}-(b^{2}-4c)$
in $\mathbb{G}[y]$, then I need to expand the field again. This time
I set 
\[
y^{2}+by+c\equiv0
\]
and call this new field 
\[
\mathbb{J}=\mathbb{G}\left[x\right]/\left(y^{2}+by+c\right).
\]
Then just like before $y\in\mathbb{G}$ is a root of the polynomial
\[
z^{2}+bz+c\in\mathbb{J}\left[z\right].
\]
Again for convenience set $y=s_{2}$. Thus after long division of
$(z^{2}+bz+c)\div(z-s_{2})$, I will have a first degree polynomial,
so I will not need to repeat the procedure again.

To summarize: I split 
\[
z^{3}-r_{1}z^{2}+r_{2}z-r_{3}
\]
into 
\[
(z-s_{1})(z-s_{2})(z-s_{3})
\]
and $s_{1},s_{2},s_{3}$ are elements of this field $\mathbb{J}$.
This new field $\mathbb{J}$ may look very different than my original
field, $\mathbb{F}$. We may have had to make two field extensions
\[
\mathbb{F}\subseteq\mathbb{G}\subseteq\mathbb{J}
\]
in order to be able to factor $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$ completely.

\subsection*{Overview}

The previous process should be reminiscent of prime factorization.
In both instances we are decomposing something into pieces, which
cannot be decomposed anymore. In regards to prime factorization we
had prime numbers. In regards to the splitting field we have irreducible
terms. The main difference is that in prime factorization all the
possible prime numbers are available. Whereas in the splitting field,
I have to go looking for the pieces. But there is a place where all
the possible roots are available and that is where we are headed!

\subsection*{Symmetric Polynomials}
A symmetric polynomial is a polynomial whose variables can be interchanged in any way without affecting the polynomial.  For example,
\[
x+y
\]
is symmetric, if I replace $x$ with $y$ and $y$ with $x$ the expression
$y+x$ is equivalent. Likewise 
\[
x+y+z
\]
is also symmetric, we may replace $x$ with $z$ and $z$ with $x$
(or any other combination of variables) and maintain equivalancey.
However 
\[
x^{2}+y
\]
is not symmetric alternating the variables results in 
\[
y^{2}+x\neq x^{2}+y
\]
The left hand side is not equivalent to the right hand side.


I know that $\mathbb{J}[z]$ is a commutative ring thus all the usual properties
of addition and multiplication hold (except multiplicative inverse). Therefore there is nothing holding
me back from distributing and multiplying: 
\[
(z-s_{1})(z-s_{2})(z-s_{3})=z^{3}-(s_{1}+s_{2}+s_{3})z^{2}+(s_{1}s_{2}+s_{1}s_{3}+s_{2}s_{3})z-s_{1}s_{2}s_{3}
\]
Look at this equation and notice how $s_{1}$ and $s_{2}$ could interchange
positions and maintain an equivalent expression, likewise with $s_{3}$.
So each of the coefficients on the right-hand side 
\begin{align*}
e_{1}\left(s_{1},s_{2},s_{3}\right) & =s_{1}+s_{2}+s_{3}\\
e_{2}\left(s_{1},s_{2},s_{3}\right) & =s_{1}s_{2}+s_{1}s_{3}+s_{2}s_{3}\\
e_{3}\left(s_{1},s_{2},s_{3}\right) & =s_{1}s_{2}s_{3}
\end{align*}
is a polynomial whose value is unchanged if the three variables $s_{1},s_{2},s_{3}$
are permuted in any way. These polynomials 
\[
p\left(s_{1},s_{2},s_{3}\right)
\]
are symmetric.

The three polynomials 
\begin{align*}
e_{1} & =s_{1}+s_{2}+s_{3}\\
e_{2} & =s_{1}s_{2}+s_{1}s_{3}+s_{2}s_{3}\\
e_{3} & =s_{1}s_{2}s_{3}
\end{align*}
are called elementary symmetric polynomials in the three variables
$s_{1},s_{2},s_{3}$ . In the same way, the $n$ coefficients of the
polynomial 
\[
(x-s_{1})(x-s_{2})\cdots(x-s_{n})=x^{n}-e_{1}(s_{1},\ldots,s_{n})x^{2}\pm e_{n-1}(s_{1},\ldots,s_{n})x\mp e_{n}(s_{1},\ldots,s_{n})
\]
are called the elementary symmetric polynomials in the $n$ variables
$s_{1},\ldots,s_{n}$.

Viéte proved that every symmetric polynomial 
\[
p\left(s_{1},\ldots,s_{n}\right)
\]
can be written as a polynomial 
\[
q\left(e_{1},\ldots,e_{n}\right)
\]
whose ``variables' are elementary symmetric polynomials in the variables
$s_{1},\ldots,s_{n}$. Viéte discovered that the necessary formulas
to prove this only utilize the basic properties of addition, subtraction,
multiplication, and division, so that they apply for coefficients
in any of the fields we might be interested in. Said otherwise, if
the coefficients of $p\left(s_{1},\ldots,s_{n}\right)$ lie in some
number system $\mathbb{S}$, then the coefficients in the polynomial
$q\left(e_{1},\ldots,e_{n}\right)$ lie in the same number system
$\mathbb{S}$.

\paragraph*{Example:}

Using Viéte's formulas to rewrite 
\[
s_{1}^{2}+s_{2}^{2}+s_{3}^{2}
\]
as the previously defined elementary symmetric polynomials.

First notice how $s_{1}^{2}+s_{2}^{2}+s_{3}^{2}$ is symmetric in
$s_{1},s_{2},s_{3}$. I can interchange any of the variables and I'll
have an equivalent expression.

Next I will need the sum of squares: 
\[
e_{1}^{2}=s_{1}^{2}+s_{2}^{2}+s_{3}^{2}+2s_{1}s_{2}+2s_{1}s_{3}+2s_{2}s_{3}
\]
Now I need to remove the cross terms, $e_{2}$ should work nicely
for that 
\[
e_{1}^{2}-2e_{2}=s_{1}^{2}+s_{2}^{2}+s_{3}^{2}+2s_{1}s_{2}+2s_{1}s_{3}+2s_{2}s_{3}-2(s_{1}s_{2}+s_{1}s_{3}+s_{2}s_{3})
\]
after zeroing out terms I arrive at 
\[
e_{1}^{2}-2e_{2}=s_{1}^{2}+s_{2}^{2}+s_{3}^{2}.
\]
\mbox{%
%
}\\

Returning to the polynomial in question, $z^{3}-r_{1}z^{2}+r_{2}z-r_{3}$,
the elementary symmetric polynomials $e_{1},e_{2},e_{3}$ are the
same as the coefficients, $r_{1},r_{2},r_{3}$ respectively. Let's
suppose the coefficients $r_{i}$ are real. This means I can relate
the roots in our strange field $\mathbb{J}$ with the coefficients
of the original polynomial that lie in $\mathbb{R}$. Since the $r_{1},r_{2},r_{3}$
lie in $\mathbb{R}$, i.e. the $e_{1},e_{2},e_{3}$ lie in $\mathbb{R}$.
But I still can't make the leap to $s_{1},s_{2},s_{3}$ lying in $\mathbb{R}$.
For example suppose 
\begin{align*}
s_{1}=1-i\\
s_{2}=1+i\\
s_{3}=1
\end{align*}
Then the elementary symmetric polynomials would become 
\begin{align*}
e_{1}=3\\
e_{2}=4\\
e_{3}=2
\end{align*}

The elementary symmetric polynomials are real, however only one of
the three roots is real valued. \\

In our proof of the Fundamental Theorem of Algebra, we will need to
use induction on polynomial equations whose coefficients are symmetric
polynomials. For example, if we start with 
\[
z^{3}-r_{1}z^{2}+r_{2}z-r_{3}=(z-s_{1})(z-s_{2})(z-s_{3})
\]
with the left-hand side in $\mathbb{R}\left[z\right]$ and the right-hand
side in $\mathcal{\mathbb{J}}\left[z\right]$, we will want to form
the polynomial 
\[
G_{t}(z)=(z-s_{1}-s_{2}-ts_{1}s_{2})(z-s_{1}-s_{3}-ts_{1}s_{3})(z-s_{2}-s_{3}-ts_{2}s_{3}).
\]
When I expand $G_{t}(z)$ I will get a polynomial in the variable
$z$ whose coefficients are polynomials in $s_{1},s_{2},s_{3}$ and
those coefficient polynomials in the $s_{1},s_{2},s_{3}$ are symmetric
polynomials in the $s_{1},s_{2},s_{3}$ and their coefficients are
in the number system 
\[
\mathbb{S}=\mathbb{Z}\left[s\right].
\]
Let's check. Here is what we get:

%$$ G_s(x) = x^2 - x(a_1 + 2a_2 + a_3 + s (a_1 a_2 + a_2 a_3))  + a_1 a_2 + a_1 a_3 + a_2 a_3 + a_2^2 + s(a_1 a_2^2 + 2a_1 a_2 a_3 + a_2^2 a_3) + s^2 a_1 a_2^2 a_3$$

{\color{red} Lyx messes up the equation below, so I will wait to
include it}

\begin{flalign*} Gs(x) \&= x3 \\
 \&\hphantom{{}=x} {\color{green} - x2 ( 2 a1 + 2 a2 + 2 a3 + a1
a2 s + a1 a3 s + a2 a3 s )} \&\\
 \&\hphantom{{}=x+x} {\color{blue} + x( a1 a2 a32 s2 + a1 a22 a3
s2 + a12 a2 a3 s2 + a1 a22 s + a1 a32 s + a2 a32 s + a12 a2 s } \\
 \&\hphantom{{}=x+x+x+x} {\color{blue} + a12 a3 s + a22 a3 s +
6 a1 a2 a3 s + a12 + a22 + a32 + 3 a1 a2 + 3 a1 a3 + 3 a2 a3 )} \&\\
 \&\hphantom{{}=x+x+x} {\color{red} -a12 a22 a32 s3 - 2 a1 a22
a32 s2 - 2 a12 a2 a32 s2 - 2 a12 a22 a3 s2 - a12 a22 s - a12 a32 s
- a22 a32 s} \\
 \&\hphantom{{}=x+x+x+x} {\color{red} - 3 a1 a2 a32 s - 3 a1 a22
a3 s - 3 a12 a2 a3 s - a1 a22 - a1 a32 - a2 a32 - a12 a2 - a12 a3}\\
 \&\hphantom{{}=x+x+x+x} {\color{red} - a22 a3 - 2 a1 a2 a3 }
\end{flalign*}

Notice how I could switch all the $s_{1}$ with $s_{2}$ (likewise
with $s_{1}$ and $s_{3}$ or $s_{2}$ and $s_{3}$) and I would end
up with an equivalent equation. Thus the polynomial viewed as a function
of $s_{1},s_{2},s_{3}$ is symmetric and $G_{t}(z)$ is symmetric
in the coefficients $s_{1},s_{2},s_{3}$. Since $G_{t}(z)$ is symmetric
then I can use Viéte's formulas and rewrite $G_{t}(z)$ in terms of
its elementary symmetric polynomials.\footnote{This is known as the Fundamental Theorem of Symmetric Polynomials,
the proof of which is a bit beyond this paper.} Recall that the elementary symmetric polynomials are 
\begin{align*}
e_{1} & =s_{1}+s_{2}+s_{3}\\
e_{2} & =s_{1}s_{2}+s_{1}s_{3}+s_{2}s_{3}\\
e_{3} & =s_{1}s_{2}s_{3}
\end{align*}

Using the elementary symmetric polynomials I can rewrite

{\color{red} Lyx messes up the equation below, so I will wait to
include it}

\begin{flalign*} Gs(x) \&= x3 \\
 \&\hphantom{{}=x} {\color{green}- x2 ( 2 e1 + e2 s )} \&\\
 \&\hphantom{{}=x+x} {\color{blue}+ x( e1 e3 s2 + e1 e2 s + e12
- 2e2 + 3 e2 ) }\&\\
 \&\hphantom{{}=x+x+x} {\color{red}-e32 s3 - 2 e2 e3 s2 - (e22
- 2e1e3) s - 3 e1 e3 s - e1 e2 + e3} \end{flalign*} The coefficients
of the elementary symmetric polynomials are real valued. Furthermore
the elementary symmetric polynomials are real valued. Recall that
the $e_{1},e_{2},e_{3}$ relate to the original polynomial, 
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
Not only did $e_{1},e_{2},e_{3}$ relate to the roots, but they related
to the coefficients, i.e., 
\begin{align*}
e_{1}=r_{1}\\
e_{2}=r_{2}\\
e_{3}=r_{3}
\end{align*}

Remember that $r_{1},r_{2},r_{3}$ are real valued, $\mathbb{R}$.
Now $G_{t}(z)$ may be rewritten as a polynomial in the elementary
symmetric polynomials with coefficients in $\mathbb{S}=\mathbb{Z}\left[s\right]$,
which are equivalent to the coefficients in the original polynomial.
Thus the coefficients of $G_{t}(z)$ must be real valued, that is,
must lie in $\mathbb{R}\left[s\right].$

\subsubsection*{Complex number review}

Recall that to take the conjugate of a complex number you just switch
the addition or subtraction on the imaginary part (e.g. if $z=1+2i$
then the conjugate of $z$ is $\bar{z}=1-2i$). So a complex number
$a+bi$ is in fact a real number if and only if $b=0$, that is, if
and only if
\[
a+bi=\overline{a+bi}.
\]
Also, in the complex numbers, the sum of conjugates equals the conjugate
of the sum and the product of conjugates equals the conjugate of the
product.

Something neat also happens when you multiply a complex number with
its conjugate. Let's let $z=a+bi$ be any complex number, then the
conjugate is $\bar{z}=a-bi$ and if we multiply them together we get
\[
z\bar{z}=a^{2}+abi-abi-(bi)^{2}
\]
which becomes 
\[
z\bar{z}=a^{2}+b^{2}
\]
which is a real number, i.e., the imaginary part is 0 (or there is
no imaginary part). Furthermore it is a positive real number.

\section*{The Fundamental Theorem of Algebra}

So now we are ready to begin our proof of the Fundamental Theorem
of Algebra, namely the theorem that says that any polynomial of degree
$d>0$ with complex coefficients has at least one complex root. 

\subsection*{Reduction to polynomials with real coefficients}

We start with any polynomial of degree $d$ with coefficients which
are complex numbers. We can always divide through by the coefficient
of $x^{d}$ to reduce our polynomial to one of the form
\[
p\left(x\right)=x^{d}+a_{d-1}x^{d-1}+\ldots+a_{1}x+a_{0}\in\mathbb{C}\left[x\right].
\]
We need to show that there is a complex number $z_{1}$ that is a
root of this polynomial. We start by forming the polynomial
\[
P\left(x\right)=\left(x^{d}+a_{d-1}x^{d-1}+\ldots+a_{1}x+a_{0}\right)\left(x^{d}+\overline{a_{d-1}}x^{d-1}+\ldots+\overline{a_{1}}x+\overline{a_{0}}\right).
\]
Multiplying out the right-hand side we get
\[
\begin{array}{c}
P\left(x\right)=x^{2d}+\left(a_{d-1}+\overline{a_{d-1}}\right)x^{2d-1}+\\
\ldots\\
+\left(a_{1}\overline{a_{0}}+a_{0}\overline{a_{1}}\right)+a_{0}\overline{a_{0}}.
\end{array}
\]
Now each coefficient in $P\left(x\right)$ has the property that its
conjugate is itself\textendash that just follows from the fact that
the conjugate of a sum is the sum of the conjugates and the conjugate
of a product is the product of the conjugates. So all the coefficients
in the polynomial $P\left(x\right)$ are real! Also, it suffices to
find a complex root $z_{1}$ of $P\left(x\right)$ since such a $z_{1}$
is either a root of 
\[
x^{d}+a_{d-1}x^{d-1}+\ldots+a_{1}x+a_{0}
\]
so we can put $z_{0}=z_{1}$ or it is a root of 
\[
x^{d}+\overline{a_{d-1}}x^{d-1}+\ldots+\overline{a_{1}}x+\overline{a_{0}}.
\]
But in this second case
\[
\begin{array}{c}
z_{1}^{d}+\overline{a_{d-1}}z_{1}^{d-1}+\ldots+\overline{a_{1}}z_{1}+\overline{a_{0}}=0\\
\overline{z_{1}^{d}+\overline{a_{d-1}}z_{1}^{d-1}+\ldots+\overline{a_{1}}z_{1}+\overline{a_{0}}}=\overline{0}=0\\
\overline{z_{1}}^{d}+a_{d-1}\overline{z_{1}}^{d-1}+\ldots+a_{1}\overline{z_{1}}+a_{0}=0
\end{array}
\]
so we can put $z_{0}=\overline{z_{1}}$ . So it suffices to show that
every polynomial with real coefficients has a complex root.

\subsection*{Roots of polynomials with real coefficients}

Now suppose that we have any polynomial 
\[
x^{d}+r_{d-1}x^{d-1}+\ldots+r_{1}x+r_{0}
\]
with real coefficients. We can always factor
\[
d=2^{k}\text{·}m
\]
with $m$ odd. We will show that for any value $k$ there is at least
one complex root of this polynomial. We will do this by inducting
on $k$.

\subsubsection*{Base Case}

For the base case $k=0$, the polynomial has odd degree, and, as we
have explored above, the end behavior of odd polynomials with positive
leading coefficient is $x\rightarrow-\infty$ the value of the polynomial
$\rightarrow-\infty$ and as $x\rightarrow\infty$ the value of the
polynomial $\rightarrow\infty$. Recall that a polynomial is continuous,
there are no jumps, therefore the polynomial must cross the $x-$axis
at some place in between $(-\infty,\infty)$ therefore there is at
least one real root. (We give a more rigorous proof in the Appendix.)
This real root of course counts as our complex root since every real
number is a complex number. This argument takes care of any polynomials
with odd degree. 

\subsubsection*{Induction Step}

Now for the induction step, where we will use the induction hypothesis
that every polynomial with real coefficients and degree $d=2^{k-1}m'$
(where $m'$ is odd) has at least one complex root. 

As we have seen above, there is $some$ field $\mathbb{F}\supseteq\mathbb{C}\supseteq\mathbb{R}$
so that we can factor 
\[
x^{d}+r_{d-1}x^{d-1}+\ldots+r_{1}x+r_{0}=\left(x-x_{1}\right)\text{·}\ldots\text{·}\left(x-x_{d}\right)
\]
 with all the $x_{i}\in\mathbb{F}$.

Here comes the ingenious step! Let $s$ be an arbitrary real number
and let $y_{s,i,j}=x_{i}+x_{j}+sx_{i}x_{j}$, where $1\leq i<j\leq d$.
Now define: 
\[
G_{s}(x)=(x-y_{s,1,2})(x-y_{s,1,3})\cdots(x-y_{s,1,d})(x-y_{s,2,3})\cdots(x-y_{s,2,d})\cdots(x-y_{s,d-1,d})
\]
This may be succinctly written as: 
\[
G_{s}(x)=\Pi_{1\leq i<j\leq d}(x-y_{s,i,j})
\]
The coefficients of $x$ in $G_{s}(x)$ are polynomials in $x_{1},\ldots,x_{d}$
whose coefficients are in the number system $\mathbb{Z}\left[s\right]$.
But each of those coefficients is a symmetric polynomial in $x_{1},\ldots,x_{d}$
since it doesn't change under any permutation of the $x_{i}$ . So
by Viéte's theorem, each coefficient of $x$ in $G_{s}(x)$ is a polynomial
in the elementary symmetric functions in $x_{1},\ldots,x_{d}$ with
coefficients in $\mathbb{Z}\left[s\right]$. But those elementary
elementary symmetric functions in $x_{1},\ldots,x_{d}$ are just $r_{d-1},\ldots,r_{0}$!
And $r_{d-1},\ldots,r_{0}$ are all real numbers! So each coefficient
of $x$ in $G_{s}(x)$ lies in the number system $\mathbb{R}\left[s\right]$.

But what is the degree of $G_{s}(x)$? That is, how many terms there
are in $G_{s}(x)$? We get exactly one term for each way of choosing
two distinct numbers out of the set $\left\{ 1,\ldots,d\right\} $.
You may recognize this as the 'choose number' $\left(\begin{array}{c}
d\\
2
\end{array}\right)$. If not, here's a way to calculate it. There are $d-1$ terms that
have $i=1$, then there are $d-2$ terms that have $i=2$, because
$i<j$. So there are $d-3$ terms that have $i=3$ and so on. 
\[
G_{s}(x)=\underbrace{(x-y_{s,1,2})(x-y_{s,1,3})\cdots(x-y_{s,1,d})}_{d-1}\underbrace{(x-y_{s,2,3})\cdots(x-y_{s,2,d})}_{d-2}\cdots \underbrace{(x-y_{s,d-1,d}}_{1})
\]

So the degree of $G_{s}(x)$ is
\[
\left(d-1\right)+\left(d-2\right)+\ldots+2+1.
\]
But
\[
\begin{array}{c}
\left(d-1\right)+\left(d-2\right)+\ldots+2+1+\\
1+2+\ldots\left(d-2\right)+\left(d-1\right)\\
=d\text{·}\left(d-1\right)
\end{array}
\]
so that the degree of $G_{s}(x)$ is
\[
\frac{d\text{·}\left(d-1\right)}{2}=\frac{2^{k}\text{·}m\text{·}\left(2^{k}\text{·}m-1\right)}{2}=2^{k-1}\text{·}m\text{·}\left(2^{k}\text{·}m-1\right).
\]
But $m$ is odd and, since $k>0$, $\left(2^{k}\text{·}m-1\right)$
is also odd and so $m'=m\text{·}\left(2^{k}\text{·}m-1\right)$ is
also odd. So, by the induction hypothesis, for any fixed real number
$s$, the polynomial $G_{s}(x)\in\mathbb{R}\left[x\right]$ has a
complex root! So, for each real number $s$, at least one of the $y_{s,i,j}$
must be a complex number!

I do not know which 
\[
x_{i}+x_{j}+sx_{i}x_{j}
\]
is complex for a given $s$ but there are infinitely many real numbers
$s$ and only finitely many pairs $ij$ so there must be some $ij$
such that
\[
x_{i}+x_{j}+sx_{i}x_{j}
\]
 for an infinite number of real numbers $s$. Pick two of those, say
$s'$ and $s''$ . So we have a system of two linear equations 
\[
\begin{array}{c}
\left(x_{i}+x_{j}\right)+s'x_{i}x_{j}=z'\in\mathbb{C}\\
\left(x_{i}+x_{j}\right)+s''x_{i}x_{j}=z''\in\mathbb{C}
\end{array}
\]
in two unknowns $b=\left(x_{i}+x_{j}\right)$ and $c=x_{i}x_{j}$.
Solving the system of two linear equations in two unknowns, we conclude
that since $s'$, $s''$, $z'$, and $z''$ all lie in the complex
number system, so do the unknowns $b=\left(x_{i}+x_{j}\right)$ and
$c=x_{i}x_{j}$.  
{\color{red} Was my version incorrect?}
Finally consider the quadratic equation 
\[
x^{2}-(x_{i}+x_{j})x+x_{i}x_{j}=x^{2}-b\text{·}x+c=0
\]
with complex coefficients. Applying the quadratic formula, the solutions
are
\[
x=\frac{b\text{\textpm}\sqrt{b^{2}-4c}}{2}.
\]
On the other hand
\begin{align*}
x^{2}-(x_{i}+x_{j})x+x_{i}x_{j}&=\left(x-x_{i}\right)\left(x-x_{j}\right)\\
&= \left(x- \frac{b\text{\textpm}\sqrt{b^{2}-4c}}{2}\right)\left(x-\frac{b\mp\sqrt{b^{2}-4c}}{2}\right)
\end{align*}
So
\[
\begin{array}{c}
x_{i}=\frac{b\pm\sqrt{b^{2}-4c}}{2}\\
x_{j}=\frac{b\mp\sqrt{b^{2}-4c}}{2}.
\end{array}
\]
So to show that $x^{d}+r_{d-1}x^{d-1}+\ldots+r_{1}x+r_{0}$ has a
complex root, we only need to show that the square root of a complex
number $z=b^{2}-4c$ is again a complex number. 


\subsubsection*{Proof the square root of a complex number is complex}

Write $z$ in polar coordinates as $z=r\text{·}(\cos(\theta)+i\sin(\theta))$.
We seek a complex number $w$ such that $w^{2}=z$. Using the sum
of angles formulae from trigonometry, we compute 
\[
\begin{array}{c}
\left(r^{1/2}\text{·}(\cos(\theta/2)+i\sin(\theta/2))\right)^{2}=\\
\left(r^{1/2}\right)^{2}\text{·}((\cos(\theta/2)+i\sin(\theta/2)))^{2}=\\
r\text{·}\left(\left(\cos^{2}\left(\theta/2\right)-\sin^{2}\left(\theta/2\right)\right)+2i\sin(\theta/2)\text{·}\cos(\theta/2)\right)\\
r\text{·}(\cos(\theta)+i\sin(\theta))=z.
\end{array}
\]
So the complex number
\[
w=r^{1/2}\text{·}(\cos(\theta/2)+i\sin(\theta/2))
\]
is the square root of the complex number $z$.

So we have finished the proof of the Fundamental Theorem of Algebra.
Namely we have shown that every polynomial with coefficients which
are complex numbers
\[
p\left(x\right)=x^{d}+a_{d-1}x^{d-1}+\ldots+a_{1}x+a_{0}\in\mathbb{C}\left[x\right]
\]
has at least one root that is a complex number. So we are almost done
with the story. But we can say a bit more.

\subparagraph{Every single variable polynomial of degree $d$ in $\mathbb{C}\left[x\right]$
has exactly $d$ roots}

Let's prove this as a corollary of the Fundamental Theorem of Algebra.
Set: 
\[
f_{0}(x)=a_{d}x^{d}+a_{d-1}x^{d-1}+\cdots+a_{1}x+a_{0}
\]
where the $a_{i}$ are complex. We know by the Fundamental Theorem
of Algebra that $f_{0}(x)$ has at least one complex root, let's call
it $r_{1}$. But since $\mathbb{C}$ is a field, we can divide polynomials
in $\mathbb{C}\left[x\right]$ by the linear polynomial $x-r_{1}$
so that we can write
\[
f_{0}(x)=(x-r_{1})\text{·}\underbrace{f_{1}(x)}_{\textrm{\text{quotient}}}+\underbrace{b_{0}}_{\textrm{remainder}}.
\]
Substituting we have
\[
0=f_{0}\left(r_{1}\right)=\left(r_{1}-r_{1}\right)f_{1}\left(r_{1}\right)+a_{0}=a_{0}.
\]
So in fact
\[
f_{0}(x)=(x-r_{1})\text{·}f_{1}\left(x\right)
\]
with $f_{1}\left(x\right)\in\mathbb{C}\left[x\right].$\\

The degree of $f_{0}(x)$ was $d$, therefore the degree of $f_{1}(x)$
must be $d-1$, since 
\[
\underbrace{f_{0}(x)}_{\text{degree }n}=\underbrace{(x-r_{1})}_{\text{degree }1}f_{1}(x)
\]
and exponents add.\\

Now we apply the Fundamental Thereom of Algebra to $f_{1}(x)$ this
gives us a root we will call $r_{2}$. We divide $f_{1}(x)$ by $(x-r_{2})$
to get $f_{2}(x)$. Again the remainder must be zero and the degree
of $f_{2}(x)$ will be $d-2$. Now we have found two roots and we've
reduced the polynomial by two degrees. Therefore if we repeat this
process $d$ times we will have exactly $d$ roots and we will have
reduced the polynomial to degree $d-d=0$, for which no roots will
exist. Thus we have found exactly $d$ roots for a polynomial of degree
$d$. In fact in $\mathbb{C}\left[x\right]$ we have the complete
factorization 
\[
p\left(x\right)=x^{d}+a_{d-1}x^{d-1}+\ldots+a_{1}x+a_{0}=\left(x-r_{1}\right)\text{·}\cdots\text{·}\left(x-r_{d}\right).
\]
Another way to say this is that the only irreducible polynomials in
$\mathbb{C}\left[x\right]$ are linear polynomials $ax+b$.\pagebreak{}


\subsection*{Conclusion}

The original polynomial $C(x)$ had complex coefficients, which when
multiplied by its conjugate $\bar{C(x)}$ produced a polynomial with
real coefficients, we called this polynomial $R(x)$. The goal was
to show that this polynomial $R(x)$ had at least one complex root.
We found that complex root by inducting on the even part of the degree
of the polynomial $R(x)$. Induction enabled us to assume that polynomials
of \char`\"{}lesser'' degree do have at least one complex root. We
were able to construct a polynomial, $G_{t}(x)$ that not only satisfied
the induction hypothesis, but whose roots were the roots of $R(x)$.
So when we found $G_{t}(x)$'s complex root, we also found $R(x)$'s
complex root. We then used this result to reduce the degree of the
initial polynomial, and find the complex roots. Using this result
we were able to find $n$ roots for a $n^{\text{th}}$ degree polynomial.
In other words, we were able to decompose any polynomial into its
constituent parts. Just as any integer may be decomposed into its
prime factors, polynomials may be completely decomposed into linear
factors in the complex plane. So the Fundamental Theorem of Arithmetic
is to Integers, as the Fundamental Theorem of Algebra is to polynomials.

\begin{appendices}

\section*{The Mapping $\Phi$}

Let $f(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{0}$, where $a_{i}\in\mathbb{Z}$,
for $i=0,1,2,\ldots,n$.\\

Define: 
\[
\Phi_{b}(f(x))=\int f(x)\delta(b-x)dx=f(b)
\]
where $\delta$ is the dirac delta, and $b$ is the base of the integer
representation we are mapping into.

So if we were mapping into the base-10 representation of the integers,
denoted $\mathbb{Z}_{10}$, we would have: 
\[
\Phi_{10}(f(x))=\int f(x)\delta(10-x)dx=f(10)
\]
It should be stated that $\Phi$ maps the polynomials with integer
coefficients to the integers.

\subsection*{$\Phi$ preserves addition and multiplication}

%In order to show that $\Phi$ is a ring homomorphism it is necessary to show that it is closed under addition and multiplication, \emph{but I also need to show $\Phi(1_F) = 1$}.

Given two polynomials with integer coefficients $f_{1},f_{2}$.

\paragraph*{Preserves addition}

\begin{align*}
\Phi_{b}(f_{1}(x)+f_{2}(x)) & =\int(f_{1}(x)+f_{2}(x))\delta(b-x)dx\\
 & =\int f_{1}(x)\delta(b-x)dx+\int f_{2}(x)\delta(b-x)dx\\
 & =f_{1}(b)+f_{2}(b)\\
 & =\Phi_{b}(f_{1}(x))+\Phi_{b}(f_{2}(x))
\end{align*}


\paragraph{Preserves multiplication}

\begin{align*}
\Phi_{b}(f_{1}(x)f_{2}(x)) & =\int f_{1}(x)f_{2}(x)\delta(b-x)dx\\
 & =f_{1}(b)f_{2}(b)-\int f'_{1}(x)g_{2}(x)dx\text{ By integrating by parts }\\
 & =f_{1}(b)f_{2}(b)\\
 & =\Phi_{b}(f_{1}(x))\Phi_{b}(f_{2}(x))
\end{align*}

Where $\int f'_{1}(x)g_{2}(x)dx=0$ because $g_{2}(b)=f_{2}(b)$\footnote{$f_{2}(b)\in\mathbb{Z}$}
when $x=b$, but $g_{2}(x)$ is zero everywhere else, so the integral
has measure zero.

\section*{Polynomials are continuous}

Below is a proof that polynomials are continuous. In other words,
in the real plane a polynomial function is continuous at every point,
therefore it is continuous on every interval in $\mathbb{R}$.

We need the product rule for limits. Let $f,g$ be real valued $\Lim{x\rightarrow c}f(x)=l$
and $\Lim{x\rightarrow c}g(x)=k$. Then: $\Lim{x\rightarrow c}(f(x)g(x))=lk$

We will also need to remember the combined sum rule for limits: Let
$f,g$ be real valued $\Lim{x\rightarrow c}f(x)=l$ and $\Lim{x\rightarrow c}g(x)=k$.
Let $\lambda,\kappa\in\mathbb{R}$. Then $\Lim{x\rightarrow c}(\lambda f(x)+\kappa g(x))=\lambda l+\kappa k$

Consider the function $l(x)=x$. Then $\Lim{x\rightarrow c}l(x)=c$.
Then by applying the product rule for limits to $\Lim{x\rightarrow c}l(x)l(x)=\Lim{x\rightarrow c}x^{2}=c^{2}$.
We can continue applying this rule so that for any value $d\in\mathbb{N}$
we have $\Lim{x\rightarrow c}x^{d}=c^{d}$. Let $P(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{1}x+a_{0}$.
Now by applying the combined sum rule to $P(x)$ we get $\Lim{x\rightarrow c}P(x)=\Lim{x\rightarrow c}a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{1}x+a_{0}=a_{n}c^{n}+a_{n-1}c^{n-1}+\cdots+a_{1}c+a_{0}=P(c)$.
Therefore $P(x)$ is continuous for any value $c$.

%Using induction, first prove that a polynomial of degree 1 is continuous.  Let $m,b \in \mathbb{R}$, and let $f$ be a real function defined as $f(x) = m x + b$.  Then we want to show that $f$ is continuous at every real number $c \in \mathbb{R}$.

%Assume $m \neq 0$, let $\epsilon > 0$, and $\delta = \frac{\epsilon}{|m|}$.  Then whenever $|x-c| < \delta$.
%\begin{align*}
%|f(x)-f(c)| &= |mx+b -mc -b| \\
%&= |m(x-c)| \\
%&= |m||x-c| \\
%&< |m| \delta \\
%= \epsilon
%\end{align*}
%Therefore we have found a $\delta$ for a given $\epsilon$ so that $|f(x)-f(c)|< \epsilon$ whenever $|x-c| < \delta$.  The case when $m=0$, follows similar, but simpler logic.

\section*{Proof of Bolzano's Theorem}\footnote{Refrenced from: \url{http://www.cut-the-knot.org/fta/brodie.shtml}}


Given $f$ continuous on some closed interval $[a,b]$, with $f(a)<0<f(b)$ and let $S$ be a set of numbers $x$ in $[a,b]$, where $f(x)<0$.  $S$ cannot be empty since it will at least contain $a$, and $S$ is bounded since it is a subset of a closed interval.  Then denote the least upper bound by $c$.\\

Three possibilities exist for $f(c)$:
\begin{itemize}
\item $f(c)<0$
	\begin{itemize}
		\item Then $c$ must be a member of $S$ thus there must exist an open interval containing $c$, where $f$ is negative, but $c$ is the least upper bound.  Therefore $f(c) \nless 0$
	\end{itemize}

\item $f(c)>0$
	\begin{itemize}
	\item Now if $f(c)>0$ then there is an open interval $(c-\delta, c+\delta)$ containing $c$ where $f>0$, therefore there is some value less than $c$ which would have to be in $S$.  But there cannot be any point from $(c-\delta, c+\delta)$ in $S$, because $S$ contains only points such that $f<0$.  Therefore $f(c) \ngtr	 0$.
	\end{itemize}

\item $f(c)=0$
	\begin{itemize}
	\item Which must be the conclusion since both $f(c)<0$ and $f(c)>0$ were contradictions.
	\end{itemize}
\end{itemize}

\subsection*{Sign Preserving property of a Continuous Function}
This property states that if $f$ is continuous at $a$ and $f(a) \neq 0$, then there is an open interval $(a-\delta,a+\delta)$ such that $f$ has the same sign as $f(a)$ for every $x$ in $(a-\delta,a+\delta)$.\\

\emph{Proof:}\footnote{Found in: \url{https://www.math.auckland.ac.nz/class255/08s1/01s2/H5.pdf}}

Suppose $f(a)<0$, by continuity there exists a $\delta>0$ for each $\epsilon>0$ such that $f(a)-\epsilon<f(x)<f(a)+\epsilon$, whenever $a-\delta<x<a+\delta$.  Let $\epsilon = -\frac{f(a)}{2}$ then $\frac{3}{2}f(a) < f(x) < \frac{1}{2} f(a)$, whenerver $a-\delta<x<a+\delta$ (recall $f(a)$ is negative and $-\frac{3}{2} < -\frac{1}{2}$).  Now we take the corresponding $\delta$ and $f(x)<0$ in the interval, $a-\delta<x<a+\delta$.  The proof for $f(x)>0$ follows the same reasoning, but with $\epsilon = \frac{f(a)}{2}$.

\section*{Proof of Base Case}
%\begin{wrapfigure}{r}{8cm}%
%\definecolor{qqwuqq}{rgb}{0.,0.39215686274509803,0.} \begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=0.8cm,y=0.8cm]
%\draw[->,color=black] (-4.604132231404959,0.) -- (4.784297520661155,0.);
%\foreach \x in {-4.,-3.,-2.,-1.,1.,2.,3.,4.}
%\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt);
%\draw[->,color=black] (0.,-2.9905785123966946) -- (0.,4.249090909090907);
%\foreach \y in {-2.,-1.,1.,2.,3.,4.}
%\draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt);
%\clip(-4.604132231404959,-2.9905785123966946) rectangle (4.784297520661155,4.249090909090907);
%\draw[line width=1.2pt,color=qqwuqq,smooth,samples=100,domain=-4.604132231404959:4.784297520661155] plot(\x,{((\x)+2.0)*((\x)-1.0)*(\x)});
%\begin{scriptsize}
%\draw[color=qqwuqq] (-2.157851239669423,-2.7757024793388436) node {};
%\end{scriptsize}
%\end{tikzpicture} \end{wrapfigure}%

Considering the base case $k=0$ of the induction on $k$ of $d=2^k \cdot m$, then $P(x)$ is degree $d=m$,
where $m$ is odd. 

Recall Bolzano's theorem states that a continuous function, $f$, takes values $f(a)<0$ at some point $a$ and $f(b)>0$ at some point $b$ must be 0 at some point.

Its already been shown that any polynomial $P(x)$ is continuous.  So we need to show that somewhere $P<0$ and somewhere else $P>0$.

\begin{align*}
\lim_{x \rightarrow -\infty} P(x) &= \lim_{x \rightarrow -\infty}  x^d + r_{d-1}x^{d-1} + \cdots +r_1x +r_0 \\
&= \lim_{x \rightarrow -\infty} x^d \left( 1 + \frac{r_{d-1}}{x^{d-1}} + \cdots +\frac{r_1}{x} + \frac{r_0}{x^d} \right) \\\\
\text{Recall: }\lim_{x \rightarrow -\infty} x^d \rightarrow -\infty \\
\lim_{x \rightarrow -\infty} 1 + \frac{r_{d-1}}{x^{d-1}} + \cdots +\frac{r_1}{x} + \frac{r_0}{x^d} = 1 \\\\
&\Rightarrow \lim_{x \rightarrow -\infty} P(x) \rightarrow - \infty
\end{align*}

Thus since $P(x)$ is continuous we know there is some distinct value such that, $P(a)<0$.

The proof that there exists some place $P>0$ follows the same reasoning. I.e.,
\begin{align*}
\lim_{x \rightarrow \infty} P(x) &= \lim_{x \rightarrow \infty}  x^d + r_{d-1}x^{d-1} + \cdots +r_1x +r_0 \\
&=\lim_{x \rightarrow \infty} x^d \left( 1 + \frac{r_{d-1}}{x^{d-1}} + \cdots +\frac{r_1}{x} + \frac{r_0}{x^d} \right) \\
&= \rightarrow  \infty
\end{align*}

Therefore, for $d$ odd, $P(x)=0$ for some value between $x \in (-\infty,\infty)$.





\end{appendices} 
\end{document}


%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%
% my explanation of the complex root
%

\subsection*{The Complex root}

Without a loss
of generality suppose that the root occurs when $i=r$, $j=l$, therefore
\[
y_{t,r,l}=x_{r}+x_{l}+tx_{r}x_{l}
\]
is complex for some $t$. So suppose 
\[
t=0
\]
then 
\[
x_{r}+x_{l}
\]
must be complex. Now Suppose 
\[
x_{r}+x_{l}
\]
is real, that would mean that the imaginary parts cancel out, but
when I take 
\[
x_{r}x_{l}
\]
the imaginary parts would not cancel out, therefore $x_{r}x_{l}$
is also complex for some $t$. Since both $x_{r}+x_{l}$ and $x_{r}x_{l}$
are complex numbers. To show that $x_{r}$ or $x_{l}$ is complex
consider the equation 
\[
x^{2}-(x_{r}+x_{l})x+x_{r}x_{l}
\]
Notice that the coefficients of this equation are complex. So define
them as such 
\begin{align*}
x_{r}+x_{l}=u+iv\\
x_{r}x_{l}=w+iz
\end{align*}
where $u,v,z,w$ are real valued. Substituting these values into the
equation gives: 
\[
x^{2}-(u+iv)x+w+iz
\]
Applying the quadratic formula to this equation: 
\begin{align*}
x & =\frac{u+iv\pm\sqrt{(u+iv)^{2}-4(w+iz)}}{2}\\
 & =\frac{u+iv\pm\sqrt{u^{2}+2iuv-v^{2}-4w-4iz}}{2}\\
 & =\frac{u+iv\pm\sqrt{\underbrace{u^{2}-v^{2}-4w}_{p}+i(\underbrace{2uv-4z}_{q})}}{2}\\
 & =\frac{u+iv\pm\sqrt{p+iq}}{2}\\
 & =\frac{u+iv\pm(p'+iq')}{2}\\
\end{align*}

Thus the roots of $x^{2}-(u+iv)x+w+iz=x^{2}-(x_{r}+x_{l})x+x_{r}x_{l}$
are complex. Notice in the third step I set $u^{2}-v^{2}-4w=p$ and
$2uv-4z=q$. Further a critical and subtle part of the above is that
the square root of complex numbers is complex, i.e if $\sqrt{p+iq}$
is complex then there exist some $p',q'$ such that $p'+iq'=\sqrt{p+iq}$.
A proof of this is included in the appendix.\\

The roots of $x^{2}-(x_{r}+x_{l})x+x_{r}x_{l}$ are complex, but how
does that help in figuring out if $x_{r}$ or $x_{l}$ is complex.
Well let's now find the roots of $x^{2}-(x_{r}+x_{l})x+x_{r}x_{l}$
\begin{align*}
x & =\frac{x_{r}+x_{t}\pm\sqrt{(x_{r}+x_{l})^{2}-4x_{r}x_{l}}}{2}\\
 & =\frac{x_{r}+x_{t}\pm\sqrt{x_{r}^{2}+2x_{r}x_{l}+x_{l}^{2}-4x_{r}x_{l}}}{2}\\
 & =\frac{x_{r}+x_{l}\pm\sqrt{x_{r}^{2}-2x_{r}x_{l}+x_{l}^{2}}}{2}\\
 & =\frac{x_{r}+x_{l}\pm\sqrt{(x_{r}-x_{l})^{2}}}{2}\\
 & =\frac{x_{r}+x_{l}\pm(x_{r}-x_{l})}{2}\\
 & =x_{r},x_{l}
\end{align*}

So the roots of $x^{2}-(x_{r}+x_{l})x+x_{r}x_{l}$ are non other than
$x_{r}$ and $x_{l}$, which must be complex.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~























%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%
% Delete below?




\subsection*{The three roots}

Suppose that $s_{1}$ is indeed the complex root. Then I can factor
$s_{1}$ out of our original equation $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$.
When I do this I arrive at 
\[
(x-s_{1})(x^{2}-\rho_{1}x-\rho_{2})
\]
Now I can apply the same process I went through and find that $x^{2}-\rho_{1}x-\rho_{2}$
has a \char`\"{}complex''\footnote{Complex is in quotes, because the imaginary part might be zero}
root, suppose this time it is $s_{3}$, I can factor out this term
arriving at: 
\[
(x-s_{1})(x-s_{3})(x-\gamma)
\]
Again I apply this process to $x-\gamma$ and I find that $\gamma=s_{2}$
is complex and I have a full set of roots.

The previous steps for identifying how many roots $x^{3}-r_{1}x^{2}+r_{3}x-r_{3}$
has, are equivalent to the general proof for the Fundamental Theorem
of Algebra.

% now work through the fact that g is symmetric then elementary symmetric, then use the fact that there are two roots in the complex blah blah

% does this proof only work for polynomials greater than a certain degree, find out.
\begin{enumerate}
\item The field of complex numbers is closed under algebra. 
\end{enumerate}
These statements are all equivalent, recall a field\footnote{A field is an algebraic structure with a form of addition, subtraction,
multiplication and division, satisfying the commutative and distributive
properties} $K$ is called \emph{algebraically closed} if every non-constant
polynomial $f(c)\in K[x]$ has a root in $K$.

So a field, $K$ is algebraically closed if the roots of every non-constant
polynomial (e.g. $x^{2}+1$) are in $K$.

%\paragraph*{Comprehension check:} With only rereading the definition and explanation of algebraic closure, think of an example of why the field of integers is not closed under algebra.\footnote{$x^2+1$ would work because the roots of $x^2+1$ are $x=\pm i$}

From the definition of algebraically closed it follows that \textbf{statement
2} and \textbf{statement 3} are identical, in other words \textbf{statement
3} is just shorthand for stating \textbf{statement 2}. Remember that
a complex number with no imaginary part is a real number.

\textbf{Statement 1} is how the Fundamental Theorem of Algebra is
typically stated in the secondary setting and it really is a corollary
of \textbf{statement 2}. As in the previous section I can factor out
the complex number I find. This leaves a linear term multiplying a
polynomial which is one degree less than our original. I can then
find the complex root of this new polynomial, and factor it out. I
can repeat this process, but I can only do it $n$ times, thus I get
$n$ roots.

\section*{Algebraic proof of The Fundamental Theorem of Algebra}

I will attempt to prove that every non-constant polynomial with complex
coefficients has a complex root in a manner that does not require
rigorous mathematical study.\footnote{Many thanks to \url{https://www.artofproblemsolving.com/wiki/index.php?title=Fundamental_Theorem_of_Algebra##Algebraic_Proof}}

%It is necessary to assume the following
%\begin{enumerate}
%\item Every odd degree polynomial with real coefficients has at least one real root.\footnote{One may prove this with the Intermediate Value Theorem} \emph{Include example picture, proof using IVT in appendix}
%\item Every polynomial of degree two with complex coefficients has a complex root.\footnote{Proven by the fact that every polynomial with real coefficients has a complex root} \emph{Include example picture}
%\end{enumerate}
%\paragraph*{Lemma} 

\subsection*{Set up}

Think of a polynomial with complex coefficients (e.g. $x^{2}+4x+2$,
or $(1+2i)x+3$), now think of another. Now think of every polynomial
with complex coefficients and let's let $C(x)$ represent one of them,
we don't know which, but it is one of them.\\


\subsubsection*{Set up summary}

To summarize the above, since the goal is to show that any polynomial
with complex coefficients has a complex root then I can build a polynomial
with real coefficients by multiplying my polynomial with complex coefficients
with its conjugate. Then if I show that this new polynomial with real
coefficients has a complex root, I'm done.

\subsection*{Proof by Induction}

Suppose the degree of $R(x)$ is $d=2^{n}q$, where $q$ is odd and
$n$ is a natural number, $\mathbb{N}$. %$R(x)$ will always have an even degree, because the degree of $d(R(x))=d(C(x))*2$.  
Then lets induct on $n$, in other words, we show that the first case
is true, then we show that each case follows from the previous, therefore
each case must be true.

\pagebreak{}

\subsubsection*{Base Case}



\subsubsection*{Induction Step}

Now for the fun part. Suppose again that $d=2^{n}q$, where $q$ is
odd and $n>0$, however lets \char`\"{}believe\char`\"{} that the
theorem has been proven when the degree is $2^{n-1}q'$ where $q'$
is odd. Therefore, we can use the \char`\"{}fact\char`\"{} (induction
hypothesis) that any polynomial less than or equal to degree $2^{n-1}q'$
has a complex root.\\

Let's start by splitting $R(x)$ into all its linear terms like we
did previously, we will also put a factor $a$ out front. This $a$
would correspond to the coefficient of $x^{n}$. This step requires
that we \char`\"{}find\char`\"{} a field, $\mathbb{F}$ over which
we can split the polynomial into its linear factors.\footnote{For completeness, we are finding the smallest field extension of $\mathbb{C}$,
such that $R(x)$ decomposes into linear factors.} In math jargon this would look like:

\[
c_{1}(x)c_{2}(x)\cdots c_{n}(x)\bar{c_{1}(x)}\bar{c_{2}(x)}\cdots\bar{c_{d}(x)}
\]
where the $c_{i}(x)$ are linear functions in the field, $\mathbb{F}$.
Now let $x_{1}$ be the root of $c_{1}(x)$ in the field, $\mathbb{F}$,
so on and so forth. So the roots of $R(x)$ in the field $\mathbb{F}$
are $x_{1},\ldots,x_{d}$. If this field, $\mathbb{F}$ were the field
of polynomials with real valued coeffcients, $\mathbb{R}[x]$ we would
be done, but its not. So now we need to figure out how to relate these
roots we found back to the real or complex field. In order to do that
we use the same trickery we used previously.\\

Let $s$ be an arbitrary real number and let $y_{s,i,j}=x_{i}+x_{j}+sx_{i}x_{j}$,
where $1\leq i<j\leq d$. Now define: 
\[
G_{t}(x)=(x-y_{t,1,2})(x-y_{t,1,3})\cdots(x-y_{t,1,d})(x-y_{t,2,3})\cdots(x-y_{t,2,d})\cdots(x-y_{t,d-1,d})
\]
This may be succinctly written as: 
\[
G_{t}(x)=\Pi_{1\leq i<j\leq d}(x-y_{t,i,j})
\]

The $G_{t}(z)$ we saw previously was constructed in the exact same
manner. It is important to know that the coefficients of $G_{t}$
are symmetric in $x_{1},x_{2},\ldots,x_{d}$. To review, symmetric
in this context means that the variables could be interchanged in
any way and we would not need to change the coefficients, in fact,
we would have the same polynomial.\footnote{For further investigation read about symmetric polynomials.}
In other words the coefficient of $x_{1}$ would be the same as the
coefficient as $x_{d}$, likewise the coefficient of $x_{2}x_{1}$
would be the same as $x_{2}x_{d}$, so on and so forth. It might help
to think of Pascal's triangle. Let's look at a case when $d=2$.

\subsubsection*{Example of $G_{t}(x)$ when $d=2$ }

This example is very similar to the previous example section, but
is included to help concretize the proof. In this example we will
derive $G_{t}(x)$ from the original polynomial 
\[
f(x)=x^{2}+bx+c
\]
where $b,c$ are real valued.

Following the layout of the proof first we find the splitting field
over which $f(x)$ splits. From which we get 
\[
f(x)=(x-x_{1})(x-x_{2})
\]
At this point we don't know what type of numbers $x_{1}$ or $x_{2}$
are. They could be complex, they could be something else altogether.
Let's now look at what $G_{t}(x)$ looks like when $d=2$. Since 
\[
G_{t}(x)=(x-y_{t,1,2})
\]
let's first create $y_{t,1,2}$: 
\[
y_{t,1,2}=x_{1}+x_{2}+tx_{1}x_{2}
\]
so 
\[
G_{t}(x)=x-x_{1}-x_{2}-tx_{1}x_{2}
\]

There are many important features to notice about $G_{t}(x)$. $G_{t}(x)$
is a polynomial in $x$, its degree is one, so it is one degree lower
than our original polynomial $f(x)$, this feature will come in handy
when we use our induction hypothesis. However in order to use our
induction hypothesis we need to know that the coefficients of $G_{t}(x)$
are real valued. The trick we use here is rather clever.

Recall that $f(x)=(x-x_{1})(x-x_{2})$, which if we distribute twice
we get 
\[
f(x)=x^{2}+x(-x_{1}-x_{2})+x_{1}x_{2}
\]
further 
\[
f(x)=x^{2}+bx+x
\]
Therefore

\begin{align*}
b=-x_{1}-x_{2}\\
c=x_{1}x_{2}
\end{align*}

The $b,c\in\mathbb{R}$ are real valued. Thus $-x_{1}-x_{2},x_{1}x_{2}$
must be real valued. This does not necessarily mean that $x_{1}\in\mathbb{R}$
or that $x_{2}\in\mathbb{R}$ are real valued. For example if $x_{1}=1+i$
and $x_{2}=1-i$ then $-x_{1}-x_{2}=-2$ and $x_{1}x_{2}=2$. But
it does mean that the coefficents of 
\[
G_{t}(x)=x-(x_{1}+x_{2})+x_{1}x_{2}
\]
namely $x_{1}+x_{2}$ and $x_{1}x_{2}$ are real valued. Here you
saw a concrete version but the fact that $x_{1}+x_{2}$ and $x_{1}x_{2}$
are real valued stems from Viéte's formulas, which explain how to
rewrite the coefficients of any symmetric polynomial as a elementary
symmetric polynomials.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% This is incorrect!
%To help with this idea of symmetric coefficients, when $d=3$ we have
%$$
%G_s(x) = (x-x_1-x_2-sx_1x_2)(x-x_2-x_3-sx_2x_3)
%$$
%which expands to
%$$
%x^2+x_2x_3+x_1x_3 + x_1x_2 + x_2^2 -xx_3 - 2x x_2-x x_1 + 2sx_1x_2x_3 + sx_2^2 x_3 + s x_1 x_2^2 - s x x_2 x_3 - s x x_1 x_2 + s^2 x_1 x_2^2 x_3
%$$
%Notice how we could interchange $x_1$ and $x_2$ and the coefficients would remain the same.  In this example the coefficients are all real, but we can't assume the same will occur when $d>3$.\\
%The same analysis that applied in the example when $d=2$ and $d=3$ applies when $d>3$, we reduce the degree of the polynomial using $G_s(x)$, check that the coefficients are real then use the induction hypothesis.\\
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% explanation of induction
%In order to do that we need to use induction\footnote{This step is not completely necessary for this example, however understanding this process will help immensely when we turn towards the proof for any polynomial}.    Remember induction is like climbing a ladder, we show that the first rung is true, then we show that if a previous rung existed then the next one does as well.  Suppose the degree of our polynomial was one this will act as our base case.  A polynomial of degree one is a non-horizontal line, which will cross the $x-$axis once, therefore we will have one root for a polynomial of degree one.  Next we will use the induction hypothesis that a polynomial of degree two has two complex/real roots to show that our third degree polynomial has three complex/real roots.
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\subsubsection*{Coefficents of $G_{t}(x)$ are real}

Now back to: 
\[
G_{t}(x)=(x-y_{t,1,2})(x-y_{t,1,3})\cdots(x-y_{t,1,d})(x-y_{t,2,3})\cdots(x-y_{t,2,d})\cdots(x-y_{t,d-1,d})
\]
we need to the coefficients of $G_{t}(x)$ to be real and we need
the degree to be $2^{n-1}q'$, where $q'$ is odd. %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% Delete ?
%We have some motivation to believe that the coefficients of $G_s(x)$ are real.  We will have to take on belief that they are indeed real.\footnote{For the curious, to the best of my knowledge it has not be proven that we may do this.  What the proofs so far claim is that the coefficients of $G_s(x)$ may be viewed as elementary symmetric polynomials.  These polynomials have special formulas which Fran\c{c}ois Vi\'ete discovered in the late 1500's, which enable one to rewrite them as real numbers.  However these formulas only apply to the reals or complex numbers, not necessarily some field extension of the complex field, which is where we are using them.}  
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In order to use our induction step we need the coefficients of $G_{t}(x)$
to be real. Recall that the induction step states: 
\begin{center}
\fbox{\begin{minipage}[c]{30em}%
Polynomials of degree $2^{n-1}q'$ with real valued coefficients have
at least one complex root. %
\end{minipage}} 
\par\end{center}

%In the two examples we saw a concrete version of $G_s(x)$ in these example the terms when multiplied out were composed of symmetric polynomials in the roots, the $x_i$'s.  

Symmetry is going to play an important role, in enabling us to be
able to apply the induction hypothesis. Recall that symmetry in this
situation is when we are able to alternate the coefficients and maintain
equality.



\paragraph*{Return to the example for $d=2$}

Before we move on let's make sense of this step with a concrete example.
We want to use the induction hypothesis which loosely stated for this
example is: \\

\emph{Any polynomial with real valued coefficents and degree \char`\"{}less
than'' the original polynomial has at least one complex root.}\footnote{Less than is put in quotations, because strictly speaking this is
not true, we are inducting on the even part of the degree, the $2^{n}$
part. As long as the new polynomial has an $n'<n$ we can utilize
our induction hypothesis.}\\

Let's check everything our induction hypothesis requires. 
\begin{enumerate}
\item Real valued coefficients: Because the $G_{t}(x)$ is symmetric the
coefficients of $G_{t}(x)$ multiplied by values from the integers
may be related to the coefficients of the original polynomial which
are real. 
\item Lesser degree: $G_{t}(x)$ is constructed such that it will be \char`\"{}less
than'' the degree of the orginal polynomial.\footnote{see previous footnote} 
\end{enumerate}
Since the criterion for the induction hypothesis is satisfied we know
that $G_{t}(x)$ has at least one complex root, so 
\[
-(x_{1}+x_{2})+sx_{1}x_{2}
\]
must be complex. In order to figure out whether $x_{1}$ or $x_{2}$
is complex we need to analyze $-(x_{1}+x_{2})+sx_{1}x_{2}$. In identifying
that $-(x_{1}+x_{2})+sx_{1}x_{2}$ is complex we did not consider
$s$ at all. Thus, whether or not $x_{1}$ or $x_{2}$ is complex
does not depend on $s$. So if $s=0$ then 
\[
-(x_{1}+x_{2})
\]
must be complex. So $x_{1}+x_{2}$ must be complex. Now let's assume
$x_{1}=u+iv$ and $x_{2}=z+iw$, then if $x_{1}+x_{2}$ is real valued
we must have $iv=iw$ in other words the imaginary parts must cancel
out. Then: 
\begin{align*}
x_{1}x_{2} & =(u+iv)(z+iw)\\
 & =(u+iv)(z-iv)\\
 & =uz-ivu+izu-i^{2}v^{2}\\
 & =uz+v^{2}+i(vz-uv)
\end{align*}
which is complex unless $vz=uv\Rightarrow z=u$ for $v\neq0$.

In order to identify if $x_{1}$ or $x_{2}$ is our complex root we
would need to use the induction hypothesis again, this will be detailed
further in the paper.

\paragraph*{Back to the proof}

We have a very similar situation for the generalized proof. $G_{t}(x)$
is constructed so that the coefficients are symmetric. This stems
from the fact that 
\[
y_{t,i,j}=x_{i}+x_{j}+tx_{i}x_{j}
\]
is symmetric in the $x_{i},x_{j}$. If we then multiply all the combinations
of $x-y_{t,i,j}$, where $i<j$ then we will have polynomial that
is symmetric in all the $x_{i}$'s.

Since the coefficients are symmetric and we can add and multiply them
in the usual way then by Viete's formulas we can express the additive
and multiplicative combinations of the $x_{1},x_{2},\ldots x_{d}$
as elementary symmetric polynomials\footnote{By the Fundamental Theorem of Symmetric Polynomials},
just as we did in the examples. These elementary symmetric polynomials,
have coefficients which are integer valued. Additionally, the elementary
symmetric polynomials relate to the coefficients of the original polynomial.
The coefficients of $R(x)$ are real, therefore the elementary symmetric
polynomials are real. Since the elementary symmetric polynomials have
coefficients which are integer valued and consist of additive and
multiplicative combinations of the $x_{1},x_{2},\ldots x_{d}$, the
elementary symmetric polynomials must also real. From which it follows
that the coefficients of $G_{t}(x)$ must therefore be real-valued.\\

Put another way we related the coefficients of $G_{t}(x)$ to the
coefficients of our original polynomial, $R(x)$. Since the coefficients
of $R(x)$ are real, the coefficients of $G_{t}(x)$ must be real.
Note that the coefficients of $G_{t}(x)$ are additive and multiplicative
combinations of $x_{1},x_{2},\ldots x_{d}$ multiplied by some integer.
\\

Thus the coefficients of $G_{t}(x)$ are real. In order to use the
induction hypothesis we need to figure out the degree of $G_{t}(x)$.

\subsubsection*{Degree of $G_{t}(x)$}

How many terms there are in $G_{t}(x)$?

There are $d-1$ terms that have $i=1$, then there are $d-2$ terms
that have $i=2$, because $i<j$. So there are $d-3$ terms that have
$i=3$ and so on. 
\[
G_{t}(x)=\underbrace{(x-y_{s,1,2})(x-y_{s,1,3})\cdots(x-y_{s,1,d})}_{d-1}\underbrace{(x-y_{s,2,3})\cdots(x-y_{s,2,d})}_{d-2}\cdots(x-y_{s,d-1,d})
\]

Thus I have a decreasing sequence, since I am trying to find the degree
of $G_{t}(x)$ I take the $d-1$ terms and add them to $d-2$ terms,
so on and so forth. This gives a familiar series: 
\[
1+2+3+4+\cdots+(d-2)+(d-1)
\]

The trick for finding the sum of this sequence is often attributed
to Gauss. First define: 
\[
S_{n}=1+2+3+4+\cdots+(d-2)+(d-1)
\]

then rewrite $S_{n}$ as 
\[
S_{n}=(d-1)+(d-2)+\cdots+4+3+2+1
\]

add $S_{n}$ to itself, but I need to be clever here, take notice
of how the additive terms were grouped 
\[
2S_{n}=(1+(d-1))+(2+(d-2))+(3+(d-3))+\cdots((d-2)+2)+((d-1)+1)
\]

Perform the arithmetic inside each set of parenthesis 
\[
2S_{n}=d+d+d+\cdots+d+d
\]

Recall that there were $d-1$ terms in $S_{n}$ and I did nothing
to alter that since I paired up each component: 
\[
2S_{n}=\underbrace{d+d+d+\cdots+d+d}_{d-1}
\]

A model to make multiplication understandable is that multiplication
is repeated addition, applying that model here: 
\[
2S_{n}=d(d-1)
\]

Which results in: 
\[
S_{n}=\frac{d(d-1)}{2}
\]

So the sum of the series $S_{n}=1+2+3+4+\cdots+(d-2)+(d-1)$ is $\frac{d(d-1)}{2}$.
Recall that 
\[
d=2^{n}q
\]
so 
\[
\frac{d(d-1)}{2}=\frac{2^{n}q(d-1)}{2}=2^{n-1}q(d-1)
\]
but $q(d-1)$ is an odd number so 
\[
2^{n-1}q(d-1)=2^{n-1}q'
\]

Thus $G_{t}(x)$ has real coefficients and a degree less than or equal
to $2^{n-1}q'$, therefore I can apply the induction hypothesis.


%$y_{s,1,1} = 2x_1 + s x_1^2$ and $y_{s,1,2} = x_1 +x_2 + s x_1 x_2$ and $y_{s,2,2} = 2x_2 + s x_2^2$.  Therefore 
%\begin{align*}
%G_s(x) &= (x-y_{s,1,1})(x-y_{s,1,2})(x-y_{s,2,2}) \\
%&= x^3 \\
%& \hspace{1em} -(s(x_1^2+x_1 x_2+x_2^2 \\
%& \hspace{10em} +3(x_1+x_2))x^2 \\
%& \hspace*{1em} + (s^2(x_1 x_2^3 + x_1^3 x_2) \\
%& \hspace{9em} + s(x_1^3 +x_2^3 \\
%& \hspace{15em} + 5(x_1 x_2^2 + x_1^2 x_2)) \\
%& \hspace{23em} +2(x_1^2+x_2^2) + 8x_1 x_2)x \\
%& \hspace*{1em} -s^3 x_1^3 x_2^3 \\
%& \hspace{5em} - 3s^2(x_1^2x_2^3 + x_1^3 x_2^2) \\
%& \hspace{14em} -2s(x_1 x_2^3 + 4x_1^2 x_2^2 + x_1^3 x_2) \\
%& \hspace{26em} -4(x_1 x_2^2 + x_1^2 x_2)
%\end{align*}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% Maybe include? probably not
%\section*{So What? An argument for the Fundamental Theorem of Algebra}  
%The Fundamental Theorem of Algebra states that there is at least one complex root.  What if there was not?  In other words let's suppose that every equation has a solution, and the solution is not in the complex numbers, where would it be?
%\subsection*{A tour of different fields}
%\emph{Take readers on a tour of ordered and non-ordered fields}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~