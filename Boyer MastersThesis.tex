%% LyX 2.2.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[12pt]{article}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose}
\setcounter{secnumdepth}{0}
\usepackage{wrapfig}
\usepackage{calc}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amssymb}
\PassOptionsToPackage{normalem}{ulem}
\usepackage{ulem}
\usepackage[unicode=true,
 bookmarks=false,
 breaklinks=false,pdfborder={0 0 1},backref=section,colorlinks=false]
 {hyperref}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.


\usepackage{exsheets}
\usepackage[toc,page]{appendix}
%\usepackage{amsthm} 
\usepackage{array}
\mathtoolsset{showonlyrefs} 
\usepackage{graphicx}
\usepackage{listings}
%\usepackage{indentfirst}
%\usepackage{setspace}
%\doublespacing
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{dcolumn}
\newcolumntype{2}{D{.}{}{2.0}}


\usepackage{pgf}
\usepackage{tikz}\usepackage{mathrsfs}
\usetikzlibrary{arrows}


\newcommand{\ssol}{\vspace{3em}}
\newcommand{\lsol}{\vspace{10em}}
\newcommand{\blnk}{{\underline {\hspace{1.5in}}}}
\newcommand{\Lim}[1]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle \lim_{#1}\;$}}}


%\usepackage{mcode}

 % Turns off automatic numbering of sections.

%\SetupExSheets{headings=block}








\makeatother

\begin{document}

\section{Comparing the integers and the polynomials with coefficients in the
rational, real or complex number fields}

Suppose I want to solve a polynomial equation
\[
\frac{a_{d}}{b_{d}}x^{d}+\frac{a_{d-1}}{b_{d-1}}x^{d-1}+\ldots+\frac{a_{0}}{b_{0}}=0
\]
with rational numbers $a_{1}$ as coefficients. I could turn it into
a polynomial equation with integer coefficients just by multiplying
both sides of the above equation by a common multiple of $\left\{ b_{d},b_{d-1},\ldots,b_{0}\right\} $.
So every root of a polynomial with rational coefficients is a root
of a polynomial with integer coefficients. A deeper fact, called Gauss's
Lemma, says that any factorization of a polynomial
\[
a_{d}x^{d}+a_{d-1}x^{d-1}+\ldots+a_{0}
\]
with integer coefficients $a_{i}$ that can't be factored into two
polynomial factors with integer coefficients cannot be factored into
two polynomial factors with rational coefficients. 
These two facts led mathematicians to study systems of polynomials
with coefficients in a number system in which you could add, subtract,
multiply $and$ divide. They called such a system a $field$. Besides
the field of rational numbers, denoted $\mathbb{Q}$, other examples
of fields are the field $\mathbb{R}$ of real numbers and the field $\mathbb{C}$
of complex numbers. They also noticed that the system $\mathbb{F}\left[x\right]$
of polynomials with coefficients in a field $\mathbb{F}$ behaves
a lot like the most familiar number system, the integers. Both have
unique factorization into prime factors, greatest common divisors,
least common multiples, etc.

\section{The Mapping $\Phi_{b}$}

What I want to do is create a function (also called a mapping) that
takes a polynomial with integer coefficients to the (corresponding)
number system formed by its coefficients. For example I could ask
that the mapping take any polynomial with integer coefficients to
the integer I get by substituting an integer $b$ for every "$x$''. As an example of this, if $b=10$, the mapping would send $2x^{2}+3x+4\xrightarrow[\Phi_{10}]{}234$.
It is common to use the letter $\mathbb{Z}$ to denote the integer
number system and $\mathbb{Z}\left[x\right]$ to denote the ring of
polynomials with integer coefficients. 

\subparagraph{Example}

We would then denote the function described just above as
\[
\begin{array}{c}
\Phi_{10}:\mathbb{Z}[x]\rightarrow\mathbb{Z}\\
f(x)\mapsto f(10).
\end{array}
\]
In other words, if $f(x)=5x^{4}+4x^{3}+2x+1$, 

\[
\Phi_{10}(f(x))=f(10)=5(10)^{4}+4(10)^{3}+2(10)+1=54021.
\]

More generally
\[
\Phi_{b}(f(x))=f(b)
\]


\subsection*{Properties of $\Phi_{b}$ }

The following proofs use two polynomials $f_{1},f_{2}$. with coefficients
in any of the numbersystems $\mathbb{Z}$, $\mathbb{Q}$, $\mathbb{R}$
or $\mathbb{C}$. 
\begin{itemize}
\item $\Phi_{b}$ preserves addition
\end{itemize}
\begin{align*}
\Phi_{b}(f_{1}(x)+f_{2}(x)) & =f_{1}(b)+f_{2}(b)\\
 & =\Phi_{b}(f_{1}(x))+\Phi_{b}(f_{2}(x))
\end{align*}

\begin{itemize}
\item $\Phi_{b}$ preserves multiplication
\end{itemize}
\begin{align*}
\Phi_{b}(f_{1}(x)f_{2}(x)) & =f_{1}(b)f_{2}(b)\\
 & =\Phi_{b}(f_{1}(x))\Phi_{b}(f_{2}(x))
\end{align*}

\begin{itemize}
\item A polynomial maps to zero if and only if the polynomial is divisible
by $x-b$
\end{itemize}
To see why this last statement is true, suppose we are given $f(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{0}$,
and a number $r$ such that 
\[
f(r)=0.
\]

By long division of polynomials 
\[
a_{n}x^{n}+\cdots+a_{0}=(x-r)\text{·}(c_{n-1}x^{n-1}+\cdots c_{0})+\text{constant}
\]
Substituting $x=r$ into the both sides of the equation above we get
\[
0=0+\text{constant}.
\]
So by necessity $\text{constant}=0$, therefore 
\[
f(x)=(x-r)(c_{n-1}x^{n-1}+\cdots c_{0}).
\]


\subsection*{A rough comparison of prime factorization}

This section will compare prime factorization in the ring of integers
and the ring of polynomials with coefficients in a field. Prime factorization
is decomposing something into its constituent primes. Among the integers
we have the \emph{the fundamental theorem of arithmetic}, which says
that every positive integer has a unique prime factorization. In math
speak this states that any positive integer $k\geq2$, $k$ may be
rewritten as 
\[
k=p_{1}^{l_{1}}p_{2}^{l_{2}}\cdots p_{n}^{l_{n}}
\]
where the $p_{i}$'s are the $n$ distinct prime factors, each of
order $l_{i}$. One way to understand this is to think of it as taking
a positive integer, breaking it up into several unique parts, multiplying
those parts together and getting the same positive integer back. \\

Do the polynomials with coefficients in a field $\mathbb{F}$ have
a similar analogue? In this set-up, polynomials with only constant
terms play the role of $\text{\textpm1}$ in the integers, so that
prime factors are always polynomials of degree $d\geq1$. Well we
know we are looking to break up the polynomial into a bunch of parts,
each of which can't be broken up further\textendash multiply those
parts together and get our original polynomial.

For example in $\mathbb{Q}\left[x\right]$
\[
x^{2}-1=\left(x+1\right)\left(x-1\right)
\]
but 
\[
x^{2}-2
\]
is prime in the polynomial system in $\mathbb{Q}\left[x\right]$ since
$\sqrt{2}$ is not in the number system $\mathbb{Q}$. However in
$\mathbb{R}\left[x\right]$ we have the prime factorization
\[
x^{2}-2=\left(x+\sqrt{2}\right)\left(x-\sqrt{2}\right).
\]
Similarly the prime factorization
of $f(x)=x^{2}+1$ in $\mathbb{R}\left[x\right]$ is $x^2+1$. Since the square
of any real number is $\geq0$ 
\[
x^{2}+1
\]
is prime in $\mathbb{R}\left[x\right]$. However in the system of
polynomials $\mathbb{C}\left[x\right]$ we have the factorization
\[
x^{2}+1=\left(x+i\right)\text{·}\left(x-i\right).
\]

So what mathematicians have decided is that a polynomial $f\left(x\right)$
in $\mathbb{F}\left[x\right]$ is called irreducible (irreducible is the word mathematicians
use for when a polynomial is in this example "prime'') if the polynomial cannot
be written as a product
\[
f\left(x\right)=g\left(x\right)\text{·}h\left(x\right)
\]
where $g\left(x\right)$ and $h\left(x\right)$ are polynomials in
the same system $\mathbb{F}\left[x\right]$ and the degrees of $g\left(x\right)$
and $h\left(x\right)$ are both less than the degree of $f\left(x\right)$.
In other words, $f\left(x\right)$ is prime or irreducible if you
have any polynomial $g\left(x\right)$ in the same system $\mathbb{F}\left[x\right]$
whose degree is less than the degree of $f\left(x\right)$, and if
you divide $g\left(x\right)$ into $f\left(x\right)$, you will always
have a remainder. So the only thing that divides an irreducible polynomial
is the irreducible polynomial itself and the constant polynomials $a_{0}$.
Remind you of anything?

\subparagraph{Exercise:}

If you don't know already, try to figure out how to do prime factorization
in any system $\mathbb{F}\left[x\right]$ of polynomials where $\mathbb{F}$
is one of our fields. \\

The miracle is that every polynomial $f(x)$ in $\mathbb{R}\left[x\right]$
can be factored  into irreducible factors of degrees $1$ and $2$, by the quadratic formula.
%\[
%f(x)=(x-r_{1})^{l_{1}}(x-r_{2})^{l_{2}}\cdots(x-r_{i})^{l_{i}}
%\]
In fact, any polynomial $f(x)$ in
$\mathbb{C}\left[x\right]$ can be factored completely as
\[
f(x)=(x-r_{1})^{l_{1}}(x-r_{2})^{l_{2}}\cdots(x-r_{i})^{l_{i}},
\]
it doesn't matter whether the coefficients of $f(x)$ are real or
complex. We know this fact as the \emph{fundamental theorem of algebra}!
The fundamental theorem of algebra (roughly) states that every $d$-th
degree polynomial in $\mathbb{C}\left[x\right]$ has $d$ complex
roots. (Remember that real numbers are also complex numbers, it's
just that their imaginary part is zero). Tie this together with long
division of polynomials and we find that we could rewrite the polynomial
$f(x)$ as a product
\[
f\left(x\right)=c_{0}\left(x-s_{1}\right)\text{·}\cdots\text{·}\left(x-s_{d}\right)
\]
I.e., if the $r_{i}$'s are the roots of $f(x)$ and the $l_{i}$
are the multiplicities of the roots, then each root $r_{i}$ occurs
in the list 
\[
s_{1},s_{2},\ldots,s_{d}
\]
exactly $l_{i}$ times. 

\section{Toward a proof of the Fundamental Theorem of Algebra}

The Fundamental Thereom of Algebra is critical and the proof is often waved away as being beyond the scope. In order to follow the proof in its entirety an example is given as a preamble, which highlights all the necessary components of the proof, while giving some concreteness.

\section*{How many roots does $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$ have?}

Consider the following polynomial 
\[
p(x)=x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
how many roots does it have? Does it help if $r_{1},r_{2},r_{3}$
are real numbers? Because of the Fundamental Theorem of Algebra we know it has three roots in the complex field, but
we know that because of a theorem.  The following is a proof/derivation of the fact that $p(x)$ has three roots in the complex field. It will be a bit circuitous, but the idea is to give a concrete example of all the components necessary for a general algebraic proof.

Suppose for a moment that the roots of
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
are not real, not even complex, they lie in some field, $\mathbb{F}$,
so the operations of addition, subtraction, multiplication,
and division hold. Let's call these roots $a_{1},a_{2},a_{3}$.

Typically one would rewrite 
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
as: 
\[
(x-a_{1})(x-a_{2})(x-a_{3})
\]

Why can polynomials be written as linear factors?  Recall the previous section on the relationship between prime numbers and irreducible factors.  It is possible to factor a polynomial if it is factored in a large enough field.  In other words consider the analogy: suppose you are working on a car
in your garage. Everything is going fine, until you go to take a bracket
off, the tools you have don't work for the job at hand. So you call
up your friend he has the tools, so you tow your car to his place
and fix it there, then when you are done you drive your car back into
your garage. In this analogy your car is the polynomial and the garage
is the real numbers $\mathbb{R}$, when you tow your car to your friends
you are using what we will call a field extension, so that you can
use your friends tool, the splitting field, to take the car apart
and figure out how to fix (find all the roots) it.

\subsection*{Field Extension}

In the analogy the \emph{field extension} was towing t car
to a place where we can fix it. Suppose you have a playing field of
four numbers $\{0,1,2,3\}$, what can we do with these four numbers,
we could add them $1+1=2$, $1+2=3$, what about $0+1=1$ we have
an additive identity. What do you think should happen when I try $1+3=$?
Any answer could be correct. Mathematicians like to play games, so
they pick the situation that will continue the game. They decide to
make $1+3=0$ then we can play longer. because then $2+2=0$ as well.
In other words in this playing field we map the number 4 to the number
0, this implies the number 5 would be mapped to 1. I.e., 
\[
2+3=1
\]
\[
3+3=2
\]
\[
1+2+3=2
\]
\[
2+2+2+3=1
\]
so on and so forth. If you teach kids then your mind probably wandered
on the last equation in the series to repeated addition and multiplication.
Exactly, now we might be able to get a sense of multiplication in
this playing field. 
\begin{align*}
2+2+2+3 & =1\\
2\cdot3+3 & =1\\
(2+1)\cdot3 & =1\text{ Using distributive property}\\
3\cdot3 & =1
\end{align*}
If $3\cdot3=1$, then multiplying both sides by 3, we get 
\begin{align*}
3\cdot3\cdot3 & =3\\
1\cdot3 & =3
\end{align*}
We have a multiplicative identity. What else is (not) possible with
this playing field?\\

So we have addition, we have multiplication, let's try division. If
we rely on our models about division we might have problems, we need
to really understand what division is. The Egyptians had a pretty
good idea, they were one of the first peoples to record their method
of division. They framed it as a multiplication problem with the multiplier
(or multiplicand)\footnote{Why does it not matter if it is the multiplier or multiplicand that
is missing?} missing. So 
\[
3\div3
\]
would be 
\[
3\cdot?=3
\]
The Egyptians then used their times tables to deduce the the answer.
At its core this is what division is, multiplication in reverse, multiplication
in-reverse, multiplication in-verse. We invert multiplication, this
is why in dividing fractions we flip and multiply, division \underline{is}
inverting and multiplying. So that saying: \char`\"{}Dividing fractions
don't know why, flip the second number and multiply'' is frighteningly
ironic.\\

From here it will help to discover division if we have our own times
table: 
\begin{center}
\global\long\def\arraystretch{1.3}
 \setlength{\doublerulesep}{0pt} %
\begin{tabular}{r||2|2|2|2}
$\times$  & 0  & 1  & 2  & 3 \tabularnewline
\hline 
\hline 
0  & 0  & 0  & 0  & 0 \tabularnewline
\hline 
1  & 0  & 1  & 2  & 3 \tabularnewline
\hline 
2  & 0  & 2  & 0  & 2 \tabularnewline
\hline 
3  & 0  & 3  & 2  & 1 \tabularnewline
\hline 
\end{tabular}
\par\end{center}

Notice that we have something interesting going on. 
\begin{align*}
2\cdot0=0\\
2\cdot1=2\\
2\cdot2=0\\
2\cdot3=2
\end{align*}

Now if we try to define division we don't have uniqueness, for example:
recall 
\[
2\div2=?
\]
is equivalent to 
\[
2\cdot?=2
\]
well both 1 and 3 satisfy the equation, $2\cdot?=2$. Why is this?
After some digging you would uncover that the root of the problem
is the fact that 4 is a composite of 2. These times tables are built
off what is the remainder when we divide by 4, since 4 is composed
of two twos, we run into problems when we would try division by 2.
How could we avoid this problem?\\

We could extend our playing field to a set of numbers whose size (cardinality)
is not composite, for example we could extend it to using five numbers.
The multiplication table for this new playing field is below. 
\begin{center}
\global\long\def\arraystretch{1.3}
 \setlength{\doublerulesep}{0pt} %
\begin{tabular}{r||2|2|2|2|2}
$\times$  & 0  & 1  & 2  & 3  & 4 \tabularnewline
\hline 
\hline 
0  & 0  & 0  & 0  & 0  & 0 \tabularnewline
\hline 
1  & 0  & 1  & 2  & 3  & 4 \tabularnewline
\hline 
2  & 0  & 2  & 4  & 1  & 3 \tabularnewline
\hline 
3  & 0  & 3  & 1  & 4  & 2 \tabularnewline
\hline 
4  & 0  & 4  & 3  & 2  & 1 \tabularnewline
\hline 
\end{tabular}
\par\end{center}

If you look down the columns you notice something that might remind
you of sudoku. Each column (except 0) contains a full set of the numbers
we have in our playing field. This makes our division unique because
multiplication is unique.

\paragraph*{Example}

Let's pick the two's column again, what is $2\div2$ rewriting this
becomes $2\cdot?=2$ which looking at our table is one. What about
$2\div1$? Rephrasing as multiplication $2\cdot?=1$ so two divided
by one is three. Or phrased another way three is the inverse of two.\\

We coudn't divide in our field when it had four elements, but when
we added an element and made the size of our playing field prime,
we could. This is the idea of a field extension. We start with some
(playing) field and we make the field larger so that it will satisfy
some additional properties.

\subsubsection*{Applying the idea of field extension to polynomials}

Let's suppose that we are in the real numbers with any polynomial
of the form 
\[
x^{3}+x^{2}+x+1
\]
and we want to find the splitting field. It looks like $x=-1$ might
be a root. So we complete long division\footnote{Why can we use long division here? Because we are in a field, if we
are in a field, then long division is defined. The field we are in
is the field of polynomials with integer coefficients.} to find that 
\[
x^{3}+x^{2}+x+1=(x+1)(x^{2}+1)
\]
You might look at the $x^{2}+1$ term and be tempted to say it has
\char`\"{}no solution in the reals'', which is technically correct.
But there is another way to frame it. We can go on what we know, which
is 
\[
x^{2}+1=0
\]
and from that it follows that 
\[
x^{2}=-1
\]
This would be a field extension. For example we can look at multiplication
in this field extension, commonly called $\mathbb{Z}[x]/(x^{2}+1)$
(which means the field of polynomials with integer coefficents modular
$x^{2}+1$). 
\begin{align*}
(a+bx)(c+dx) & =(a+bx)c+(a+bx)dx\\
 & =ac+bcx+adx+bdx^{2}\\
 & =(ac+bdx^{2})+(ad+bc)x\\
 & =(ac-bd)+(ad+bc)x
\end{align*}
Where the last step used the identity $x^{2}=-1$. In some ways working
from this place one might say we are building the complex numbers
from the real numbers. But the reason we are doing this, is so we
can split any polynomial up. We are extending the field as a quotient
ring generated by $x^{2}+1$. $x^{2}+1$ is also know as an irreducible
factor, and when it plays this role in the quotient ring, it is known
as the ideal.

\paragraph*{Example}

Let's calculate the splitting field for 
\[
x^{4}+x^{3}+x^{2}+1
\]
We'll we already know $x^{2}+1=0$ so 
\begin{align*}
x^{4}+x^{3}+x^{2}+1 & =x^{4}+x^{3}+0\\
 & =x^{3}(x+1)
\end{align*}
Thus we split our polynomial by extending our field. But we're not
quite done, recall $x^{2}=-1$ this implies $x^{3}=-x$. Therefore:
\[
x^{4}+x^{3}+x^{2}+1=-x(x+1)
\]
in our new field $\mathbb{Z}/(x^{2}+1)$. Furthermore we have split
our polynomial into linear factors, this is the splitting field, and
we can see that 0 and -1 are the roots in this new field.\\

{\color{red}\emph{Need Help! Prove that there exists a splitting
field and that it has $n$ terms!}}\\

We may write 
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
as 
\[
(x-a_{1})(x-a_{2})(x-a_{3})
\]
because we look at it in a field in which it is possible, just as
we did in the above examples. \\

{\color{red}\emph{How would I do it for this example?}} \\

%What we want to do first is factor $x^3 -r_1 x^2 +r_2 x - r_3$ as far as possible, suppose $x^3 -r_1 x^2 +r_2 x - r_3$ is as far as we can factor it in our field.   Then $$x^3 -r_1 x^2 +r_2 x - r_3 =0$$
%This then implies that $$x^3 -r_1 x^2 = r_3- r_2 x$$
%Thus $$x^3 -r_1 x^2 +r_2 x - r_3 = r_3- r_2 x +r_2 x - r_3 = 0$$ it works!  
%Let $$f(x) = x^n + b_{n-1}x^{n-1} + \cdots + b_1 x + b_0$$ be our ideal (like $x^2+1$), with $n \leq 3$  we know $a_1$ is a root, therefore:
%\begin{align*}
%a_1^n + b_{n-1}a_1^{n-1} + \cdots + b_1 a_1 + b_0 &=0 \\
%a_1^n  &= -(b_{n-1}a_1^{n-1} + \cdots + b_1 a_1 + b_0) \\
%a_1^na_1^{3-n} &= -(b_{n-1}a_1^{n-1} + \cdots + b_1 a_1 + b_0)a_1^{3-n} \\
%&= -(b_{n-1}a_1^{3-1} + \cdots + b_1 a_1^{3-n+1} + b_0 a_1^{3-n}) \\
%&= -(b_{n-1}a_1^{2} + \cdots + b_1 a_1^{4-n} + b_0 a_1^{3-n})
%\end{align*}
%In this case we would take $n=0,1,2$

%in this case we know the roots, so let's just do long division, which gives us:
%$$x^4+x^3+x^2+1 = (x-a_1)(x^3 + (a_1+1)x^2 + (a_1^2+a_1+1) + (a_1^3+a_1^2+a_1))$$

To recap, we found some field $\mathbb{F}$ that allows us to split
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
into 
\[
(x-a_{1})(x-a_{2})(x-a_{3})
\]
and $a_{1},a_{2},a_{3}$ are elements of this field $\mathbb{F}$,
we are not really sure what $\mathbb{F}$ is, we just know that it
exists.\\

{\color{red}\emph{How do we know we will have 3 roots in C?}}\\

We might be tempted to say we're done, we have three roots! But we
can't, because we are in this strange field $\mathbb{F}$, in other
words our car is taken apart in our friends garage. We first need
to make sense of these roots $a_{1},a_{2},a_{3}$.

Well let's look at $a_{1},a_{2},a_{3}$ more closely. We know that
$\mathbb{F}$ is a field so all the properties of addition and multiplication
we are used to remain the same. So we are welcome to distribute and
multiply, when we do this we get: 
\[
(x-a_{1})(x-a_{2})(x-a_{3})=x^{3}-(a_{1}+a_{2}+a_{3})x^{2}+(a_{1}a_{2}+a_{1}a_{3}+a_{2}a_{3})x-a_{1}a_{2}a_{3}
\]

Something really cool is going on! Figure it out before you move on.

\subsection*{Symmetric Polynomials}

Let's investigate 
\[
x^{3}-(a_{1}+a_{2}+a_{3})x^{2}+(a_{1}a_{2}+a_{1}a_{3}+a_{2}a_{3})x-a_{1}a_{2}a_{3}
\]
more throughly. Let's assign the following: 
\begin{align*}
e_{1}=a_{1}+a_{2}+a_{3}\\
e_{2}=a_{1}a_{2}+a_{1}a_{3}+a_{2}a_{3}\\
e_{3}=a_{1}a_{2}a_{3}
\end{align*}

The $e_{1},e_{2},e_{3}$ we just designated are called elementary
symmetric polynomials. Viéte found formulas that draw a connection
between a polynomial's coefficents and its roots. We are going to
leverage those formulas now. It has been shown that Viéte's formulas
may be applied with polynomials with coefficients in any field.{\color{red}\emph{This
is correct right? if Vietes works in most integral domians then it
surely will work in a field}}

As an example of how Viéte's formulas relate roots and coefficents,
let's look at how we could write 
\[
a_{1}^{2}+a_{2}^{2}+a_{3}^{2}
\]
as our elementary symmetric polynomials. We need the sum of squares
so we will need 
\[
e_{1}^{2}=a_{1}^{2}+a_{2}^{2}+a_{3}^{2}+2a_{1}a_{2}+2a_{1}a_{3}+2a_{2}a_{3}
\]
Now we need to remove those cross terms well $e_{2}$ should work
nicely for that 
\[
e_{1}^{2}-2e_{2}=a_{1}^{2}+a_{2}^{2}+a_{3}^{2}+2a_{1}a_{2}+2a_{1}a_{3}+2a_{2}a_{3}-2(a_{1}a_{2}+a_{1}a_{3}+a_{2}a_{3})
\]
after zeroing out terms we arrive at 
\[
e_{1}^{2}-2e_{2}=a_{1}^{2}+a_{2}^{2}+a_{3}^{2}
\]

The elementary symmetric polynomials make the rest of our \char`\"{}proof''
possible. As you noticed the $e_{1},e_{2},e_{3}$ are the same as
$r_{1},r_{2},r_{3}$ respectively. This means we can relate the roots
in our strange field $\mathbb{F}$ with the coefficents of our original
polynomial. We can put our car back together. Since the $r_{1},r_{2},r_{3}$
are real valued, then $e_{1},e_{2},e_{3}$ must be real valued. But
we still can't make the leap to $a_{1},a_{2},a_{3}$ being real valued,
for example suppose 
\begin{align*}
a_{1}=1-i\\
a_{2}=1+i\\
a_{3}=1
\end{align*}
Then our elementary symmetric polynomials would become 
\begin{align*}
e_{1}=3\\
e_{2}=4\\
e_{3}=2
\end{align*}

The elementary symmetric polynomials are real, however our roots are
not necessarily real valued. Thus we cannot yet say we have three
roots.

In order to sort this root problem out, we \char`\"{}need'' to use
induction, but before we do that we should to develop our intuition
on a new equation. This new equation will have the following form:
\[
G_{s}(x)=(x-a_{1}-a_{2}-sa_{1}a_{2})(x-a_{1}-a_{3}-sa_{1}a_{3})(x-a_{2}-a_{3}-sa_{2}a_{3})
\]
where the $a_{1},a_{2},a_{3}$ are the roots of our original polynomial
in some field $\mathbb{F}$ and $s$ is a real number. When we expand
$G_{s}(x)$ we get: %$$ G_s(x) = x^2 - x(a_1 + 2a_2 + a_3 + s (a_1 a_2 + a_2 a_3))  + a_1 a_2 + a_1 a_3 + a_2 a_3 + a_2^2 + s(a_1 a_2^2 + 2a_1 a_2 a_3 + a_2^2 a_3) + s^2 a_1 a_2^2 a_3$$

\begin{flalign*} Gs(x) \&= x3 \\
 \&\hphantom{{}=x} {\color{green} - x2 ( 2 a1 + 2 a2 + 2 a3 + a1
a2 s + a1 a3 s + a2 a3 s )} \&\\
 \&\hphantom{{}=x+x} {\color{blue} + x( a1 a2 a32 s2 + a1 a22 a3
s2 + a12 a2 a3 s2 + a1 a22 s + a1 a32 s + a2 a32 s + a12 a2 s + a12
a3 s + a22 a3 s} \\
 \&\hphantom{{}=x+x+x+x} {\color{blue} + 6 a1 a2 a3 s + a12 + a22
+ a32 + 3 a1 a2 + 3 a1 a3 + 3 a2 a3 )} \&\\
 \&\hphantom{{}=x+x+x} {\color{red} -a12 a22 a32 s3 - 2 a1 a22
a32 s2 - 2 a12 a2 a32 s2 - 2 a12 a22 a3 s2 - a12 a22 s - a12 a32 s
- a22 a32 s} \\
 \&\hphantom{{}=x+x+x+x} {\color{red} - 3 a1 a2 a32 s - 3 a1 a22
a3 s - 3 a12 a2 a3 s - a1 a22 - a1 a32 - a2 a32 - a12 a2 - a12 a3
- a22 a3 - 2 a1 a2 a3 } \end{flalign*}

Notice how we could switch all the $a_{1}$ with $a_{2}$ (likewise
with $a_{1}$ and $a_{3}$ or $a_{2}$ and $a_{3}$) and we would
end up with an equivalent equation. We call polynomials of this form
symmetric and $G_{s}(x)$ is symetric in the coefficients $a_{1},a_{2},a_{3}$.
Since $G_{s}(x)$ is symmetric there it can be shown that we can then
write $G_{s}(x)$ in terms of its elementary symmetric polynomials.\footnote{This is known as the Fundamental Theorem of Symmetric Polynomials,
the proof of which is a bit beyond this paper.} Recall that our elementary symmetric polynomials were 
\begin{align*}
e_{1}=a_{1}+a_{2}+a_{3}\\
e_{2}=a_{1}a_{2}+a_{1}a_{3}+a_{2}a_{3}\\
e_{3}=a_{1}a_{2}a_{3}
\end{align*}

Then using these we may write \begin{flalign*} Gs(x) \&= x3 \\
 \&\hphantom{{}=x} {\color{green}- x2 ( 2 e1 + e2 s )} \&\\
 \&\hphantom{{}=x+x} {\color{blue}+ x( e1 e3 s2 + e1 e2 s + e12
- 2e2 + 3 e2 ) }\&\\
 \&\hphantom{{}=x+x+x} {\color{red}-e32 s3 - 2 e2 e3 s2 - (e22
- 2e1e3) s - 3 e1 e3 s - e1 e2 + e3} \end{flalign*}

The coefficients of the elementary symmetric polynomials are real
valued. Furthermore the elementary symmetric polynomials are real
valued {\color{red}\emph{WHY?.. attempt}}. Recall that the $e_{1},e_{2},e_{3}$
relate to the original polynomial, 
\[
x^{3}-r_{1}x^{2}+r_{2}x-r_{3}
\]
Not only did $e_{1},e_{2},e_{3}$ relate to the roots, but they related
to the coefficients, i.e., 
\begin{align*}
e_{1}=r_{1}\\
e_{2}=r_{2}\\
e_{3}=r_{3}
\end{align*}

Remember that $r_{1},r_{2},r_{3}$ are real valued, $\mathbb{R}$.
So since we may write $G_{s}(x)$ in terms of the elementary symmetric
polynomials, which are equivalent to the coefficents in our original
equation. The coefficients of $G_{s}(x)$ are real valued.

\subsection*{Induction}

Now we will abuse induction in preparation for the coming proof. We
will need to pretend that the orignial polynomial $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$
has degree $n=2^{k}m$ where $k>0$ and is the largest integer such
that $n=2^{k}m$ is true and $m$ is odd. We will show that for any
value $k$ there is at least one complex root. We will do this by
inducting on $k$.

\subsubsection*{Base Case}

For the base case $k=0$, the polynomial is odd, the end behavior
of odd polynomials with positive leading coefficent is $x\rightarrow-\infty$
the value of the polynomial $\rightarrow-\infty$ and as $x\rightarrow\infty$
the value of the polynomial $\rightarrow\infty$. Additionally recall
that a polynomial is continuous, there are no jumps, therefore the
polynomial must cross the $x-$axis at some place in between $(-\infty,\infty)$
and we therefore have at least one complex root. This argument also
takes care of any polynomials with odd degree. We will elaborate on
this argument in the generalized proof.

\subsubsection*{Induction Step}

Now for the induction step, we use the induction hypothesis that every
polynomial with real coefficients and degree $n=2^{k-1}m'$ (where
$m'$ is odd) has at least one complex root.\\

Our original polynomial has degree $n=2^{k}m$, but the degree of
$G_{s}(x)$ is $2^{k-1}m(n-1)$ (this is proven in the generalized
case). For our specific case $n=3$, which implies that $k=0$ and
$m=3$ thus the degree of $G_{s}(x)$ is $2^{-1}3(2)=3$.\footnote{Technically the following does not hold, because $m'$ is not odd}
Since $G_{s}(x)$ is of the form $2^{k-1}m'$, we can apply the induction
hypothesis. Thus $G_{s}(x)$ has at least one complex root.\\

We do not know if this complex root is 
\[
a_{1}+a_{2}+sa_{1}a_{2}
\]
or 
\[
a_{1}+a_{3}+sa_{1}a_{3}
\]
or 
\[
a_{2}+a_{3}+sa_{2}a_{3}
\]

For now let's pretend that $a_{1}+a_{2}+sa_{1}a_{2}$ is complex (the
rigor is left for the generalized proof). Recall that $s$ is real
valued, $s\in\mathbb{R}$. If $s=0$ then $a_{1}+a_{2}$ must be complex.
What if $s\neq0$? If the sum $a_{1}+a_{2}$ is real for a complex
number, then the imaginary parts must zero out, then in order for
$a_{1}+a_{2}+sa_{1}a_{2}$ to be complex $a_{1}a_{2}$ must be complex.
Remember that $s$ is our parameter and it is real valued, so the
fact that $a_{1}+a_{2}+sa_{1}a_{2}$ is complex does not depend on
$s$. Thus the sum $a_{1}+a_{2}$ is complex and the product $a_{1}a_{2}$
is complex. Now we need to show that $a_{1}$ is complex and $a_{2}$
is complex.\\

Consider the equation 
\[
x^{2}-(a_{1}+a_{2})x+a_{1}a_{2}
\]
the roots of this equation are $a_{1}$ and $a_{2}$, since the degree
of this equation is of the form $2^{k-1}m'$ by the induction hypothesis
$x^{2}-(a_{1}+a_{2})x+a_{1}a_{2}$ has at least one complex root.
Thus $a_{1}$ or $a_{2}$ is complex. So we have at least one complex
root. What about the other three?

\subsection*{Our three roots}

Let's pretend that $a_{1}$ is indeed our complex root. Then we can
factor $a_{1}$ out of our original equation $x^{3}-r_{1}x^{2}+r_{2}x-r_{3}$.
When we do this we arrive at 
\[
(x-a_{1})(x^{2}-\rho_{1}x-\rho_{2})
\]
Now we can apply the same process we went through and find that $x^{2}-\rho_{1}x-\rho_{2}$
has a \char`\"{}complex''\footnote{Complex is in quotes, because the imaginary part might be zero}
root, suppose this time it is $a_{3}$, we can factor out this term
and when we do this, we get: 
\[
(x-a_{1})(x-a_{3})(x-\gamma)
\]
Again we apply our process to $x-\gamma$ and we find that $\gamma=a_{2}$
is complex and we have our full set of roots.

% now work through the fact that g is symmetric then elementary symmetric, then use the fact that there are two roots in the complex blah blah

% does this proof only work for polynomials greater than a certain degree, find out.

\section*{The Fundamental Theorem of Algebra}

The Fundamental Theorem of Algebra as we typically teach and learn
it states one of the following three: 
\begin{enumerate}
\item That every single variable polynomial of degree $n$ has exactly $n$
roots (counting multiplicity). 
\item Any polynomial with complex coefficients greater than degree 0 has
at least one complex root. 
\item The field of complex numbers is closed under algebra. 
\end{enumerate}
All of these statements are equivalent, recall a field\footnote{A field is an algebraic structure with a form of addition, subtraction,
multiplication and division, satisfying the commutative and distributive
properties} $K$ is called \emph{algebraically closed} if every non-constant
polynomial $f(c)\in K[x]$ has a root in $K$.

So a field, $K$ is algebraically closed if the roots of every non-constant
polynomial (e.g. $x^{2}+1$) are in $K$.

%\paragraph*{Comprehension check:} With only rereading the definition and explanation of algebraic closure, think of an example of why the field of integers is not closed under algebra.\footnote{$x^2+1$ would work because the roots of $x^2+1$ are $x=\pm i$}

From the definition of algebraically closed it follows that \textbf{statement
2} and \textbf{statement 3} are identical, \textbf{statement 3} is
just shorthand for stating \textbf{statement 2}. Remember that a complex
number with no imaginary part is a real number.

\textbf{Statement 1} is how the Fundamental Theorem of Algebra is
typically stated in the secondary setting and it really is a corollary
of \textbf{statement 2}. As we saw in the previous section we can
factor out the complex number we find from our polynomial. This leaves
us with a linear term multiplying a polynomial one degree less than
our original. We can then find the complex root of this new polynomial,
and factor it out. We can continue this process, but we can only do
it $n$ times, thus we get $n$ roots.

\section*{Algebraic proof of The Fundamental Theorem of Algebra}

I will attempt to prove that every non-constant polynomial with complex
coefficients has a complex root in a manner that does not require
rigorous mathematical study.\footnote{Many thanks to \url{https://www.artofproblemsolving.com/wiki/index.php?title=Fundamental_Theorem_of_Algebra##Algebraic_Proof}}\\

%It is necessary to assume the following
%\begin{enumerate}
%\item Every odd degree polynomial with real coefficients has at least one real root.\footnote{One may prove this with the Intermediate Value Theorem} \emph{Include example picture, proof using IVT in appendix}
%\item Every polynomial of degree two with complex coefficients has a complex root.\footnote{Proven by the fact that every polynomial with real coefficients has a complex root} \emph{Include example picture}
%\end{enumerate}
%\paragraph*{Lemma} 

\subsection*{Set up}

Think of a polynomial with complex coefficients (e.g. $x^{2}+4x+2$,
or $(1+2i)x+3$), now think of another. Now think of every polynomial
with complex coefficients and let's let $C(x)$ represent one of them,
we don't know which, but it is one of them.\\


\subsubsection*{Complex number review}

Recall that to take the conjugate of a complex number you just switch
the addition or subtraction on the imaginary part (e.g. if $z=1+2i$
the the conjugate of $z$ is $\bar{z}=1-2i$). Something neat happens
when you multiply a complex number with its conjugate. Let's let $z=a+bi$
be any complex number, then the conjugate is $\bar{z}=a-bi$ and if
we multiply them together we get 
\[
z\bar{z}=a^{2}+abi-abi-(bi)^{2}
\]
which becomes 
\[
z\bar{z}=a^{2}+b^{2}
\]
which is a real number, i.e., the imaginary part is 0 (or there is
no imaginary part). Furthermore it is a positive real number!\\


\subsubsection*{Creating a polynomial with real coefficients}

So back to our polynomial with complex coefficients, $C(x)$ if we
take $R(x)=C(x)\bar{C(x)}$ (which is $C(x)$ multiplied by its conjugate)
then $R(x)$ will be a polynomial with real coefficients, and the
roots of $C(x)$ will also be the roots of $R(x)$. To see this think
about how we find roots, one way is to factor, so if we factor $C(x)$
into a bunch of linear terms, say $c_{1}(x)c_{2}(x)\cdots c_{n}(x)$
then we could factor $R(x)$ into 
\[
c_{1}(x)c_{2}(x)\cdots c_{n}(x)\overline{c_{1}(x)c_{2}(x)\cdots c_{n}(x)}=c_{1}(x)c_{2}(x)\cdots c_{n}(x)\bar{c_{1}(x)}\bar{c_{2}(x)}\cdots\bar{c_{n}(x)}
\]
. Since, the roots of $C(x)$ are also the roots of $R(x)$, showing
that every polynomial with real coefficients has a complex root will
complete the proof!

\subsubsection*{Set up summary}

To summarize the above, since the goal is to show that any polynomial
with complex coefficients has a complex root then I can build a polynomial
with real coefficients by multiplying my polynomial with complex coefficients
with its conjugate. Then if I show that this new polynomial with real
coefficients has a complex root, I'm done.

\subsection*{Proof by Induction}

Suppose the degree of $R(x)$ is $d=2^{n}q$, where $q$ is odd and
$n$ is a natural number, $\mathbb{N}$. %$R(x)$ will always have an even degree, because the degree of $d(R(x))=d(C(x))*2$.  
Then lets induct on $n$, in other words, we show that the first case
is true, then we show that each case follows from the previous, therefore
each case must be true.

\pagebreak{}

\subsubsection*{Base Case}

\begin{wrapfigure}{r}{8cm}%
 \definecolor{qqwuqq}{rgb}{0.,0.39215686274509803,0.} \begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=0.8cm,y=0.8cm]
\draw[->,color=black] (-4.604132231404959,0.) -- (4.784297520661155,0.);
\foreach \x in {-4.,-3.,-2.,-1.,1.,2.,3.,4.}
\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt);
\draw[->,color=black] (0.,-2.9905785123966946) -- (0.,4.249090909090907);
\foreach \y in {-2.,-1.,1.,2.,3.,4.}
\draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt);
\clip(-4.604132231404959,-2.9905785123966946) rectangle (4.784297520661155,4.249090909090907);
\draw[line width=1.2pt,color=qqwuqq,smooth,samples=100,domain=-4.604132231404959:4.784297520661155] plot(\x,{((\x)+2.0)*((\x)-1.0)*(\x)});
\begin{scriptsize}
\draw[color=qqwuqq] (-2.157851239669423,-2.7757024793388436) node {};
\end{scriptsize}
\end{tikzpicture} \end{wrapfigure}%

If we look at the base case $n=0$, then $R(x)$ is degree $d=q$,
where $q$ is odd. Recall that the leading coefficient of a polynomial
will determine the overall slope of the equation. The leading coefficient
for $R(x)$ must be positive, because the leading coefficient is the
coefficient of the highest degreed term in $C(x)$ multiplied with
its conjugate, the product of which will be positive. So the polynomial
$R(x)$ will \char`\"{}start'' somewhere in quadrant three (i.e.,
$\Lim{x\rightarrow-\infty}R(x)\rightarrow-\infty$), so when $x$
is negative and large enough, $R(x)<0$. Then as we traverse the function
$R(x)$ it will rise up and \char`\"{}end'' in quadrant one (i.e.,
$\Lim{x\rightarrow\infty}R(x)\rightarrow\infty$), so when $x$ is
positive and large enough, $R(x)>0$.\\

By combining this analysis with the fact that this function is continuous
we \emph{know} this function will intersect the x-axis. The mathematical
justification for \emph{knowing} is by implicating the intermediate
value theorem, which states that a continuous function, $f$, with
an interval $[a,b]$ as its domain, takes values $f(a)$ and $f(b)$
at each end of the interval, then it also takes any value between
$f(a)$ and $f(b)$ at some point within the interval. For the sake
of completeness the corollary that applies beautifully here is \textbf{Bolzano's
theorem}. Bolzano's theorem states that if a continuous function has
values of opposite sign inside an interval (which we have, i.e., at
some point $x<0,R(x)<0$ and also $x>0,R(x)>0$), then it has a root
in that interval. For proofs of the intermediate value theorem and
continuity of polynomials see the appendix.

\subsubsection*{Induction Step}

Now for the fun part. Suppose again that $d=2^{n}q$, where $q$ is
odd and $n>0$, however lets \char`\"{}believe\char`\"{} that the
theorem has been proven when the degree is $2^{n-1}q'$ where $q'$
is odd. Therefore, we can use the \char`\"{}fact\char`\"{} (induction
hypothesis) that any polynomial less than or equal to degree $2^{n-1}q'$
has a complex root.\\

Let's start by splitting $R(x)$ into all its linear terms like we
did previously, we will also put a factor $a$ out front. This $a$
would correspond to the coefficient of $x^{n}$. This step requires
that we \char`\"{}find\char`\"{} a field, $\mathbb{F}$ over which
we can split the polynomial into its linear factors.\footnote{For completeness, we are finding the smallest field extension of $\mathbb{C}$,
such that $R(x)$ decomposes into linear factors.} This just means, we might not have the tools to take this polynomial
apart where we currently are. So we go to our friends house that has
the tools (find the field) and take it a part there. In math jargon
this would look like: \\
 
\[
c_{1}(x)c_{2}(x)\cdots c_{n}(x)\bar{c_{1}(x)}\bar{c_{2}(x)}\cdots\bar{c_{d}(x)}
\]
where the $c_{i}(x)$ are linear functions in the field, $\mathbb{F}$.
Now let $x_{1}$ be the root of $c_{1}(x)$ in the field, $\mathbb{F}$,
so on and so forth. So the roots of $R(x)$ in the field $\mathbb{F}$
are $x_{1},\ldots,x_{d}$. If this field, $\mathbb{F}$ were the field
of polynomials with real valued coeffcients, $\mathbb{R}[x]$ we would
be done, but its not. So now we need to figure out how to relate these
roots we found back to the real or complex field. In order to do that
we use the same trickery we used previously.\\

Let $s$ be an arbitrary real number and let $y_{s,i,j}=x_{i}+x_{j}+sx_{i}x_{j}$,
where $1\leq i<j\leq d$. Now define: 
\[
G_{s}(x)=(x-y_{s,1,2})(x-y_{s,1,3})\cdots(x-y_{s,1,d})(x-y_{s,2,3})\cdots(x-y_{s,2,d})\cdots(x-y_{s,d-1,d})
\]
This may be written succinctly as: 
\[
G_{s}(x)=\Pi_{1\leq i<j\leq d}(x-y_{s,i,j})
\]

The $G_{s}(x)$ we saw previously was constructed in the exact same
manner. It is important to know that the coefficients of $G_{s}$
are symmetric in $x_{1},x_{2},\ldots,x_{d}$. This means that the
variables could be interchanged in any way and we would not need to
change the coefficients, in fact, we would have the same polynomial!\footnote{For further investigation read about symmetric polynomials.}
In other words the coefficient of $x_{1}$ would be the same as the
coefficient as $x_{d}$, likewise the coefficient of $xx_{1}$ would
be the same as $xx_{d}$, so on and so forth. It might help to think
of Pascal's triangle. Let's look at a case when $d=2$.

\subsubsection*{Example of $G_{s}(x)$ when $d=2$ }

This example is very similar to the previous example section, but
is included to help concretize the proof. In this example we will
derive $G_{s}(x)$ from the original polynomial 
\[
f(x)=x^{2}+bx+c
\]
where $b,c$ are real valued.

Following the layout of the proof first we find the splitting field
over which $f(x)$ splits. From which we get 
\[
f(x)=(x-x_{1})(x-x_{2})
\]
At this point we don't know what type of numbers $x_{1}$ or $x_{2}$
are. They could be complex, they could be something else altogether.
Let's now look at what $G_{s}(x)$ looks like when $d=2$. Since 
\[
G_{s}(x)=(x-y_{s,1,2})
\]
let's first create $y_{s,1,2}$: 
\[
y_{s,1,2}=x_{1}+x_{2}+sx_{1}x_{2}
\]
so 
\[
G_{s}(x)=x-x_{1}-x_{2}-sx_{1}x_{2}
\]

There are many important features to notice about $G_{s}(x)$. $G_{s}(x)$
is a polynomial in $x$, its degree is one, so it is one degree lower
than our original polynomial $f(x)$, this feature will come in handy
when we use our induction hypothesis. However in order to use our
induction hypothesis we need to know that the coefficients of $G_{s}(x)$
are real valued. The trick we use here is rather clever.

Recall that $f(x)=(x-x_{1})(x-x_{2})$, which if we distribute twice
we get 
\[
f(x)=x^{2}+x(-x_{1}-x_{2})+x_{1}x_{2}
\]
further 
\[
f(x)=x^{2}+bx+x
\]
Therefore

\begin{align*}
b=-x_{1}-x_{2}\\
c=x_{1}x_{2}
\end{align*}

Well we know that $b,c\in\mathbb{R}$ are real valued. Thus $-x_{1}-x_{2},x_{1}x_{2}$
must be real valued. This does not necessarily mean that $x_{1}\in\mathbb{R}$
or that $x_{2}\in\mathbb{R}$ are real valued. For example if $x_{1}=1+i$
and $x_{2}=1-i$ then $-x_{1}-x_{2}=-2$ and $x_{1}x_{2}=2$. But
it does mean that the coefficents of 
\[
G_{s}(x)=x-(x_{1}+x_{2})+x_{1}x_{2}
\]
namely $x_{1}+x_{2}$ and $x_{1}x_{2}$ are real valued. Here you
saw a concrete version but the fact that $x_{1}+x_{2}$ and $x_{1}x_{2}$
are real valued stems from Viéte's formulas, which relate the coefficents
of a polynomial to functions of the polynomial. These functions are
what we have been calling the elementary symmetric polynomials. Now
let's return to the proof.

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% This is incorrect!
%To help with this idea of symmetric coefficients, when $d=3$ we have
%$$
%G_s(x) = (x-x_1-x_2-sx_1x_2)(x-x_2-x_3-sx_2x_3)
%$$
%which expands to
%$$
%x^2+x_2x_3+x_1x_3 + x_1x_2 + x_2^2 -xx_3 - 2x x_2-x x_1 + 2sx_1x_2x_3 + sx_2^2 x_3 + s x_1 x_2^2 - s x x_2 x_3 - s x x_1 x_2 + s^2 x_1 x_2^2 x_3
%$$
%Notice how we could interchange $x_1$ and $x_2$ and the coefficients would remain the same.  In this example the coefficients are all real, but we can't assume the same will occur when $d>3$.\\
%The same analysis that applied in the example when $d=2$ and $d=3$ applies when $d>3$, we reduce the degree of the polynomial using $G_s(x)$, check that the coefficients are real then use the induction hypothesis.\\
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% explanation of induction
%In order to do that we need to use induction\footnote{This step is not completely necessary for this example, however understanding this process will help immensely when we turn towards the proof for any polynomial}.    Remember induction is like climbing a ladder, we show that the first rung is true, then we show that if a previous rung existed then the next one does as well.  Suppose the degree of our polynomial was one this will act as our base case.  A polynomial of degree one is a non-horizontal line, which will cross the $x-$axis once, therefore we will have one root for a polynomial of degree one.  Next we will use the induction hypothesis that a polynomial of degree two has two complex/real roots to show that our third degree polynomial has three complex/real roots.
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\subsubsection*{Coefficents of $G_{s}(x)$ are real}

Now back to: 
\[
G_{s}(x)=(x-y_{s,1,2})(x-y_{s,1,3})\cdots(x-y_{s,1,d})(x-y_{s,2,3})\cdots(x-y_{s,2,d})\cdots(x-y_{s,d-1,d})
\]
we need to the coefficients of $G_{s}(x)$ to be real and we need
the degree to be $2^{n-1}q'$, where $q'$ is odd. %~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% Delete ?
%We have some motivation to believe that the coefficients of $G_s(x)$ are real.  We will have to take on belief that they are indeed real.\footnote{For the curious, to the best of my knowledge it has not be proven that we may do this.  What the proofs so far claim is that the coefficients of $G_s(x)$ may be viewed as elementary symmetric polynomials.  These polynomials have special formulas which Fran\c{c}ois Vi\'ete discovered in the late 1500's, which enable one to rewrite them as real numbers.  However these formulas only apply to the reals or complex numbers, not necessarily some field extension of the complex field, which is where we are using them.}  
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In order to use our induction step we need the coefficients of $G_{s}(x)$
to be real. Recall that the induction step states: 
\begin{center}
\fbox{\begin{minipage}[c]{40em}%
 Polynomials of degree $2^{n-1}q'$ with real valued coefficients
have at least one complex root. %
\end{minipage}}
\par\end{center}

%In the two examples we saw a concrete version of $G_s(x)$ in these example the terms when multiplied out were composed of symmetric polynomials in the roots, the $x_i$'s.  

Symmetry is going to play an important role, in enabling us to be
able to apply the induction hypothesis. Recall that symmetry in this
situation is when we are able to alternate the coefficents and maintain
equality. 

\paragraph*{Symmetric example}

\[
x+y
\]
is symmetric, if I replace $x$ with $y$ and $y$ with $x$ the expression
$y+x$ is equivalent. Likewise 
\[
x+y+z
\]
is also symmetric, we may replace $x$ with $z$ and $z$ with $x$
(or any other combination of variables) and maintain equivalancey.
However 
\[
x^{2}+y
\]
is not symmetric alternating the variables results in 
\[
y^{2}+x\neq x^{2}+y
\]
The left hand side is not equivalent to the right hand side. Let's
take a quick jaunt back to our example and see how this and the induction
step apply there.

\paragraph*{Return to the example for $d=2$}

Before we move on let's make sense of this step with a concrete example.
We want to use the induction hypothesis which loosely stated for this
example is: \\

\emph{Any polynomial with real valued coefficents and degree \char`\"{}less
than'' the original polynomial has at least one complex root.}\footnote{Less than is put in quotations, because stictly speaking this is not
true, we are inducting on the even part of the degree, the $2^{n}$
part. As long as the new polynomial has an $n'<n$ we can utilize
our induction hypothesis.}\\

Let's check everything our induction hypothesis requires.
\begin{enumerate}
\item Real valued coefficients: Because the $G_{s}(x)$ is symmetric the
coefficents may be related to the coefficents of the original polynomial,
which are real. 
\item Lesser degree: $G_{s}(x)$ is constructed such that it will be \char`\"{}less
than'' the degree of the orginal polynomial.\footnote{see previous footnote} 
\end{enumerate}
Since the criterion for the induction hypothesis is satisfied we know
that $G_{s}(x)$ has at least one complex root, so 
\[
-(x_{1}+x_{2})+sx_{1}x_{2}
\]
must be complex. In order to figure out whether $x_{1}$ or $x_{2}$
is complex we need to analyze $-(x_{1}+x_{2})+sx_{1}x_{2}$. In identifying
that $-(x_{1}+x_{2})+sx_{1}x_{2}$ is complex we did not consider
$s$ at all. Thus, whether or not $x_{1}$ or $x_{2}$ is complex
does not depend on $s$. So if $s=0$ then 
\[
-(x_{1}+x_{2})
\]
must be complex. So $x_{1}+x_{2}$ must be complex. Now let's assume
$x_{1}=u+iv$ and $x_{2}=z+iw$, then if $x_{1}+x_{2}$ is real valued
we must have $iv=iw$ in other words the imaginary parts must cancel
out. Then: 
\begin{align*}
x_{1}x_{2} & =(u+iv)(z+iw)\\
 & =(u+iv)(z-iv)\\
 & =uz-ivu+izu-i^{2}v^{2}\\
 & =uz+v^{2}+i(vz-uv)
\end{align*}
which is complex unless $vz=uv\Rightarrow z=u$ for $v\neq0$.

In order to identify if $x_{1}$ or $x_{2}$ is our complex root we
would need to use the induction hypothesis again, this will be detailed
further in the paper.

\paragraph*{Back to the proof}

We have a very similar situation for the generalized proof. $G_{s}(x)$
is constructed so that the coefficients are symmetric. This stems
from the fact that 
\[
y_{s,i,j}=x_{i}+x_{j}+sx_{i}x_{j}
\]
is symmetric in the $x_{i},x_{j}$. If we then multiply all the combinations
of $x-y_{s,i,j}$, where $i<j$ then we will have polynomial that
is symmetric in all the $x_{i}$'s.

Since the coefficients are symmetric then we can express the additive
and multiplciative combinations of the $x_{1},x_{2},\ldots x_{d}$
as elementary symmetric polynomials\footnote{By the Fundamental Theorem of Symmetric Polynomials},
just as we did in the examples. The elementary symmetric polynomials
relate to the original polynomial and are equivalent to the coefficients
of $R(x)$. The coefficients of $R(x)$ are real, therefore the elementary
symmetic polynomials are real. Likewise the additive and multiplciative
combinations of the $x_{1},x_{2},\ldots x_{d}$ are also real.\\

Put another way we related the coefficients of $G_{s}(x)$ to the
coefficents of our original polynomial, $R(x)$. Since the coefficents
of $R(x)$ are real, the coefficients of $G_{s}(x)$ must be real.
Note that the coefficents of $G_{s}(x)$ are additive and multiplicative
combinations of $x_{1},x_{2},\ldots x_{d}$. These additive and multiplicative
combinations are what are known as the elementary symmetric polynomials.\\

So we have show that the coefficents of $G_{s}(x)$ are real. In order
to use the induction hypothesis we need to figure out the degree of
$G_{s}(x)$.

\subsubsection*{Degree of $G_{s}(x)$}

Let's try counting how many terms there are in $G_{s}(x)$.

There are $d-1$ terms that have $i=1$, then there are $d-2$ terms
that have $i=2$, because $i<j$. So there are $d-3$ terms that have
$i=3$ and so on. 
\[
G_{s}(x)=\underbrace{(x-y_{s,1,2})(x-y_{s,1,3})\cdots(x-y_{s,1,d})}_{d-1}\underbrace{(x-y_{s,2,3})\cdots(x-y_{s,2,d})}_{d-2}\cdots(x-y_{s,d-1,d})
\]

Thus we have a decreasing sequence, since we are trying to find the
degree of $G_{s}(x)$ we take the $d-1$ terms and add them to $d-2$
terms, so on and so forth. This gives us a familiar series: 
\[
1+2+3+4+\cdots+(d-2)+(d-1)
\]

The trick for finding the sum of this sequence is often attributed
to Gauss. First define: 
\[
S_{n}=1+2+3+4+\cdots+(d-2)+(d-1)
\]

then rewrite $S_{n}$ as 
\[
S_{n}=(d-1)+(d-2)+\cdots+4+3+2+1
\]

add $S_{n}$ to itself, but we need to be clever here, take notice
of how the additive terms were grouped 
\[
2S_{n}=(1+(d-1))+(2+(d-2))+(3+(d-3))+\cdots((d-2)+2)+((d-1)+1)
\]

Perform the arithmetic inside each set of parenthesis 
\[
2S_{n}=d+d+d+\cdots+d+d
\]

Recall that there were $d-1$ terms in $S_{n}$ and we did nothing
to alter that since we paired up each component so: 
\[
2S_{n}=\underbrace{d+d+d+\cdots+d+d}_{d-1}
\]

A model to make multiplication understandable is that multiplication
is repeated addition, let's apply that model here: 
\[
2S_{n}=d(d-1)
\]

Which leaves us with our result: 
\[
S_{n}=\frac{d(d-1)}{2}
\]

So the sum of this series is $\frac{d(d-1)}{2}$. Recall that 
\[
d=2^{n}q
\]
so 
\[
\frac{d(d-1)}{2}=\frac{2^{n}q(d-1)}{2}=2^{n-1}q(d-1)
\]
but $q(d-1)$ is an odd number so 
\[
2^{n-1}q(d-1)=2^{n-1}q'
\]

Thus $G_{s}(x)$ has real coefficents and a degree less than or equal
to $2^{n-1}q'$, therefore we can apply the induction hypothesis.

\subsection*{Our Complex root}

Since the degree of $G_{s}(x)$ is $2^{n-1}q'$ and it has real coefficents
the induction hypothesis applies:
\begin{center}
\fbox{\begin{minipage}[c]{40em}%
 Polynomials of degree $2^{n-1}q'$ with real valued coefficients
have at least one complex root. %
\end{minipage}} 
\par\end{center}

Now we can assume $G_{s}(x)$ has at least one complex root in order
to show that $R(x)$ has at least one complex root. Without a loss
of generality let us say that the root occurs when $i=r$, $j=t$,
therefore 
\[
y_{s,r,t}=x_{r}+x_{t}+sx_{r}x_{t}
\]
is complex for some $s$. So suppose 
\[
s=0
\]
then 
\[
x_{r}+x_{t}
\]
must be complex. Now Suppose 
\[
x_{r}+x_{t}
\]
is real, that would mean that the imaginary parts cancel out, but
when we take 
\[
x_{r}x_{t}
\]
the imaginary parts would not cancel out, therefore $x_{r}x_{t}$
is also complex for some $s$. Since both $x_{r}+x_{t}$ and $x_{r}x_{t}$
are complex numbers. To show that $x_{r}$ or $x_{t}$ is complex
we consider the equation 
\[
x^{2}-(x_{r}+x_{t})x+x_{r}x_{t}
\]
Notice that the coefficents of this equation are complex. So let's
define them as such 
\begin{align*}
x_{r}+x_{t}=u+iv\\
x_{r}x_{t}=w+iz
\end{align*}
where $u,v,z,w$ are real valued. Substituting these values into the
equation gives us: 
\[
x^{2}-(u+iv)x+w+iz
\]
Let's apply the quadratic formula to this equation and see what we
get. 
\begin{align*}
x & =\frac{u+iv\pm\sqrt{(u+iv)^{2}-4(w+iz)}}{2}\\
 & =\frac{u+iv\pm\sqrt{u^{2}+2iuv-v^{2}-4w-4iz}}{2}\\
 & =\frac{u+iv\pm\sqrt{\underbrace{u^{2}-v^{2}-4w}_{p}+i(\underbrace{2uv-4z}_{q})}}{2}\\
 & =\frac{u+iv\pm\sqrt{p+iq}}{2}\\
 & =\frac{u+iv\pm(p'+iq')}{2}\\
\end{align*}

Thus the roots of $x^{2}-(u+iv)x+w+iz=x^{2}-(x_{r}+x_{t})x+x_{r}x_{t}$
are complex. Notice in the third step we set $u^{2}-v^{2}-4w=p$ and
$2uv-4z=q$. Further a critical and subtle part of the above is that
the square root of complex numbers is complex, i.e if $\sqrt{p+iq}$
is complex then there exist some $p',q'$ such that $p'+iq'=\sqrt{p+iq}$.
A proof of this is included in the appendix.\\

Great the roots of $x^{2}-(x_{r}+x_{t})x+x_{r}x_{t}$ are complex,
but how does that help us in figuring out if $x_{r}$ or $x_{t}$
is complex. Well let's now find the roots of $x^{2}-(x_{r}+x_{t})x+x_{r}x_{t}$
\begin{align*}
x & =\frac{x_{r}+x_{t}\pm\sqrt{(x_{r}+x_{t})^{2}-4x_{r}x_{t}}}{2}\\
 & =\frac{x_{r}+x_{t}\pm\sqrt{x_{r}^{2}+2x_{r}x_{t}+x_{t}^{2}-4x_{r}x_{t}}}{2}\\
 & =\frac{x_{r}+x_{t}\pm\sqrt{x_{r}^{2}-2x_{r}x_{t}+x_{t}^{2}}}{2}\\
 & =\frac{x_{r}+x_{t}\pm\sqrt{(x_{r}-x_{t})^{2}}}{2}\\
 & =\frac{x_{r}+x_{t}\pm(x_{r}-x_{t})}{2}\\
 & =x_{r},x_{t}
\end{align*}

So the roots of $x^{2}-(x_{r}+x_{t})x+x_{r}x_{t}$ are non other than
$x_{r}$ and $x_{t}$, which must be complex.

\subsection*{Conclusion}

There is a lot involved in this proof, so let's recap what has been
done. We started with some polynomial $C(x)$ this polynomial had
complex coefficents, we multiplied $C(x)$ with its conjugate $\bar{C(x)}$,
which left us with a polynomial with real coefficents, we called this
polynomial $R(x)$. Now our goal was to show that this polynomial
$R(x)$ had at least one complex root. We found that complex root
by inducting on the even part of the degree of the polynomial $R(x)$.
Induction enabled us to assume that polynomials of \char`\"{}lesser''
degree do have at least one complex root. We were able to construct
a polynomial, $G_{s}(x)$ that not only satisfied the induction hypothesis,
but whose roots were the roots of $R(x)$. So when we find $G_{s}(x)$'s
complex root, we also found $R(x)$ complex root.

\subsection*{Every single variable polynomial of degree $n$ has exactly $n$
roots}

I stated earlier that the statement:

\fbox{\begin{minipage}[c]{30em}%
 Every single variable polynomial of degree $n$ has exactly $n$
roots %
\end{minipage}}\\

Is really a corollary of what I will be referring to as the Fundamental
Thereom of Algebra, which is:\\

\emph{Any polynomial with complex coefficents greater than degree
zero has at least one complex root}.\\

Let's prove this using the Fundamental Thereom of Algebra.

Let 
\[
f_{0}(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{1}x+a_{0}
\]
where the $a_{i}$ are real (it still holds if they are complex).
we know by the Fundamental Thereom of Algebra that $f_{0}(x)$ has
at least one complex root, let's call it $r_{1}$. Then we can factor
$r_{1}$ out of $f_{0}(x)$ using long division. We know this step
is legitamate, because in both the field of reals and complex we can
divide polynomials by linear factors. In other words since they are
fields, we can divide.\\

Let's denote the new function we obtain when we divide $f_{0}(x)$
by $(x-r_{1})$ as $f_{1}(x)$. Further when we compute this division
we will have no remainder. If we did have a remainder then 
\[
f_{0}(r_{1})=\underbrace{(r_{1}-r_{1})f_{1}(r_{1})}_{\text{zero}}+\underbrace{\text{remainder}}_{\text{not zero}}
\]
would not equal zero. Also the degree of $f_{0}(x)$ was $n$, therefore
the degree of $f_{1}(x)$ must be $n-1$, since 
\[
\underbrace{f_{0}(x)}_{\text{degree }n}=\underbrace{(x-r_{1})}_{\text{degree }1}f_{1}(x)
\]
since exponents add $f_{1}(x)$ must have degree $n-1$.\\

Now we apply the Fundamental Thereom of Algebra to $f_{1}(x)$ this
gives us a root we will call $r_{2}$. We divide $f_{1}(x)$ by $(x-r_{2})$
to get $f_{2}(x)$. Again the remainder must be zero and the degree
of $f_{2}(x)$ will be $n-2$.\\

It's important to see that we have found two roots and we've reduced
the polynomial by two degrees. Therefore if we repeat this process
$n$ times we will have exactly $n$ roots and we will have reduced
the polynomial to degree $n-n=0$, for which no roots will exist.
Thus we have found exactly $n$ roots for a polynomial of degree $n$.

%$y_{s,1,1} = 2x_1 + s x_1^2$ and $y_{s,1,2} = x_1 +x_2 + s x_1 x_2$ and $y_{s,2,2} = 2x_2 + s x_2^2$.  Therefore 
%\begin{align*}
%G_s(x) &= (x-y_{s,1,1})(x-y_{s,1,2})(x-y_{s,2,2}) \\
%&= x^3 \\
%& \hspace{1em} -(s(x_1^2+x_1 x_2+x_2^2 \\
%& \hspace{10em} +3(x_1+x_2))x^2 \\
%& \hspace*{1em} + (s^2(x_1 x_2^3 + x_1^3 x_2) \\
%& \hspace{9em} + s(x_1^3 +x_2^3 \\
%& \hspace{15em} + 5(x_1 x_2^2 + x_1^2 x_2)) \\
%& \hspace{23em} +2(x_1^2+x_2^2) + 8x_1 x_2)x \\
%& \hspace*{1em} -s^3 x_1^3 x_2^3 \\
%& \hspace{5em} - 3s^2(x_1^2x_2^3 + x_1^3 x_2^2) \\
%& \hspace{14em} -2s(x_1 x_2^3 + 4x_1^2 x_2^2 + x_1^3 x_2) \\
%& \hspace{26em} -4(x_1 x_2^2 + x_1^2 x_2)
%\end{align*}

%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% Maybe include? probably not
%\section*{So What? An argument for the Fundamental Theorem of Algebra}  
%The Fundamental Theorem of Algebra states that there is at least one complex root.  What if there was not?  In other words let's suppose that every equation has a solution, and the solution is not in the complex numbers, where would it be?
%\subsection*{A tour of different fields}
%\emph{Take readers on a tour of ordered and non-ordered fields}
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

\begin{appendices} 

\section*{The Mapping $\Phi$}

Let $f(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{0}$, where $a_{i}\in\mathbb{Z}$,
for $i=0,1,2,\ldots,n$.\\

Define: 
\[
\Phi_{b}(f(x))=\int f(x)\delta(b-x)dx=f(b)
\]
where $\delta$ is the dirac delta, and $b$ is the base of the integer
representation we are mapping into.

So if we were mapping into the base-10 representation of the integers,
denoted $\mathbb{Z}_{10}$, we would have: 
\[
\Phi_{10}(f(x))=\int f(x)\delta(10-x)dx=f(10)
\]
It should be stated that $\Phi$ maps the polynomials with integer
coefficients to the integers.

\subsection*{$\Phi$ preserves addition and multiplication}

%In order to show that $\Phi$ is a ring homomorphism it is necessary to show that it is closed under addition and multiplication, \emph{but I also need to show $\Phi(1_F) = 1$}.

Given two polynomials with integer coefficients $f_{1},f_{2}$. 

\paragraph*{Preserves addition}

\begin{align*}
\Phi_{b}(f_{1}(x)+f_{2}(x)) & =\int(f_{1}(x)+f_{2}(x))\delta(b-x)dx\\
 & =\int f_{1}(x)\delta(b-x)dx+\int f_{2}(x)\delta(b-x)dx\\
 & =f_{1}(b)+f_{2}(b)\\
 & =\Phi_{b}(f_{1}(x))+\Phi_{b}(f_{2}(x))
\end{align*}


\paragraph{Preserves multiplication}

\begin{align*}
\Phi_{b}(f_{1}(x)f_{2}(x)) & =\int f_{1}(x)f_{2}(x)\delta(b-x)dx\\
 & =f_{1}(b)f_{2}(b)-\int f'_{1}(x)g_{2}(x)dx\text{ By integrating by parts }\\
 & =f_{1}(b)f_{2}(b)\\
 & =\Phi_{b}(f_{1}(x))\Phi_{b}(f_{2}(x))
\end{align*}

Where $\int f'_{1}(x)g_{2}(x)dx=0$ because $g_{2}(b)=f_{2}(b)$\footnote{$f_{2}(b)\in\mathbb{Z}$}
when $x=b$, but $g_{2}(x)$ is zero everywhere else, so the integral
has measure zero.

\section*{Polynomials are continuous}

Below is a proof that polynomials are continuous. In other words,
in the real plane a polynomial function is continuous at every point,
therefore it is continuous on every interval in $\mathbb{R}$.

We need the product rule for limits. Let $f,g$ be real valued $\Lim{x\rightarrow c}f(x)=l$
and $\Lim{x\rightarrow c}g(x)=k$. Then: $\Lim{x\rightarrow c}(f(x)g(x))=lk$

We will also need to remember the combined sum rule for limits: Let
$f,g$ be real valued $\Lim{x\rightarrow c}f(x)=l$ and $\Lim{x\rightarrow c}g(x)=k$.
Let $\lambda,\kappa\in\mathbb{R}$. Then $\Lim{x\rightarrow c}(\lambda f(x)+\kappa g(x))=\lambda l+\kappa k$

Consider the function $l(x)=x$. Then $\Lim{x\rightarrow c}l(x)=c$.
Then by applying the product rule for limits to $\Lim{x\rightarrow c}l(x)l(x)=\Lim{x\rightarrow c}x^{2}=c^{2}$.
We can continue applying this rule so that for any value $d\in\mathbb{N}$
we have $\Lim{x\rightarrow c}x^{d}=c^{d}$. Let $P(x)=a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{1}x+a_{0}$.
Now by applying the combined sum rule to $P(x)$ we get $\Lim{x\rightarrow c}P(x)=\Lim{x\rightarrow c}a_{n}x^{n}+a_{n-1}x^{n-1}+\cdots+a_{1}x+a_{0}=a_{n}c^{n}+a_{n-1}c^{n-1}+\cdots+a_{1}c+a_{0}=P(c)$.
Therefore $P(x)$ is continuous for any value $c$.

%Using induction, first prove that a polynomial of degree 1 is continuous.  Let $m,b \in \mathbb{R}$, and let $f$ be a real function defined as $f(x) = m x + b$.  Then we want to show that $f$ is continuous at every real number $c \in \mathbb{R}$.

%Assume $m \neq 0$, let $\epsilon > 0$, and $\delta = \frac{\epsilon}{|m|}$.  Then whenever $|x-c| < \delta$.
%\begin{align*}
%|f(x)-f(c)| &= |mx+b -mc -b| \\
%&= |m(x-c)| \\
%&= |m||x-c| \\
%&< |m| \delta \\
%= \epsilon
%\end{align*}
%Therefore we have found a $\delta$ for a given $\epsilon$ so that $|f(x)-f(c)|< \epsilon$ whenever $|x-c| < \delta$.  The case when $m=0$, follows similar, but simpler logic.

\section*{Proof of Intermediate Value Theorem}

Below is a proof of the Intermediate Value Theorem.

Consider a function f

\section*{Proof the square root of a complex number is complex}

\footnote{Thanks to \url{http://math.stackexchange.com/questions/883030/proof-for-complex-numbers-and-square-root}}
Let $w=s(\cos(\theta)+i\sin(\theta))$ be a non-zero complex number
such that $w^{2}=z$. Therefore: 
\[
w^{2}=(s(\cos(\theta)+i\sin(\theta)))^{2}=s^{2}(\cos(2\theta)+i\sin(2\theta))=r(\cos(\theta')+i\sin(\theta')
\]
by De Moivre's theorem.

$\cos(2\theta)=\cos(\theta')$ so $2\theta=\pm\theta'+2n\pi$, where
$n$ is an integer

Similarly $\sin(2\theta)=\sin(\theta')$ so $2\theta=\pm\theta'+2l\pi$,
where $l$ is an integer.

\emph{TBD}

\end{appendices} 
\end{document}
