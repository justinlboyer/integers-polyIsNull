\documentclass[12pt]{article}

\usepackage{exsheets}
\usepackage[toc,page]{appendix} 
\usepackage[fleqn]{amsmath}
\usepackage{amssymb}
\usepackage{amstext}  
%\usepackage{amsthm} 
\usepackage{array}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs} 
\usepackage{graphicx,hyperref}
\usepackage{listings}
%\usepackage{indentfirst}
\usepackage[top=1.5cm, bottom=1.5cm, left=.5in, right=.5in]{geometry}
%\usepackage{setspace}
%\doublespacing
\usepackage{enumerate}
\usepackage{multicol}
\usepackage{dcolumn}
\newcolumntype{2}{D{.}{}{2.0}}


\usepackage{pgf,tikz}
\usepackage{mathrsfs}
\usetikzlibrary{arrows}
\usepackage{wrapfig}

\newcommand{\ssol}{\vspace{3em}}
\newcommand{\lsol}{\vspace{10em}}
\newcommand{\blnk}{{\underline {\hspace{1.5in}}}}
\newcommand{\Lim}[1]{\raisebox{0.5ex}{\scalebox{0.8}{$\displaystyle \lim_{#1}\;$}}}


%\usepackage{mcode}

\setcounter{secnumdepth}{0} % Turns off automatic numbering of sections.

%\SetupExSheets{headings=block}






\begin{document}
%%%(change to appropriate class and semester)



Justin Boyer\\
Comparing the integers and the polynomials with integer coefficients\\

%Idea: Compare often means divide, so look at these two when operated on with division.

This write up:
\begin{itemize}
\item Gives an understandable proof of the fundamental theorem of algebra
\end{itemize}

\section*{The Mapping $\Phi_b$}
$$
\Phi_b: \text{Polynomials with integer coefficients} \mapsto \text{The integers in base }b
$$
What I want to do is create a mapping that takes a polynomial with integer coefficients to a corresponding element in any base $b$.  For example I want this mapping to take $2x^2+3x+4 \xrightarrow[\Phi_{10}]{} 234$.\\  A mapping such as $\Phi_b$ could be achieved by simply  substituting $x$ in the polynomial $f(x) = a_n x^n +a_{n-1} x^{n-1} + \cdots + a_0$ with a constant $b$ (note: $a_i \in \mathbb{Z}$, for $i=0, 1, 2, \ldots, n$).  In the case of the example: $2x^2+3x+4 \xrightarrow[\Phi_{10}]{} 234$, I substituted $x$ with $10$.  Hence the symbol $\Phi_{10}$ denotes the map that takes the polynomial to the integers base $10$, which is the set of integers we are all used to.  In other words:
$$ \Phi_b(f(x)) = f(b)$$

All this map, $\Phi_b$ does is take a polynomial and replace every $x$ with an $b$.  For further insight into how this could be accomplished analytically please see the appendix.

\paragraph*{Example}
$f(x)= 5x^4+4x^3+2x+1$, So $\Phi_{10}(f(x))= f(10) = 5(10)^4 + 4(10)^3 + 2(10) + 1 = 54021$  In other words $5x^4+4x^3+2x+1 \xrightarrow[\Phi_{10}]{} 54021$, which means $5x^4+4x^3+2x+1$ goes to $54021$ under $\Phi_{10}$.

\paragraph*{Note on notation}
It is common to use $\mathbb{Z}[x]$ to denote the ring of polynomials with integer coefficients.  I will do my best to be explicit, but this notation might slip in some places. 


\subsection*{$\Phi_b$ preserves addition and multiplication}
%In order to show that $\Phi$ is a ring homomorphism it is necessary to show that it is closed under addition and multiplication, \emph{but I also need to show $\Phi(1_F) = 1$}.

The following proofs use two polynomials with integer coefficients $f_1, f_2$.
\paragraph*{$\Phi_b$ preserves addition}
\begin{align*}
\Phi_b(f_1(x)+f_2(x)) &= f_1(b) + f_2(b)\\
&= \Phi_b(f_1(x)) + \Phi_b(f_2(x))
\end{align*}

\paragraph{$\Phi_b$ preserves multiplication}
\begin{align*}
\Phi_b(f_1(x) f_2(x)) &=  f_1(b) f_2(b)\\
&= \Phi_b(f_1(x)) \Phi_b(f_2(x))
\end{align*}


\subsection*{A polynomial maps to zero if and only if the polynomial is divisible by $x-b$}
%Now I will show that a polynomial maps to zero if and only if the polynomial is divisible by $x-b$, where $b$ is the base used for the integers.  
%\paragraph*{If $x-b$ divides the polynomial then $\Phi \rightarrow 0$}
%Take $f(x)$  a polynomial with integer coefficients.  Since $x-b$ divides $f(x)$ we may write $f(x) = (x-b)g(x)$, where $g(x)$ is a nonzero polynomial with integer coefficients.
%\begin{align*}
%\Phi_b(f(x)) &= \Phi_b(x-b) \Phi_b(g(x)) \\
%&= (b-b) g(b) \\
%&= 0 
%\end{align*}

%\paragraph*{If a polynomial maps to zero under $\Phi_b$ then the polynomial is divisible by $x-b$}
%Take a non-zero polynomial with integer coefficients, $f(x)$. By the remainder theorem, $f(x)=q(x)(x-c) + f(c)$, for some polynomial with integer coefficients $q(x)$ and $c, f(c) \in \mathbb{Z}$.\\
%
% Then by problem statement, $\Phi_b(f(x)) = f(b) = 0 $.  So $f(b) = q(b)(b-c) + f(c) = 0$. Because $q(b) \neq 0$ it is possible to write $b-c = \frac{-f(c)}{q(b)}$.\footnote{$q(b) \neq 0$, because if it did then $q(b) = 0 \Rightarrow f(c) = 0, \forall c$ then $q(b) = 0$ to begin with, which implies $f(x) = 0$ to initially.}  A solution exists if $f(c)=0$, well $f(b)=0$ so take $c=b$, which implies $b-b = \frac{f(b)}{q(b)} \Rightarrow 0 = \frac{0}{q(b)}$.  Therefore $c=b$ and if a polynomial maps to 0 under $\Phi_b$ it is divisible by $x-b$

Take $f(x) = a_n x^n +a_{n-1} x^{n-1} + \cdots + a_0$, where $a_i \in \mathbb{Z}$, for $i=0, 1, 2, \ldots, n$.  Let $r$ be the number such that $$f(r) = 0$$.

By long division of polynomials
$$ a_n x^n + \cdots + a_0 = (x-r)*(c_{n-1} x^{n-1} + \cdots c_0) + \text{constant}$$.

Substituting $x=r$ into the both sides of the equation above we get
$$ 0 = 0 + \text{constant}$$.

So by necessity $\text{constant}=0$, therefore  $$ f(x) = (x-r)(c_{n-1} x^{n-1} + \cdots c_0)$$





\section*{Prime Factorization}
\paragraph*{What questions come to mind as I consider the comparison of prime factorization in the ring of integers and in the ring of polynomials.}
I wonder what encryption would look like using polynomials instead of integers.  I wonder if the set of irreducibles are more dense (is this the correct word?) than the set of all prime numbers.  I wonder which are easier for a computer to calculate, irreducibles or primes.  I wonder if the formulas for primes port over to polynomials/irreducibles.


\subsection*{A rough comparison of prime factorization}
This section will compare prime factorization in the ring of integers and the ring of polynomials with integer coefficients.  Prime factorization is decomposing something into its constituent primes.  Among the integers we have the \emph{the fundamental theorem of arithmetic}, which says that every positive integer has a unique prime factorization.  In math speak this states that any positive integer $k \geq 2$, $k$ may be rewritten as 
$$k = p_1^{l_1} p_2^{l_2} \cdots p_n^{l_n}$$ 
where the $p_i$s are the $n$ prime factors, each of order $l_i$.  One way to understand this is to think of it as taking a positive integer, breaking it up into several unique parts, multiplying those parts together and getting the same positive integer back. \emph{A good metaphor could go here, tried baking, legos, cars, computers all of which were somewhat unsatisfactory}  \\

Do the polynomials with integer in coefficients have a similar analogue?  If you don't know already try to figure out what that analogue would look like?\\

Well we know we are looking to break up the polynomial into a bunch of parts, multiply those parts together and get our original polynomial.  This sure enough does exist we know it as the \emph{fundamental theorem of algebra}!  The fundamental theorem of algebra (roughly) states  that every $n$th degree polynomial has $n$ roots (some of which may be complex).  Tie this together with long division of polynomials and we find that we could rewrite a polynomial $f(x)$ as the product of its roots, i.e., 
$$ f(x) = (x-r_1)^{l_1}(x-r_2)^{l_2}\cdots(x-r_i)^{l_i}$$
where the $r_i$s are the roots of $f(x)$ and the $l_i$ are the multiplicities of the roots.\\

But wait, I slipped something by you, what is the prime factorization of $f(x)= x^2 + 1$?  The roots are complex.  So what mathematicians have decided is that $x^2 + 1$ is prime, well not prime, mathematicians use a different word, \emph{irreducible}.  Irreducible just means that the polynomial can't be reduced, which means that no matter what linear term we use as a divisor on the polynomial we will always have a remainder.  So the only thing that divides an irreducible polynomial is the irreducible polynomial itself and 1, remind you of anything.\\

Since the Fundamental Thereom of Algebra is so critical let's see a proof of it.  However before we do that, let's first look at an example that highlights all the necessary components of the proof, but gives us something concrete to stand on.








% That' right, the \emph{fundamental theorem of arithmetic} and the \emph{fundamental theorem of algebra} communicate the same ideas   we would take the polynomial $f(x) = a_n x^n +a_{n-1} x^{n-1} + \cdots + a_0$ and mod 




\section*{Example Proof of the Fundamental Thereom of Algebra: How many roots does $x^3 -r_1 x^2 +r_2 x - r_3$ have?}
Suppose we have the polynomial $$p(x) = x^3 -r_1 x^2 +r_2 x - r_3$$ and we want to know how many roots it has.  We know $r_1, r_2, r_3$ are real numbers.  Yes, it has three roots in the complex field, but we know that because of a thereom.  The Fundamental Thereom of Algebra to be exact.  Let's walk through an example of a "proof'' that this equation, $p(x)$ has three roots.  This proof will be of a specific case, and it will be a bit circuitious.  But the idea is lay out all the components for a general algebraic proof.

Let's wander through an exercise.  In many of today's classrooms math is seldom wandered.  There is a beginning and an end and teachers debate which path to take the students on.  However my intention for this exercise is to lead\footnote{This is a paper after all.  For the inclined I invite you to wander on your own, get lost, try to find your own way, this paper will always be here.} you on a path that feels like a walkabout.  We will take side steps when we need, because that is how math and life are experienced.  What we hope to accomplish is to prove that there are three roots to the equation $x^3 -r_1 x^2 +r_2 x - r_3$.\\

One way to start would be to set the function equal to 0,  
$$x^3 -r_1 x^2 +r_2 x - r_3 =0$$ 
and try to complete the \sout{square} cube,  which is a fruitful endeavor to sharpen rusty algebraic skills.  However we're after more than that.  We are hoping to find a method that will apply to higher degree polynomials.  Pretend for a moment that you know the roots of  
$$x^3 -r_1 x^2 +r_2 x - r_3$$ 
are not real, not even complex, they lie in some field, $\mathbb{F}$, so we can operate on them with addition, subtraction, multiplication, and division.  Let's call these roots $a_1, a_2, a_3$.  

The next step we would typically take is rewrite 
$$x^3 -r_1 x^2 +r_2 x - r_3$$ 
as: 
$$(x-a_1)(x-a_2)(x-a_3)$$  

But why can we rewrite any polynomial as its linear factors?  If you're like me you learned this through induction.  Your high school teacher made you factor a bunch of quadratic, cubic expressions and if they hated you quartic polynomials and then multiply(foil, rainbow, etc..) them back together and voila you arrived where you started.  Which is a decent motivating reason to make students factor polynomials.\\

The reason we can do this comes down to the fact that we can expand what we are working until we are in a place that allows us to do what we need to.  The analogy I use is suppose you are working on a car in your garage.  Everything is going fine, until you go to take a bracket off, the tools you have don't work for the job at hand.  So you call up your friend he has the tools, so you tow your car to his place and fix it there, then when you are done you drive your car back into your garage.  In this analogy your car is the polynomial and the garage is the real numbers $\mathbb{R}$, when you tow your car to your friends you are using what we will call a field extension, so that you can use your friends tool, the splitting field, to take the car apart and figure out how to fix (find all the roots) it.

\subsection*{Field Extension}
Let's work on wrapping our head around this idea of towing our car to a place where we can fix it.  Suppose you have a playing field of four numbers $\{0, 1, 2, 3\}$, what can we do with these four numbers, we could add them $1+1=2$, $1+2=3$, what about $0+1=1$  we have an additive identity.  What do you think should happen when I try $1+3=$?  Any answer could be correct.  Mathematicians like to play games, so they pick the situation that will continue the game.  They decide to make $1+3=0$ then we can play longer. because then $2+2=0$ as well.  In other words in this playing field we map the number 4 to the number 0, this implies the number 5 would be mapped to 1.  I.e., $$2+3=1$$ $$3+3=2$$ $$1+2+3=2$$ $$2+2+2+3=1$$ so on and so forth.  If you teach kids then your mind probably wandered on the last equation in the series to repeated addition and multiplication.  Exactly, now we might be able to get a sense of multiplication in this playing field.
\begin{align*}
2+2+2+3 &=1 \\
2 \cdot 3 +3 &= 1 \\
(2+1) \cdot 3 &= 1 \text{ Using distributive property} \\
3 \cdot 3 &= 1
\end{align*}
If $3 \cdot 3 = 1$, then multiplying both sides by 3, we get
\begin{align*}
3 \cdot 3 \cdot 3 &= 3 \\
1 \cdot 3 &= 3
\end{align*}
We have a multiplicative identity.  What else is (not) possible with this playing field?\\

So we have addition, we have multiplication, let's try division.  If we rely on our models about division we might have problems, we need to really understand what division is.  The Egyptians had a pretty good idea, they were one of the first peoples to record their method of division.  They framed it as a multiplication problem with the multiplier (or multiplicand)\footnote{Why does it not matter if it is the multiplier or multiplicand that is missing?} missing. So $$3 \div 3$$ would be $$3 \cdot ? = 3$$  
The Egyptians then used their times tables to deduce the the answer.  At its core this is what division is, multiplication in reverse, multiplication in-reverse, multiplication in-verse.  We invert multiplication, this is why in dividing fractions we flip and multiply, division \underline{is} inverting and multiplying.  So that saying: "Dividing fractions don't know why, flip the second number and multiply'' is frighteningly ironic.\\

From here it will help to discover division if we have our own times table:
\begin{center}
\renewcommand\arraystretch{1.3}
\setlength\doublerulesep{0pt}
\begin{tabular}{r||*{4}{2|}}
$\times$ & 0 & 1 & 2 & 3 \\
\hline\hline
0 & 0 & 0 & 0 & 0 \\ 
\hline
1 & 0 & 1 & 2 & 3 \\ 
\hline
2 & 0 & 2 & 0 & 2 \\ 
\hline
3 & 0 & 3 & 2 & 1 \\ 
\hline
\end{tabular}
\end{center}
Notice that we have something interesting going on. 
\begin{align*}
2 \cdot 0 = 0\\
2 \cdot 1 = 2\\
2 \cdot 2 = 0\\
2 \cdot 3 = 2
\end{align*}

Now if we try to define division we don't have uniqueness, for example: recall $$2 \div 2 = ?$$ is equivalent to $$2 \cdot ? = 2$$ well both 1 and 3 satisfy the equation, $2 \cdot ? = 2$.  Why is this?  After some digging you would uncover that the root of the problem is the fact that 4 is a composite of 2.  These times tables are built off what is the remainder when we divide by 4, since 4 is composed of two twos, we run into problems when we would try division by 2.  How could we avoid this problem?\\

We could extend our playing field to a set of numbers whose size (cardinality) is not composite, for example we could extend it to using five numbers.  The multiplication table for this new playing field is below.  
\begin{center}
\renewcommand\arraystretch{1.3}
\setlength\doublerulesep{0pt}
\begin{tabular}{r||*{5}{2|}}
$\times$ & 0 & 1 & 2 & 3 & 4 \\
\hline\hline
0 & 0 & 0 & 0 & 0 & 0 \\ 
\hline
1 & 0 & 1 & 2 & 3 & 4 \\ 
\hline
2 & 0 & 2 & 4 & 1 & 3 \\ 
\hline
3 & 0 & 3 & 1 & 4 & 2 \\ 
\hline
4 & 0 & 4 & 3 & 2 & 1 \\ 
\hline
\end{tabular}
\end{center}
If you look down the columns you notice something that might remind you of sudoku.  Each column (except 0) contains a full set of the numbers we have in our playing field.  This makes our division unique because multiplication is unique.

\paragraph*{Example} Let's pick the two's column again, what is $2 \div 2$ rewriting this becomes $2 \cdot ? = 2$ which looking at our table is one.   What about $2 \div 1$?  Rephrasing as multiplication $2 \cdot ? = 1$ so two divided by one is three. Or phrased another way three is the inverse of two.\\

We coudn't divide in our field when it had four elements, but when we added an element and made the size of our playing field prime, we could.  This is the idea of a field extension.  We start with some (playing) field and we make the field larger so that it will satisfy some additional properties. 

\subsubsection*{Applying the idea of field extension to polynomials}
Let's suppose that we are in the real numbers with any polynomial of the form $$x^3+x^2+x+1$$ and we want to find the splitting field.  It looks like $x=-1$ might be a root. So we complete long division\footnote{Why can we use long division here?  Because we are in a field, if we are in a field, then long division is defined.  The field we are in is the field of polynomials with integer coefficients.} to find that
$$x^3+x^2+x+1 = (x+1)(x^2+1)$$ 
You might look at the $x^2+1$ term and be tempted to say it has "no solution in the reals'', which is technically correct.  But there is another way to frame it.  We can go on what we know, which is 
$$x^2+1=0$$ 
and from that it follows that 
$$x^2=-1$$ 
This would be a field extension.  For example we can look at multiplication in this field extension, commonly called $\mathbb{Z}[x]/(x^2+1)$ (which means the field of polynomials with integer coefficents modular $x^2+1$).
\begin{align*}
(a+bx)(c+dx) & = (a+bx)c + (a+bx)dx \\
&= ac + bcx + adx +bdx^2 \\
&= (ac+bdx^2) + (ad + bc)x \\
&= (ac-bd) + (ad + bc)x
\end{align*}
Where the last step used the identity $x^2=-1$.  In some ways working from this place one might say we are building the complex numbers from the real numbers.  But the reason we are doing this, is so we can split any polynomial up.  We are extending the field as a quotient ring generated by $x^2+1$.  $x^2+1$ is also know as an irreducible factor, and when it plays this role in the quotient ring, it is known as the ideal.

\paragraph*{Example} Let's calculate the splitting field for $$x^4+x^3+x^2+1$$  
We'll we already know $x^2+1=0$ so
\begin{align*}
x^4+x^3+x^2+1 &= x^4 + x^3 + 0 \\
&= x^3(x+1)
\end{align*}
Thus we split our polynomial by extending our field.  But we're not quite done, recall $x^2=-1$ this implies $x^3 = -x$.  Therefore:
$$x^4+x^3+x^2+1 = -x(x+1)$$ in our new field $\mathbb{Z}/(x^2+1)$.  Furthermore we have split our polynomial into linear factors, this is the splitting field, and we can see that 0 and -1 are the roots in this new field.\\

{\color{red}\emph{Need Help! Prove that there exists a splitting field and that it has $n$ terms!}}\\


We may write $$x^3 -r_1 x^2 +r_2 x - r_3$$ as $$(x-a_1)(x-a_2)(x-a_3)$$  because we look at it in a field in which it is possible, just as we did in the above examples. \\

{\color{red}\emph{How would I do it for this example?}} \\

%What we want to do first is factor $x^3 -r_1 x^2 +r_2 x - r_3$ as far as possible, suppose $x^3 -r_1 x^2 +r_2 x - r_3$ is as far as we can factor it in our field.   Then $$x^3 -r_1 x^2 +r_2 x - r_3 =0$$
%This then implies that $$x^3 -r_1 x^2 = r_3- r_2 x$$
%Thus $$x^3 -r_1 x^2 +r_2 x - r_3 = r_3- r_2 x +r_2 x - r_3 = 0$$ it works!  
%Let $$f(x) = x^n + b_{n-1}x^{n-1} + \cdots + b_1 x + b_0$$ be our ideal (like $x^2+1$), with $n \leq 3$  we know $a_1$ is a root, therefore:
%\begin{align*}
%a_1^n + b_{n-1}a_1^{n-1} + \cdots + b_1 a_1 + b_0 &=0 \\
%a_1^n  &= -(b_{n-1}a_1^{n-1} + \cdots + b_1 a_1 + b_0) \\
%a_1^na_1^{3-n} &= -(b_{n-1}a_1^{n-1} + \cdots + b_1 a_1 + b_0)a_1^{3-n} \\
%&= -(b_{n-1}a_1^{3-1} + \cdots + b_1 a_1^{3-n+1} + b_0 a_1^{3-n}) \\
%&= -(b_{n-1}a_1^{2} + \cdots + b_1 a_1^{4-n} + b_0 a_1^{3-n})
%\end{align*}
%In this case we would take $n=0,1,2$

%in this case we know the roots, so let's just do long division, which gives us:
%$$x^4+x^3+x^2+1 = (x-a_1)(x^3 + (a_1+1)x^2 + (a_1^2+a_1+1) + (a_1^3+a_1^2+a_1))$$


To recap, we found some field $\mathbb{F}$ that allows us to split $$x^3 -r_1 x^2 +r_2 x - r_3$$ into $$(x-a_1)(x-a_2)(x-a_3)$$ and $a_1, a_2, a_3$ are elements of this field $\mathbb{F}$, we are not really sure what $\mathbb{F}$ is, we just know that it exists.\\

{\color{red}\emph{How do we know we will have 3 roots in C?}}\\

We might be tempted to say we're done, we have three roots!  But we can't, because we are in this strange field $\mathbb{F}$, in other words our car is taken apart in our friends garage.  We first need to make sense of these roots $a_1, a_2, a_3$.


Well let's look at $a_1, a_2, a_3$ more closely.  We know that $\mathbb{F}$ is a field so all the properties of addition and multiplication we are used to remain the same.  So we are welcome to distribute and multiply, when we do this we get:
$$ (x-a_1)(x-a_2)(x-a_3) = x^3 -(a_1+a_2+a_3)x^2+(a_1a_2 +a_1a_3 +a_2a_3)x -a_1a_2a_3$$

Something really cool is going on!  Figure it out before you move on.

\subsection*{Symmetric Polynomials}
Let's investigate 
$$x^3 -(a_1+a_2+a_3)x^2+(a_1a_2 +a_1a_3 +a_2a_3)x -a_1a_2a_3$$ 
more throughly.  Let's assign the following:
\begin{align*}
e_1 = a_1 + a_2 +a_3 \\
e_2 = a_1a_2 +a_1a_3 +a_2a_3 \\
e_3 = a_1a_2a_3
\end{align*}

The $e_1, e_2, e_3$ we just designated are called elementary symmetric polynomials.  Vi\'ete found formulas that draw a connection between a polynomial's coefficents and its roots.  We are going to leverage those formulas now.  It has been shown that Vi\'ete's formulas may be applied with polynomials with coefficients in any field.{\color{red}\emph{This is correct right? if Vietes works in most integral domians then it surely will work in a field}}


As an example of how Vi\'ete's formulas relate roots and coefficents, let's look at how we could write
$$ a_1^2 + a_2^2 + a_3^2$$
as our elementary symmetric polynomials.  We need the sum of squares so we will need $$e_1^2 = a_1^2 + a_2^2 + a_3^2 + 2a_1a_2 + 2a_1a_3 + 2a_2a_3$$
Now we need to remove those cross terms well $e_2$ should work nicely for that
$$e_1^2 - 2e_2 = a_1^2 + a_2^2 + a_3^2 + 2a_1a_2 + 2a_1a_3 + 2a_2a_3 - 2(a_1a_2 +a_1a_3 +a_2a_3)$$
after zeroing out terms we arrive at
$$e_1^2 - 2e_2 = a_1^2 + a_2^2 + a_3^2$$


The elementary symmetric polynomials make the rest of our "proof'' possible.  As you noticed the $e_1, e_2, e_3$ are the same as $r_1, r_2, r_3$ respectively.  This means we can relate the roots in our strange field $\mathbb{F}$ with the coefficents of our original polynomial.  We can put our car back together.  Since the $r_1, r_2, r_3$ are real valued, then $e_1, e_2, e_3$ must be real valued.  But we still can't make the leap to $a_1, a_2, a_3$ being real valued, for example suppose 
\begin{align*}
a_1 = 1-i \\
a_2 = 1+i \\
a_3 = 1
\end{align*}
Then our elementary symmetric polynomials would become
\begin{align*}
e_1 = 3 \\
e_2 = 4 \\
e_3 = 2
\end{align*}

The elementary symmetric polynomials are real, however our roots are not necessarily real valued.  Thus we cannot yet say we have three roots.


In order to sort this root problem out, we "need'' to use induction, but before we do that we should to develop our intuition on a new equation.  This new equation will have the following form:
$$G_s(x) = (x - a_1 - a_2 - sa_1a_2)(x - a_1 - a_3 - sa_1a_3)(x - a_2 - a_3 - sa_2a_3)$$
where the $a_1, a_2, a_3$ are the roots of our original polynomial in some field $\mathbb{F}$ and $s$ is a real number.  When we expand $G_s(x)$ we get:
%$$ G_s(x) = x^2 - x(a_1 + 2a_2 + a_3 + s (a_1 a_2 + a_2 a_3))  + a_1 a_2 + a_1 a_3 + a_2 a_3 + a_2^2 + s(a_1 a_2^2 + 2a_1 a_2 a_3 + a_2^2 a_3) + s^2 a_1 a_2^2 a_3$$

\begin{flalign*}
G_s(x) &= x^3 \\
&\hphantom{{}=x} {\color{green} - x^2 ( 2 a_1 + 2 a_2 + 2 a_3 + a_1 a_2 s + a_1 a_3 s + a_2 a_3 s )} &\\
&\hphantom{{}=x+x} {\color{blue} + x( a_1 a_2 a_3^2 s^2 + a_1 a_2^2 a_3 s^2 + a_1^2 a_2 a_3 s^2 + a_1 a_2^2 s  + a_1 a_3^2 s  + a_2 a_3^2 s  + a_1^2 a_2 s  + a_1^2 a_3 s  + a_2^2 a_3 s}  \\
&\hphantom{{}=x+x+x+x} {\color{blue} + 6 a_1 a_2 a_3 s  + a_1^2  + a_2^2  + a_3^2  + 3 a_1 a_2  + 3 a_1 a_3  + 3 a_2 a_3 )} &\\
&\hphantom{{}=x+x+x} {\color{red} -a_1^2 a_2^2 a_3^2 s^3   - 2 a_1 a_2^2 a_3^2 s^2 - 2 a_1^2 a_2 a_3^2 s^2 - 2 a_1^2 a_2^2 a_3 s^2  - a_1^2 a_2^2 s - a_1^2 a_3^2 s - a_2^2 a_3^2 s} \\
&\hphantom{{}=x+x+x+x} {\color{red} - 3 a_1 a_2 a_3^2 s - 3 a_1 a_2^2 a_3 s - 3 a_1^2 a_2 a_3 s  - a_1 a_2^2 - a_1 a_3^2 - a_2 a_3^2 - a_1^2 a_2 - a_1^2 a_3 - a_2^2 a_3 - 2 a_1 a_2 a_3 }
\end{flalign*}



Notice how we could switch all the $a_1$ with $a_2$ (likewise with $a_1$ and $a_3$ or $a_2$ and $a_3$) and we would end up with an equivalent equation.  We call polynomials of this form symmetric and $G_s(x)$ is symetric in the coefficients $a_1, a_2, a_3$.  Since $G_s(x)$ is symmetric there it can be shown that we can then write $G_s(x)$ in terms of its elementary symmetric polynomials.\footnote{This is known as the Fundamental Theorem of Symmetric Polynomials, the proof of which is a bit beyond this paper.}  Recall that our elementary symmetric polynomials were
\begin{align*}
e_1 = a_1 + a_2 +a_3 \\
e_2 = a_1a_2 +a_1a_3 +a_2a_3 \\
e_3 = a_1a_2a_3
\end{align*}

Then using these we may write 
\begin{flalign*}
G_s(x) &= x^3 \\
&\hphantom{{}=x} {\color{green}- x^2 ( 2 e_1 + e_2 s )} &\\
&\hphantom{{}=x+x} {\color{blue}+ x( e_1 e_3 s^2 + e_1 e_2 s  + e_1^2 - 2e_2  + 3 e_2 ) }&\\
&\hphantom{{}=x+x+x} {\color{red}-e_3^2 s^3   - 2 e_2 e_3 s^2  - (e_2^2 - 2e_1e_3) s  - 3 e_1 e_3 s  - e_1 e_2 + e_3}
\end{flalign*}

The coefficients of the elementary symmetric polynomials are real valued.  Furthermore the elementary symmetric polynomials are real valued {\color{red}\emph{WHY?.. attempt}}.  Recall that the $e_1, e_2, e_3$ relate to the original polynomial, 
$$x^3 -r_1 x^2 +r_2 x - r_3$$
Not only did $e_1, e_2, e_3$ relate to the roots, but they related to the coefficients, i.e.,
\begin{align*}
e_1 = r_1 \\
e_2 = r_2 \\
e_3 = r_3 
\end{align*}

Remember that $r_1, r_2, r_3$ are real valued, $\mathbb{R}$.
So since we may write $G_s(x)$ in terms of the elementary symmetric polynomials, which are equivalent to the coefficents in our original equation. The coefficients of $G_s(x)$ are real valued.

\subsection*{Induction}

Now we will abuse induction in preparation for the coming proof.  We will need to pretend that the orignial polynomial $x^3 -r_1 x^2 +r_2 x - r_3$ has  degree $n= 2^k m$ where $k>0$ and is the largest integer such that $n= 2^k m$ is true and $m$ is odd.  We will show that for any value $k$ there is at least one complex root.  We will do this by inducting on $k$.

\subsubsection*{Base Case}
For the base case $k=0$, the polynomial is odd, the end behavior of odd polynomials with positive leading coefficent is $x \rightarrow -\infty$ the value of the polynomial $ \rightarrow -\infty$ and as $x \rightarrow \infty$ the value of the polynomial $ \rightarrow \infty$.  Additionally recall that a polynomial is continuous, there are no jumps, therefore the polynomial must cross the $x-$axis at some place in between $(-\infty, \infty)$ and we therefore have at least one complex root.  This argument also takes care of any polynomials with odd degree.  We will elaborate on this argument in the generalized proof.

\subsubsection*{Induction Step}
Now for the induction step, we use the induction hypothesis that every polynomial with real coefficients and degree $n=2^{k-1} m'$ (where $m'$ is odd) has at least one complex root.\\

Our original polynomial has degree $n=2^k m$, but the degree of $G_s(x)$ is $2^{k-1} m (n-1)$ (this is proven in the generalized case).  For our specific case $n=3$, which implies that $k=0$ and $m=3$ thus the degree of $G_s(x)$ is $2^{-1} 3 (2) = 3$.\footnote{Technically the following does not hold, because $m'$ is not odd}  Since $G_s(x)$ is of the form $2^{k-1} m'$, we can apply the induction hypothesis.  Thus $G_s(x)$ has at least one complex root.\\

We do not know if this complex root is 
$$a_1 + a_2 + s a_1 a_2$$ 
or 
$$a_1 + a_3 + s a_1 a_3$$ 
or 
$$a_2 + a_3 + s a_2 a_3$$  

For now let's pretend that $a_1 + a_2 + s a_1 a_2$ is complex (the rigor is left for the generalized proof).  Recall that $s$ is real valued, $s \in \mathbb{R}$.  If $s=0$ then  $a_1 + a_2$ must be complex.  What if $s \neq 0$?  If the sum $a_1 + a_2$ is real for a complex number, then the imaginary parts must zero out, then in order for $a_1 + a_2 + s a_1 a_2$ to be complex $a_1 a_2$ must be complex.  Remember that $s$ is our parameter and it is real valued, so the fact that $a_1 + a_2 + s a_1 a_2$ is complex does not depend on $s$.  Thus the sum $a_1 + a_2$ is complex and the product $a_1 a_2$ is complex.  Now we need to show that $a_1$ is complex and $a_2$ is complex.\\

Consider the equation $$x^2 - (a_1 + a_2)x + a_1 a_2$$  the roots of this equation are $a_1$ and $a_2$, since the degree of this equation is of the form $2^{k-1}m'$ by the induction hypothesis $x^2 - (a_1 + a_2)x + a_1 a_2$ has at least one complex root.  Thus $a_1$ or $a_2$ is complex.  So we have at least one complex root.  What about the other three?

\subsection*{Our three roots}
Let's pretend that $a_1$ is indeed our complex root.  Then we can factor $a_1$ out of our original equation $x^3 -r_1 x^2 +r_2 x - r_3$.  When we do this we arrive at
$$(x-a_1)(x^2 - \rho_1 x - \rho_2)$$
Now we can apply the same process we went through and find that $x^2 - \rho_1 x - \rho_2$ has a "complex''\footnote{Complex is in quotes, because the imaginary part might be zero} root, suppose this time it is $a_3$, we can factor out this term and when we do this, we get:
$$(x-a_1)(x-a_3)(x-\gamma)$$
Again we apply our process to $x-\gamma$ and we find that $\gamma=a_2$ is complex and we have our full set of roots.




% now work through the fact that g is symmetric then elementary symmetric, then use the fact that there are two roots in the complex blah blah



% does this proof only work for polynomials greater than a certain degree, find out.


\section*{The Fundamental Theorem of Algebra}
The Fundamental Theorem of Algebra as we typically teach and learn it states one of the following three: 
\begin{enumerate}
\item That every single variable polynomial of degree $n$ has exactly $n$ roots (counting multiplicity).
\item Any polynomial with complex coefficients greater than degree 0 has at least one complex root.  
\item The field of complex numbers is closed under algebra.
\end{enumerate}

All of these statements are equivalent, recall a field\footnote{A field is an algebraic structure with a form of addition, subtraction, multiplication and division, satisfying the commutative and distributive properties} $K$ is called \emph{algebraically closed} if every non-constant polynomial $f(c) \in K[x]$ has a root in $K$.

So a field, $K$ is algebraically closed if the roots of every non-constant polynomial (e.g. $x^2+1$) are in $K$.

%\paragraph*{Comprehension check:} With only rereading the definition and explanation of algebraic closure, think of an example of why the field of integers is not closed under algebra.\footnote{$x^2+1$ would work because the roots of $x^2+1$ are $x=\pm i$}

From the definition of algebraically closed it follows that \textbf{statement 2} and \textbf{statement 3} are identical, \textbf{statement 3} is just shorthand for stating \textbf{statement 2}.  Remember that a complex number with no imaginary part is a real number.

\textbf{Statement 1} is how the Fundamental Theorem of Algebra is typically stated in the secondary setting and it really is a corollary of \textbf{statement 2}.  As we saw in the previous section we can factor out the complex number we find from our polynomial.  This leaves us with a linear term multiplying a polynomial one degree less than our original.  We can then find the complex root of this new polynomial, and factor it out.  We can continue this process, but we can only do it $n$ times, thus we get $n$ roots.


\section*{Algebraic proof of The Fundamental Theorem of Algebra}
I will attempt to prove that every non-constant polynomial with complex coefficients has a complex root in a manner that does not require rigorous mathematical study.\footnote{Many thanks to \url{https://www.artofproblemsolving.com/wiki/index.php?title=Fundamental_Theorem_of_Algebra##Algebraic_Proof}}\\

%It is necessary to assume the following
%\begin{enumerate}
%\item Every odd degree polynomial with real coefficients has at least one real root.\footnote{One may prove this with the Intermediate Value Theorem} \emph{Include example picture, proof using IVT in appendix}
%\item Every polynomial of degree two with complex coefficients has a complex root.\footnote{Proven by the fact that every polynomial with real coefficients has a complex root} \emph{Include example picture}
%\end{enumerate}
%
%\paragraph*{Lemma} 

\subsection*{Set up}
Think of a polynomial with complex coefficients (e.g. $x^2+4x+2$, or $(1+2i)x+3$), now think of another.  Now think of every polynomial with complex coefficients and let's let $C(x)$ represent one of them, we don't know which, but it is one of them.\\

\subsubsection*{Complex number review}
Recall that to take the conjugate of a complex number you just switch the addition or subtraction on the imaginary part (e.g. if $z = 1+2i$ the the conjugate of $z$ is $\bar{z} = 1 - 2i$).  Something neat happens when you multiply a complex number with its conjugate. Let's let $z=a+bi$ be any complex number, then the conjugate is $\bar{z}=a-bi$ and if we multiply them together we get $$z\bar{z}=a^2+abi-abi-(bi)^2$$ 
which becomes 
$$z\bar{z}=a^2+b^2$$
 which is a real number, i.e., the imaginary part is 0 (or there is no imaginary part).  Furthermore it is a positive real number!\\

\subsubsection*{Creating a polynomial with real coefficients}
So back to our polynomial with complex coefficients, $C(x)$ if we take $R(x) = C(x) \bar{C(x)}$ (which is $C(x)$ multiplied by its conjugate) then $R(x)$ will be a polynomial with real coefficients, and the roots of $C(x)$ will also be the roots of $R(x)$.  To see this think about how we find roots, one way is to factor, so if we factor $C(x)$ into a bunch of linear terms, say $c_1(x)c_2(x)\cdots c_n(x)$ then we could factor $R(x)$ into $$c_1(x)c_2(x)\cdots c_n(x)\overline{c_1(x)c_2(x) \cdots c_n(x)}= c_1(x)c_2(x)\cdots c_n(x)\bar{c_1(x)} \bar{c_2(x)} \cdots \bar{c_n(x)}$$.  Since, the roots of $C(x)$ are also the roots of $R(x)$, showing that every polynomial with real coefficients has a complex root will complete the proof!


\subsubsection*{Set up summary}
To summarize the above, since the goal is to show that any polynomial with complex coefficients has a complex root then I can build a polynomial with real coefficients by multiplying my polynomial with complex coefficients with its conjugate.  Then if I show that this new polynomial with real coefficients has a complex root, I'm done.

\subsection*{Proof by Induction}
Suppose the degree of $R(x)$ is $d=2^n q$, where $q$ is odd and $n$ is a natural number, $\mathbb{N}$.   
%$R(x)$ will always have an even degree, because the degree of $d(R(x))=d(C(x))*2$.  
Then lets induct on $n$, in other words, we show that the first case is true, then we show that each case follows from the previous, therefore each case must be true.

\pagebreak
\subsubsection*{Base Case}

\begin{wrapfigure}{r}{8cm}
\definecolor{qqwuqq}{rgb}{0.,0.39215686274509803,0.}
\begin{tikzpicture}[line cap=round,line join=round,>=triangle 45,x=0.8cm,y=0.8cm]
\draw[->,color=black] (-4.604132231404959,0.) -- (4.784297520661155,0.);
\foreach \x in {-4.,-3.,-2.,-1.,1.,2.,3.,4.}
\draw[shift={(\x,0)},color=black] (0pt,2pt) -- (0pt,-2pt);
\draw[->,color=black] (0.,-2.9905785123966946) -- (0.,4.249090909090907);
\foreach \y in {-2.,-1.,1.,2.,3.,4.}
\draw[shift={(0,\y)},color=black] (2pt,0pt) -- (-2pt,0pt);
\clip(-4.604132231404959,-2.9905785123966946) rectangle (4.784297520661155,4.249090909090907);
\draw[line width=1.2pt,color=qqwuqq,smooth,samples=100,domain=-4.604132231404959:4.784297520661155] plot(\x,{((\x)+2.0)*((\x)-1.0)*(\x)});
\begin{scriptsize}
\draw[color=qqwuqq] (-2.157851239669423,-2.7757024793388436) node {};
\end{scriptsize}
\end{tikzpicture}
\end{wrapfigure}

If we look at the base case $n=0$, then $R(x)$ is degree $d=q$, where $q$ is odd. Recall that the leading coefficient of a polynomial will determine the overall slope of the equation.  The leading coefficient for $R(x)$ must be positive, because the leading coefficient is the coefficient of the highest degreed term in $C(x)$ multiplied with its conjugate, the product of which will be positive.  So the polynomial $R(x)$ will "start'' somewhere in quadrant three (i.e., $\Lim{x \rightarrow -\infty} R(x) \rightarrow -\infty$), so when $x$ is negative and large enough, $R(x)<0$.  Then as we traverse the function $R(x)$ it will rise up and "end'' in quadrant one (i.e., $\Lim{x \rightarrow \infty} R(x) \rightarrow \infty$), so when $x$ is positive and large enough, $R(x)>0$.\\

  By combining this analysis with the fact that this function is continuous we \emph{know} this function will intersect the x-axis.  The mathematical justification for \emph{knowing} is by implicating the intermediate value theorem, which states that a continuous function, $f$, with an interval $[a,b]$ as its domain, takes values $f(a)$ and $f(b)$ at each end of the interval, then it also takes any value between $f(a)$ and $f(b)$ at some point within the interval.  For the sake of completeness the corollary that applies beautifully here is \textbf{Bolzano's theorem}.  Bolzano's theorem states that if a continuous function has values of opposite sign inside an interval (which we have, i.e., at some point $x<0, R(x)<0$ and also $x>0, R(x)>0$), then it has a root in that interval.  For proofs of the intermediate value theorem and continuity of polynomials see the appendix.

\subsubsection*{Induction Step}
Now for the fun part.  Suppose again that $d=2^n q$, where $q$ is odd and $n>0$, however lets "believe" that the theorem has been proven when the degree is $2^{n-1}q'$ where $q'$ is odd.  Therefore, we can use the "fact" (induction hypothesis) that any polynomial less than or equal to degree $2^{n-1} q'$ has a complex root.\\

Let's start by splitting $R(x)$ into all its linear terms like we did previously, we will also put a factor $a$ out front.  This $a$ would correspond to the coefficient of $x^n$.  This step requires that we "find" a field, $\mathbb{F}$ over which we can split the polynomial into its linear factors.\footnote{For completeness, we are finding the smallest field extension of $\mathbb{C}$, such that $R(x)$ decomposes into linear factors.}  This just means, we might not have the tools to take this polynomial apart where we currently are.  So we go to our friends house that has the tools (find the field) and take it a part there.  In math jargon this would look like: \\
$$c_1(x)c_2(x)\cdots c_n(x)\bar{c_1(x)}\bar{c_2(x)}\cdots \bar{c_d(x)}$$ where the $c_i(x)$ are linear functions in the field, $\mathbb{F}$.  Now let $x_1$ be the root of $c_1(x)$ in the field, $\mathbb{F}$, so on and so forth.  So the roots of $R(x)$ in the field $\mathbb{F}$ are $x_1, \ldots, x_d$.  If this field, $\mathbb{F}$ were the field of polynomials with real valued coeffcients, $\mathbb{R}[x]$ we would be done, but its not.  So now we need to figure out how to relate these roots we found back to the real or complex field.  In order to do that we use the same trickery we used previously.\\



Let $s$ be an arbitrary real number and let $y_{s,i,j} = x_i +x_j + s x_i x_j$, where $1 \leq i < j \leq d$. Now define:
$$ G_s(x) =(x-y_{s,1,2})(x-y_{s,1,3})\cdots(x-y_{s,1,d})(x-y_{s,2,3})\cdots (x-y_{s,2,d})\cdots (x-y_{s,d-1,d})$$
This may be written succinctly as:
$$ G_s(x) = \Pi_{1 \leq i < j \leq d} (x-y_{s,i,j})$$

The $G_s(x)$ we saw previously was constructed in the exact same manner.
It is important to know that the coefficients of $G_s$ are symmetric in $x_1, x_2, \ldots, x_d$.  This means that the variables could be interchanged in any way and we would not need to change the coefficients, in fact, we would have the same polynomial!\footnote{For further investigation read about symmetric polynomials.}  In other words the coefficient of $x_1$ would be the same as the coefficient as $x_d$, likewise the coefficient of $x x_1$ would be the same as $x x_d$, so on and so forth.  It might help to think of Pascal's triangle.  Let's look at a case when $d=2$.

\subsubsection*{Example of $G_s(x)$ when $d=2$ }  This example is very similar to the previous example section, but is included to help concretize the proof.  In this example we will derive $G_s(x)$ from the original polynomial $$f(x)=x^2+bx+c$$ where $b, c$ are real valued.  

Following the layout of the proof first we find the splitting field over which $f(x)$ splits.  From which we get 
$$f(x)=(x-x_1)(x-x_2)$$  
At this point we don't know what type of numbers $x_1$ or $x_2$ are.  They could be complex, they could be something else altogether.  Let's now look at what $G_s(x)$ looks like when $d=2$.  Since $$G_s(x) = (x-y_{s,1,2})$$ let's first create $y_{s,1,2}$: 
$$y_{s,1,2} = x_1 +x_2 + s x_1 x_2$$
 so 
 $$G_s(x) = x- x_1 -x_2 - s x_1 x_2$$

There are many important features to notice about $G_s(x)$.  $G_s(x)$ is a polynomial in $x$, its degree is one, so it is one degree lower than our original polynomial $f(x)$, this feature will come in handy when we use our induction hypothesis.  However in order to use our induction hypothesis we need to know that the coefficients of $G_s(x)$ are real valued.  The trick we use here is rather clever.

Recall that $f(x) = (x-x_1)(x-x_2)$, which if we distribute twice we get $$f(x)= x^2 + x(-x_1-x_2)+ x_1x_2$$
further
$$f(x) = x^2 +bx + x$$
Therefore

\begin{align*}
b = -x_1-x_2 \\ 
c = x_1 x_2
\end{align*}

Well we know that $b, c \in \mathbb{R}$ are real valued.  Thus $-x_1-x_2, x_1x_2$ must be real valued.  This does not necessarily mean that $x_1 \in \mathbb{R}$ or that $x_2 \in \mathbb{R}$ are real valued.  For example if $x_1=1+i$ and $x_2 = 1-i$ then $-x_1-x_2=-2$ and $x_1x_2 = 2$.  But it does mean that the coefficents of $$G_s(x) = x-(x_1+x_2)+x_1 x_2$$
namely $x_1+x_2$ and $x_1 x_2$ are real valued.  Here you saw a concrete version but the fact that $x_1+x_2$ and $x_1 x_2$ are real valued stems from Vi\'ete's formulas, which relate the coefficents of a polynomial to functions of the polynomial.  These functions are what we have been calling the elementary symmetric polynomials.  Now let's return to the proof.






%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
% This is incorrect!
%
%To help with this idea of symmetric coefficients, when $d=3$ we have
%$$
%G_s(x) = (x-x_1-x_2-sx_1x_2)(x-x_2-x_3-sx_2x_3)
%$$
%which expands to
%$$
%x^2+x_2x_3+x_1x_3 + x_1x_2 + x_2^2 -xx_3 - 2x x_2-x x_1 + 2sx_1x_2x_3 + sx_2^2 x_3 + s x_1 x_2^2 - s x x_2 x_3 - s x x_1 x_2 + s^2 x_1 x_2^2 x_3
%$$
%Notice how we could interchange $x_1$ and $x_2$ and the coefficients would remain the same.  In this example the coefficients are all real, but we can't assume the same will occur when $d>3$.\\
%
%The same analysis that applied in the example when $d=2$ and $d=3$ applies when $d>3$, we reduce the degree of the polynomial using $G_s(x)$, check that the coefficients are real then use the induction hypothesis.\\
%
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



% ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%
% explanation of induction
%
%In order to do that we need to use induction\footnote{This step is not completely necessary for this example, however understanding this process will help immensely when we turn towards the proof for any polynomial}.    Remember induction is like climbing a ladder, we show that the first rung is true, then we show that if a previous rung existed then the next one does as well.  Suppose the degree of our polynomial was one this will act as our base case.  A polynomial of degree one is a non-horizontal line, which will cross the $x-$axis once, therefore we will have one root for a polynomial of degree one.  Next we will use the induction hypothesis that a polynomial of degree two has two complex/real roots to show that our third degree polynomial has three complex/real roots.
%
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~






\subsubsection*{Coefficents of $G_s(x)$ are real}
Now back to:
$$ G_s(x) =(x-y_{s,1,2})(x-y_{s,1,3})\cdots(x-y_{s,1,d})(x-y_{s,2,3})\cdots (x-y_{s,2,d})\cdots (x-y_{s,d-1,d})$$
we need to the coefficients of $G_s(x)$ to be real and we need the degree to be $2^{n-1}q'$, where $q'$ is odd.
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%
% Delete ?
%
%We have some motivation to believe that the coefficients of $G_s(x)$ are real.  We will have to take on belief that they are indeed real.\footnote{For the curious, to the best of my knowledge it has not be proven that we may do this.  What the proofs so far claim is that the coefficients of $G_s(x)$ may be viewed as elementary symmetric polynomials.  These polynomials have special formulas which Fran\c{c}ois Vi\'ete discovered in the late 1500's, which enable one to rewrite them as real numbers.  However these formulas only apply to the reals or complex numbers, not necessarily some field extension of the complex field, which is where we are using them.}  
%
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In order to use our induction step we need the coefficients of $G_s(x)$ to be real.  Recall that the induction step states: 
\begin{center}
\fbox{\begin{minipage}{40em}
Polynomials of degree $2^{n-1} q'$ with real valued coefficients have at least one complex root.
\end{minipage}}

\end{center}


%In the two examples we saw a concrete version of $G_s(x)$ in these example the terms when multiplied out were composed of symmetric polynomials in the roots, the $x_i$'s.  

Symmetry is going to play an important role, in enabling us to be able to apply the induction hypothesis.  Recall that symmetry in this situation is when we are able to alternate the coefficents and maintain equality.  
\paragraph*{Symmetric example}
$$ x + y $$ is symmetric, if I replace $x$ with $y$ and $y$ with $x$ the expression $y+x$ is equivalent.  Likewise $$ x+y+z$$ is also symmetric, we may replace $x$ with $z$ and $z$ with $x$ (or any other combination of variables) and maintain equivalancey.  However $$ x^2 + y$$ is not symmetric alternating the variables results in $$ y^2 + x \neq x^2 + y$$  The left hand side is not equivalent to the right hand side.  Let's take a quick jaunt back to our example and see how this and the induction step apply there.

\paragraph*{Return to the example for $d=2$}
Before we move on let's make sense of this step with a concrete example.  We want to use the induction hypothesis which loosely stated for this example is: \\

\emph{Any polynomial with real valued coefficents and degree "less than'' the original polynomial has at least one complex root.}\footnote{Less than is put in quotations, because stictly speaking this is not true, we are inducting on the even part of the degree, the $2^n$ part.  As long as the new polynomial has an $n'<n$ we can utilize our induction hypothesis.}\\

Let's check everything our induction hypothesis requires.

\begin{enumerate}
\item Real valued coefficients: Because the $G_s(x)$ is symmetric the coefficents may be related to the coefficents of the original polynomial, which are real.
\item Lesser degree: $G_s(x)$ is constructed such that it will be "less than'' the degree of the orginal polynomial.\footnote{see previous footnote}
\end{enumerate}


Since the criterion for the induction hypothesis is satisfied we know that $G_s(x)$ has at least one complex root, so $$-(x_1 +x_2) + s x_1 x_2$$ must be complex.  In order to figure out whether $x_1$ or $x_2$ is complex we need to analyze $-(x_1 +x_2) + s x_1 x_2$.  In identifying that $-(x_1 +x_2) + s x_1 x_2$ is complex we did not consider $s$ at all.  Thus, whether or not $x_1$ or $x_2$ is complex does not depend on $s$.  So if $s=0$ then $$ -(x_1+x_2)$$ must be complex.  So $x_1+x_2$ must be complex.  Now let's assume $x_1 = u+iv$ and $x_2 = z+iw$, then if $x_1+x_2$ is real valued we must have $iv=iw$ in other words the imaginary parts must cancel out.  Then:
\begin{align*}
x_1 x_2 &= (u+iv)(z+iw) \\
&= (u+iv)(z-iv) \\
&= uz - ivu +izu - i^2v^2 \\
&= uz + v^2 + i(vz-uv)
\end{align*}
which is complex unless $vz=uv \Rightarrow z=u$ for $v \neq 0$. 

In order to identify if $x_1$ or $x_2$ is our complex root we would need to use the induction hypothesis again, this will be detailed further in the paper.  


\paragraph*{Back to the proof}
We have a very similar situation for the generalized proof.  $G_s(x)$ is constructed so that the coefficients are symmetric.  This stems from the fact that $$y_{s,i,j} = x_i + x_j + s x_i x_j$$ is symmetric in the $x_i, x_j$.    If we then multiply all the combinations of $x-y_{s,i,j}$, where $i<j$ then we will have polynomial that is symmetric in all the $x_i$'s.

 
  Since the coefficients are symmetric then we can express the additive and multiplciative combinations of the $x_1, x_2, \ldots x_d$ as elementary symmetric polynomials\footnote{By the Fundamental Theorem of Symmetric Polynomials}, just as we did in the examples. The elementary symmetric polynomials relate to the original polynomial and are equivalent to the coefficients of $R(x)$.  The coefficients of $R(x)$ are real, therefore the elementary symmetic polynomials are real.  Likewise the additive and multiplciative combinations of the $x_1, x_2, \ldots x_d$ are also real.\\
  
  Put another way we related the coefficients of $G_s(x)$ to the coefficents of our original polynomial, $R(x)$.  Since the coefficents of $R(x)$ are real, the coefficients of $G_s(x)$ must be real.  Note that the coefficents of $G_s(x)$ are additive and multiplicative combinations of $x_1, x_2, \ldots x_d$.  These additive and multiplicative combinations are what are known as the elementary symmetric polynomials.\\
  
  So we have show that the coefficents of $G_s(x)$ are real.  In order to use the induction hypothesis we need to figure out the degree of $G_s(x)$.

\subsubsection*{Degree of $G_s(x)$}
Let's try counting how many terms there are in $G_s(x)$.

There are $d-1$ terms that have $i=1$, then there are $d-2$ terms that have $i=2$, because $i<j$.  So there are $d-3$ terms that have $i=3$ and so on.
$$ G_s(x) =\underbrace{(x-y_{s,1,2})(x-y_{s,1,3})\cdots(x-y_{s,1,d})}_{d-1} \underbrace{(x-y_{s,2,3})\cdots (x-y_{s,2,d})}_{d-2} \cdots (x-y_{s,d-1,d})$$

Thus we have a decreasing sequence, since we are trying to find the degree of $G_s(x)$ we take the $d-1$ terms and add them to $d-2$ terms, so on and so forth.  This gives us a familiar series:
$$ 1 + 2 + 3 + 4 + \cdots + (d-2) + (d-1)$$

The trick for finding the sum of this sequence is often attributed to Gauss.
First define: $$S_n = 1 + 2 + 3 + 4 + \cdots + (d-2) + (d-1)$$

then rewrite $S_n$ as
$$S_n = (d-1) + (d-2) + \cdots + 4 + 3 + 2 + 1$$

add $S_n$ to itself, but we need to be clever here, take notice of how the additive terms were grouped
$$2 S_n = (1 + (d-1)) + (2+(d-2)) + (3 + (d-3)) + \cdots ((d-2) +2) + ((d-1)+1)$$

Perform the arithmetic inside each set of parenthesis
$$ 2 S_n = d + d + d + \cdots + d + d$$

Recall that there were $d-1$ terms in $S_n$ and we did nothing to alter that since we paired up each component so:
$$2 S_n = \underbrace{d + d + d + \cdots + d + d}_{d-1}$$

A model to make multiplication understandable is that multiplication is repeated addition, let's apply that model here:
$$2 S_n = d(d-1)$$

Which leaves us with our result:
$$S_n = \frac{d(d-1)}{2}$$


So the sum of this series is $\frac{d(d-1)}{2}$.  Recall that 
$$d=2^n q$$
 so 
 $$\frac{d(d-1)}{2} = \frac{2^n q (d-1)}{2} = 2^{n-1}q(d-1)$$
  but $q(d-1)$ is an odd number so 
  $$2^{n-1}q(d-1) = 2^{n-1}q'$$

Thus $G_s(x)$ has real coefficents and a degree less than or equal to $2^{n-1}q'$, therefore we can apply the induction hypothesis.

\subsection*{Our Complex root}
Since the degree of $G_s(x)$ is $2^{n-1}q'$ and it has real coefficents the  induction hypothesis applies:

\begin{center}
\fbox{\begin{minipage}{40em}
Polynomials of degree $2^{n-1} q'$ with real valued coefficients have at least one complex root.
\end{minipage}}
\end{center}

  Now we can assume $G_s(x)$ has at least one complex root in order to show that $R(x)$ has at least one complex root.  Without a loss of generality let us say that the root occurs when $i=r$, $j=t$, therefore 
$$y_{s,r,t} = x_r + x_t +s x_r x_t$$ 
is complex for some $s$.  So suppose 
$$s=0$$ 
then 
$$x_r + x_t$$ 
must be complex.  Now Suppose 
$$x_r + x_t$$ 
is real, that would mean that the imaginary parts cancel out, but when we take 
$$x_r x_t$$ 
the imaginary parts would not cancel out, therefore $x_r x_t$ is also complex for some $s$.  Since both $x_r+x_t$ and $x_r x_t$ are complex numbers.  To show that $x_r$ or $x_t$ is complex we consider the equation 
$$x^2 - (x_r +x_t)x + x_r x_t$$  Notice that the coefficents of this equation are complex.  So let's define them as such 
\begin{align*}
x_r + x_t = u + iv \\
x_r x_t = w +iz
\end{align*}
where $u,v,z,w$ are real valued.  Substituting these values into the equation gives us:
$$ x^2 -(u+iv)x + w +iz$$
Let's apply the quadratic formula to this equation and see what we get.
\begin{align*}
x &= \frac{u+iv \pm \sqrt{(u+iv)^2-4(w+iz)}}{2} \\
&= \frac{u+iv \pm \sqrt{u^2 +2iuv - v^2 - 4w - 4iz}}{2} \\
&= \frac{u+iv \pm \sqrt{\underbrace{u^2 - v^2 -4w}_p + i (\underbrace{2uv-4z}_q)}}{2}  \\
&= \frac{u+iv \pm \sqrt{p+iq}}{2} \\
&= \frac{u+ iv \pm (p' +iq')}{2} \\
\end{align*}

Thus the roots of $ x^2 -(u+iv)x + w +iz = x^2 - (x_r +x_t)x + x_r x_t$ are complex.  Notice in the third step we set $u^2 - v^2 -4w=p$ and $2uv-4z=q$.  Further a critical and subtle part of the above is that the square root of complex numbers is complex, i.e if $\sqrt{p+iq}$ is complex then there exist some $p', q'$ such that $p' +iq' = \sqrt{p+iq}$.  A proof of this is included in the appendix.\\

Great the roots of $x^2 - (x_r +x_t)x + x_r x_t$ are complex, but how does that help us in figuring out if $x_r$ or $x_t$ is complex.  Well let's now find the roots of $x^2 - (x_r +x_t)x + x_r x_t$
\begin{align*}
x &= \frac{x_r+x_t \pm \sqrt{(x_r+x_t)^2-4x_rx_t}}{2} \\
&= \frac{x_r+x_t \pm \sqrt{x_r^2 +2x_rx_t+x_t^2-4x_rx_t}}{2} \\
&= \frac{x_r+x_t \pm \sqrt{x_r^2 - 2 x_r x_t +x_t^2}}{2} \\
&= \frac{x_r +x_t \pm \sqrt{(x_r-x_t)^2}}{2} \\
&= \frac{x_r + x_t \pm (x_r-x_t)}{2} \\
&= x_r, x_t
\end{align*}

So the roots of $x^2 - (x_r +x_t)x + x_r x_t$ are non other than $x_r$ and $x_t$, which must be complex.

\subsection*{Conclusion}

There is a lot involved in this proof, so let's recap what has been done.  We started with some polynomial $C(x)$ this polynomial had complex coefficents, we multiplied $C(x)$ with its conjugate $\bar{C(x)}$, which left us with a polynomial with real coefficents, we called this polynomial $R(x)$.  Now our goal was to show that this polynomial $R(x)$ had at least one complex root.  We found that complex root by inducting on the even part of the degree of the polynomial $R(x)$.  Induction enabled us to assume that polynomials of "lesser'' degree do have at least one complex root.  We were able to construct a polynomial, $G_s(x)$ that not only satisfied the induction hypothesis, but whose roots were the roots of $R(x)$.  So when we find $G_s(x)$'s complex root, we also found $R(x)$ complex root.


\subsection*{Every single variable polynomial of degree $n$ has exactly $n$ roots}
I stated earlier that the statement:

\fbox{\begin{minipage}{30em}
Every single variable polynomial of degree $n$ has exactly $n$ roots
\end{minipage}}\\


Is really a corollary of what I will be referring to as the Fundamental Thereom of Algebra, which is:\\

 \emph{Any polynomial with complex coefficents greater than degree zero has at least one complex root}.\\

Let's prove this using the Fundamental Thereom of Algebra.

Let 
$$f_0(x) = a_n x^n + a_{n-1} x^{n-1} + \cdots + a_1 x + a_0$$
where the $a_i$ are real (it still holds if they are complex).  we know by the Fundamental Thereom of Algebra that $f_0(x)$ has at least one complex root, let's call it $r_1$.  Then we can factor $r_1$ out of $f_0(x)$ using long division.  We know this step is legitamate, because in both the field of reals and complex we can divide polynomials by linear factors.  In other words since they are fields, we can divide.\\

Let's denote the new function we obtain when we divide $f_0(x)$ by $(x-r_1)$ as $f_1(x)$.  Further when we compute this division we will have no remainder.  If we did have a remainder then 
$$f_0(r_1)= \underbrace{(r_1-r_1) f_1(r_1)}_{\text{zero}} + \underbrace{\text{remainder}}_{\text{not zero}}$$
would not equal zero.  Also the degree of $f_0(x)$ was $n$, therefore the degree of  $f_1(x)$ must be $n-1$, since 
$$ \underbrace{f_0(x)}_{\text{degree } n}= \underbrace{(x-r_1)}_{\text{degree }1} f_1(x)$$
since exponents add $f_1(x)$ must have degree $n-1$.\\

Now we apply the Fundamental Thereom of Algebra to $f_1(x)$ this gives us a root we will call $r_2$.  We divide $f_1(x)$ by $(x-r_2)$ to get $f_2(x)$.  Again the remainder must be zero and the degree of $f_2(x)$ will be $n-2$.\\

It's important to see that we have found two roots and we've reduced the polynomial by two degrees.  Therefore if we repeat this process $n$ times we will have exactly $n$ roots and we will have reduced the polynomial to degree $n-n=0$, for which no roots will exist.  Thus we have found exactly $n$ roots for a polynomial of degree $n$.








%$y_{s,1,1} = 2x_1 + s x_1^2$ and $y_{s,1,2} = x_1 +x_2 + s x_1 x_2$ and $y_{s,2,2} = 2x_2 + s x_2^2$.  Therefore 
%\begin{align*}
%G_s(x) &= (x-y_{s,1,1})(x-y_{s,1,2})(x-y_{s,2,2}) \\
%&= x^3 \\
%& \hspace{1em} -(s(x_1^2+x_1 x_2+x_2^2 \\
%& \hspace{10em} +3(x_1+x_2))x^2 \\
%& \hspace*{1em} + (s^2(x_1 x_2^3 + x_1^3 x_2) \\
%& \hspace{9em} + s(x_1^3 +x_2^3 \\
%& \hspace{15em} + 5(x_1 x_2^2 + x_1^2 x_2)) \\
%& \hspace{23em} +2(x_1^2+x_2^2) + 8x_1 x_2)x \\
%& \hspace*{1em} -s^3 x_1^3 x_2^3 \\
%& \hspace{5em} - 3s^2(x_1^2x_2^3 + x_1^3 x_2^2) \\
%& \hspace{14em} -2s(x_1 x_2^3 + 4x_1^2 x_2^2 + x_1^3 x_2) \\
%& \hspace{26em} -4(x_1 x_2^2 + x_1^2 x_2)
%\end{align*}





%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
%
% Maybe include? probably not
%
%\section*{So What? An argument for the Fundamental Theorem of Algebra}  
%The Fundamental Theorem of Algebra states that there is at least one complex root.  What if there was not?  In other words let's suppose that every equation has a solution, and the solution is not in the complex numbers, where would it be?
%\subsection*{A tour of different fields}
%\emph{Take readers on a tour of ordered and non-ordered fields}
%
%~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~













\begin{appendices}
\section*{The Mapping $\Phi$}

Let $f(x) = a_n x^n +a_{n-1} x^{n-1} + \cdots + a_0$, where $a_i \in \mathbb{Z}$, for $i=0, 1, 2, \ldots, n$.\\

Define:
$$
\Phi_b(f(x)) = \int f(x) \delta(b-x)dx = f(b)
$$
where $\delta$ is the dirac delta, and $b$ is the base of the integer representation we are mapping into.

So if we were mapping into the base-10 representation of the integers, denoted $\mathbb{Z}_{10}$, we would have:
$$
\Phi_{10}(f(x))  = \int f(x) \delta(10-x) dx = f(10)
$$
It should be stated that $\Phi$ maps the polynomials with integer coefficients to the integers.

\subsection*{$\Phi$ preserves addition and multiplication}
%In order to show that $\Phi$ is a ring homomorphism it is necessary to show that it is closed under addition and multiplication, \emph{but I also need to show $\Phi(1_F) = 1$}.

Given two polynomials with integer coefficients $f_1, f_2$.
\paragraph*{Preserves addition}
\begin{align*}
\Phi_b(f_1(x)+f_2(x)) &= \int (f_1(x) + f_2(x)) \delta(b-x) dx \\
&= \int f_1(x) \delta(b-x) dx + \int f_2(x) \delta(b-x) dx \\
&= f_1(b) + f_2(b)\\
&= \Phi_b(f_1(x)) + \Phi_b(f_2(x))
\end{align*}

\paragraph{Preserves multiplication}
\begin{align*}
\Phi_b(f_1(x) f_2(x)) &= \int f_1(x) f_2(x) \delta(b-x) dx \\
&= f_1(b) f_2(b) - \int f'_1(x) g_2(x)dx \text{ By integrating by parts }\\
&= f_1(b) f_2(b)\\
&= \Phi_b(f_1(x)) \Phi_b(f_2(x))
\end{align*}

Where $\int f'_1(x) g_2(x) dx = 0$ because $g_2(b) = f_2(b)$\footnote{$f_2(b) \in \mathbb{Z}$} when $x=b$, but $g_2(x)$ is zero everywhere else, so the integral has measure zero.



\section*{Polynomials are continuous}
Below is a proof that polynomials are continuous.  In other words, in the real plane a polynomial function is continuous at every point, therefore it is continuous on every interval in $\mathbb{R}$.

We need the product rule for limits.  Let $f,g$ be real valued $\Lim{x \rightarrow c} f(x) = l$ and $\Lim{x \rightarrow c} g(x) = k$.  Then: $\Lim{x \rightarrow c}(f(x)g(x)) = lk$

We will also need to remember the combined sum rule for limits:
Let $f,g$ be real valued $\Lim{x \rightarrow c} f(x) = l$ and $\Lim{x \rightarrow c} g(x) = k$.  Let $\lambda, \kappa \in \mathbb{R}$.  Then $\Lim{x \rightarrow c}(\lambda f(x) + \kappa g(x)) = \lambda l + \kappa k$

Consider the function $l(x) = x$.  Then $\Lim{x \rightarrow c} l(x) =c$.  Then by applying the product rule for limits to $\Lim{x \rightarrow c} l(x) l(x) = \Lim{x \rightarrow c} x^2 =c^2$.  We can continue applying this rule so that for any value $d \in \mathbb{N}$ we have $\Lim{x \rightarrow c} x^d = c^d$.  Let $P(x)=a_nx^n +a_{n-1}x^{n-1}+ \cdots + a_1x+a_0$.  Now by applying the combined sum rule to $P(x)$ we get $\Lim{x \rightarrow c} P(x) = \Lim{x \rightarrow c} a_nx^n +a_{n-1}x^{n-1}+ \cdots + a_1x+a_0 = a_nc^n +a_{n-1}c^{n-1}+ \cdots + a_1c+a_0 = P(c)$.  Therefore $P(x)$ is continuous for any value $c$.

%Using induction, first prove that a polynomial of degree 1 is continuous.  Let $m,b \in \mathbb{R}$, and let $f$ be a real function defined as $f(x) = m x + b$.  Then we want to show that $f$ is continuous at every real number $c \in \mathbb{R}$.

%Assume $m \neq 0$, let $\epsilon > 0$, and $\delta = \frac{\epsilon}{|m|}$.  Then whenever $|x-c| < \delta$.
%\begin{align*}
%|f(x)-f(c)| &= |mx+b -mc -b| \\
%&= |m(x-c)| \\
%&= |m||x-c| \\
%&< |m| \delta \\
%= \epsilon
%\end{align*}
%Therefore we have found a $\delta$ for a given $\epsilon$ so that $|f(x)-f(c)|< \epsilon$ whenever $|x-c| < \delta$.  The case when $m=0$, follows similar, but simpler logic.



\section*{Proof of Intermediate Value Theorem}
Below is a proof of the Intermediate Value Theorem.

Consider a function f




\section*{Proof the square root of a complex number is complex}\footnote{Thanks to \url{http://math.stackexchange.com/questions/883030/proof-for-complex-numbers-and-square-root}}
Let $w = s(\cos(\theta) + i \sin(\theta))$ be a non-zero complex number such that $w^2 = z$.  Therefore:
$$w^2 = (s(\cos(\theta) + i \sin(\theta)))^2 = s^2(\cos(2 \theta) + i \sin(2 \theta)) = r (\cos(\theta')+ i \sin(\theta')$$
by De Moivre's theorem.

$\cos(2 \theta) = \cos(\theta')$ so $2 \theta = \pm \theta' + 2n\pi$, where $n$ is an integer

Similarly $\sin(2 \theta) = \sin(\theta')$ so $2 \theta = \pm \theta' + 2l\pi$, where $l$ is an integer.


\emph{TBD}

\end{appendices}
\end{document}

\section*{Unit Plan: The field of polynomials with integer coefficients}

Overview: Participants will utilize secondary math topics to push their mathematical understanding further.  This is a 10 day unit plan, which is suitable for high school students or secondary math teachers.\\

Table of contents:
\begin{enumerate}
\item Addition and subtraction of polynomials
\item Multiplication of polynomials
\item Remainder Theorem
\item Review of Prime Numbers
\item Fundamental Theorem of Algebra
\end{enumerate}

Lesson structure: Students should work through the tasks in small groups.  The teacher should monitor the student's conversations, inquiring to further student reasoning and make notes of student work.  If a group is stuck, the teacher may send a group member from another team to share some ideas.  It is important to encourage positive sharing and encouragement to facilitate the mathematical thinking.  In the final 20 minutes of each day a discussion about the big ideas should be addressed.

\pagebreak

\subsection*{Recommendations for facilitating individual lessons}
\paragraph*{Lesson 1: Addition}

\paragraph*{Lesson 2: Multiplication}
Objective: Students compare and contrast the similarities and differences the various representations of multiplication.\\

Materials: Calculators, scratch paper\\

Time: 90 minutes\\

Lesson Plan:  This lesson may be used with any age group of secondary students that have a concept of multiplication.  It can be made more simple by using multiplication of only polynomials and integers (labeled group 1).  It can be made more advanced by including vectors, decimals, and trigonometric functions (labeled group 3).  Give the students 5 minutes to complete the first section.  You may want to explain what the dots mean  Choose students to present their method of multiplication, highlight the models they choose (FOIL/Rainbow, Distributive property, Vertical, or Box).  Give students 3 minutes to complete section 2.  Conduct a class discussion on their comparisons of collaboration and competition.  Conclude by telling the students that the root of competition stems from rivalry and the root of collaborate stems from work with.  Give the students 10 minutes to complete section 3.  

\pagebreak
\subsubsection*{Lesson 2: Multiplication}

\begin{multicols}{3}
\textbf{Group 1}
$$57 \times 31$$
$$(5x+7)(3x+1)$$
$$1200005 \times 21$$
$$(x^6+2x^5+5)(2x+1)$$
$$1234 \times 987$$
$$(x^3+2x^2+3x+4)(9x^2+8x+7)$$

\columnbreak

\textbf{Group 2}
$$14 \times 23$$
$$(x+4)(2x+3)$$
$$(5x^2+4x+3)(x+2)$$
$$543 \times 12$$
$$(x-1)(x+1)$$
$$(x-1)(x^2+x+1)$$
$$(x-1)(x^3+x^2+x+1)$$
$$(x-1)(x^{100}+ x^{99}+ \cdots +x+1)$$
\columnbreak



\columnbreak
\textbf{Group 3}
$$10 \times 2$$
$$x \times 2$$
$$111 \times 22$$
$$(x^2+x+1)(2x+2)$$
$$(2x+1)(3x+4)$$
$$\begin{bmatrix}
2 & 1
\end{bmatrix}
\times
\begin{bmatrix}
3 & 4
\end{bmatrix}$$
\end{multicols}

\subsubsection*{Section 1}
\begin{question}
Do your best to complete the multiplication of at least one of the above groups.
\end{question}\lsol

\subsubsection*{Section 2}
\begin{question}
What are the benefits of competition?
\end{question}\ssol

\begin{question}
What are the benefits of collaboration?
\end{question}\ssol

\begin{question}
What are the similarities between competition and collaboration?
\end{question}\ssol

\pagebreak
 
\subsubsection*{Section 3}
\begin{question}
Choose a different group in the above section and complete the multiplication.  As you complete the multiplication record the similarities and differences you notice in the table below.
\end{question}

\begin{table}[h!]
\begin{tabular}{p{4cm}|p{4cm}}
Similarities & Differences \\ \hline & \\ & \\ & \\ & \\ & \\ & \\ & \\ & \\ & \\ & \\ & \\ & \\
\end{tabular}
\end{table}

\subsubsection*{Section 4}
\begin{question}
Develop a way to multiply $\begin{bmatrix}
2\\ 1 \\ 0
\end{bmatrix}
\times
\begin{bmatrix}
3 & 4
\end{bmatrix}$.  Be sure to state your assumptions.
\end{question}\lsol
\begin{multicols}{2}
\begin{question}
How did recognizing how things are similar help you to create new things?
\end{question}
\columnbreak
\begin{question}
How did recognizing how things are different help you to create new things?
\end{question}
\end{multicols}
\ssol

\begin{question}
Did you use competition or collaboration to create something new?  Explain.
\end{question}

\pagebreak


\subsection*{Addition}

\subsection*{Multiplication of polynomials}

\end{appendices}

\end{document}


\begin{pspicture}
\psgrid[unit=.05\linewidth, subgriddiv=1, linewidth=.01 gridlabels=0pt, gridcolor=gray, griddots=10](0,0)(-10,-10)(10,10) 
\psaxes[unit=.05\linewidth, linewidth=.1 labelFontSize=\scriptscriptstyle]{<->}(0,0)(-10.5,-10.5)(10.5,10.5)
\end{pspicture}


\begin{tikzpicture}[scale=.5]	
\draw[help lines] (0,0) grid (20,20);
\end{tikzpicture}


